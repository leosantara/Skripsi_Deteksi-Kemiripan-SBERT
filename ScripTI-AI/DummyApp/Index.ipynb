{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45545ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model dari: D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Model\\Trained_SBERT\\finetuned_all-nusabert-base_v1 ...\n",
      "Membaca data: D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Preprocessing\\dataTuning.csv ...\n",
      "Total Kalimat: 24509\n",
      "Total Proposal Unik: 1279\n",
      "\n",
      "[1/4] Melakukan Encoding Kalimat...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7414f05d3fa44ab6af01cd28597c8b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/4] Membangun Index FAISS (Exact Search)...\n",
      "   Dimensi Vektor: 768\n",
      "   Total vektor terindeks: 24509\n",
      "‚úÖ Index FAISS tersimpan di: D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\\proposal_vectors.index\n",
      "‚úÖ Metadata tersimpan di: D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\\proposal_metadata.pkl\n",
      "\n",
      "[3/4] Melatih Clustering (K-Means) pada Level Kalimat...\n",
      "‚úÖ Model K-Means tersimpan di: D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\\topic_cluster_model.pkl\n",
      "\n",
      "[4/4] Menghitung Profil Topik per Proposal (Voting)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Voting Profile: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1279/1279 [00:00<00:00, 3253.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Peta Distribusi Cluster Proposal tersimpan di: D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\\proposal_clusters.csv\n",
      "\n",
      "=== CONTOH DATA CLUSTERING ===\n",
      "  proposal_id  primary_cluster  primary_ratio  secondary_cluster  \\\n",
      "0         102                3       0.583333                 15   \n",
      "1        1057               17       0.458333                 11   \n",
      "2        1063                4       0.956522                  9   \n",
      "\n",
      "   secondary_ratio  tertiary_cluster  tertiary_ratio  \n",
      "0         0.250000                 8        0.083333  \n",
      "1         0.291667                 4        0.250000  \n",
      "2         0.043478                -1        0.000000  \n",
      "\n",
      "=== SYSTEM ASSETS GENERATED SUCCESSFULLY ===\n"
     ]
    }
   ],
   "source": [
    "# ================= CELL FINAL: INDEXING FAISS & SENTENCE-LEVEL CLUSTERING =================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. KONFIGURASI PATH (SESUAIKAN DISINI) ---\n",
    "# Path Model Terbaik (Epoch 4)\n",
    "MODEL_PATH = r\"D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Model\\Trained_SBERT\\finetuned_all-nusabert-base_v1\"\n",
    "\n",
    "# File Data Mentah (Kalimat)\n",
    "INPUT_CSV = r\"D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Preprocessing\\dataTuning.csv\"\n",
    "\n",
    "# Output Folder (Tempat simpan index & metadata untuk Sistem)\n",
    "OUTPUT_DIR = r\"D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Nama File Output\n",
    "INDEX_FILE = os.path.join(OUTPUT_DIR, \"proposal_vectors.index\")\n",
    "META_FILE  = os.path.join(OUTPUT_DIR, \"proposal_metadata.pkl\")\n",
    "CLUSTER_MODEL_FILE = os.path.join(OUTPUT_DIR, \"topic_cluster_model.pkl\")\n",
    "CLUSTER_MAP_FILE   = os.path.join(OUTPUT_DIR, \"proposal_clusters.csv\")\n",
    "\n",
    "# Konfigurasi Clustering\n",
    "NUM_CLUSTERS = 20  # Jumlah topik besar yang ingin dipetakan\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Model dari: {MODEL_PATH} ...\")\n",
    "# Cek apakah perlu prefix (untuk E5)\n",
    "device = 'cuda' if faiss.get_num_gpus() > 0 else 'cpu' # Cek GPU via FAISS/Torch logic standar\n",
    "model = SentenceTransformer(MODEL_PATH)\n",
    "\n",
    "print(f\"Membaca data: {INPUT_CSV} ...\")\n",
    "df = pd.read_csv(INPUT_CSV, encoding='utf-8-sig')\n",
    "\n",
    "# Pastikan kolom text string dan proposal_id konsisten\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['proposal_id'] = df['proposal_id'].astype(str)\n",
    "\n",
    "sentences = df['text'].tolist()\n",
    "proposal_ids = df['proposal_id'].tolist()\n",
    "\n",
    "print(f\"Total Kalimat: {len(sentences)}\")\n",
    "print(f\"Total Proposal Unik: {df['proposal_id'].nunique()}\")\n",
    "\n",
    "# --- 3. ENCODING (KALIMAT -> VEKTOR) ---\n",
    "print(\"\\n[1/4] Melakukan Encoding Kalimat...\")\n",
    "# Tambahkan prefix 'query: ' jika modelnya E5 (Deteksi otomatis dari nama path)\n",
    "if \"e5\" in MODEL_PATH.lower():\n",
    "    print(\"   -> Terdeteksi model E5, menambahkan prefix 'query: ' ...\")\n",
    "    sentences_to_encode = [\"query: \" + s for s in sentences]\n",
    "else:\n",
    "    sentences_to_encode = sentences\n",
    "\n",
    "# Batch size 64 agar cepat di GPU\n",
    "embeddings = model.encode(sentences_to_encode, batch_size=64, show_progress_bar=True, convert_to_tensor=False)\n",
    "\n",
    "# Konversi ke Numpy Float32 (Wajib buat FAISS)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# --- 4. FAISS INDEXING (FLAT IP) ---\n",
    "print(\"\\n[2/4] Membangun Index FAISS (Exact Search)...\")\n",
    "\n",
    "# PENTING: Normalisasi L2 agar Inner Product (IP) = Cosine Similarity\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Dimensi vektor\n",
    "d = embeddings.shape[1] \n",
    "print(f\"   Dimensi Vektor: {d}\")\n",
    "\n",
    "# Menggunakan IndexFlatIP (Brute Force/Exact) sesuai Bab 3.9\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"   Total vektor terindeks: {index.ntotal}\")\n",
    "\n",
    "# Simpan Index\n",
    "faiss.write_index(index, INDEX_FILE)\n",
    "print(f\"‚úÖ Index FAISS tersimpan di: {INDEX_FILE}\")\n",
    "\n",
    "# Simpan Metadata (Mapping Index FAISS -> ID Proposal & Teks Asli)\n",
    "# Backend butuh ini untuk tahu \"Hasil pencarian baris ke-X itu kalimat apa?\"\n",
    "df.to_pickle(META_FILE) \n",
    "print(f\"‚úÖ Metadata tersimpan di: {META_FILE}\")\n",
    "\n",
    "# --- 5. CLUSTERING (SENTENCE-LEVEL VOTING) ---\n",
    "print(\"\\n[3/4] Melatih Clustering (K-Means) pada Level Kalimat...\")\n",
    "\n",
    "# Latih K-Means pada SEMUA vektor kalimat\n",
    "# Ini memungkinkan model membedakan nuansa topik per kalimat\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# Simpan Model K-Means (Untuk memprediksi proposal baru mahasiswa nanti)\n",
    "with open(CLUSTER_MODEL_FILE, 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "print(f\"‚úÖ Model K-Means tersimpan di: {CLUSTER_MODEL_FILE}\")\n",
    "\n",
    "# Dapatkan label cluster untuk setiap kalimat di database\n",
    "sentence_cluster_labels = kmeans.labels_\n",
    "\n",
    "print(\"\\n[4/4] Menghitung Profil Topik per Proposal (Voting)...\")\n",
    "\n",
    "# Gabungkan ID Proposal dengan Label Cluster Kalimatnya\n",
    "df_voting = pd.DataFrame({\n",
    "    'proposal_id': proposal_ids,\n",
    "    'cluster_label': sentence_cluster_labels\n",
    "})\n",
    "\n",
    "# Agregasi: Hitung persentase dominasi cluster per proposal\n",
    "proposal_profiles = []\n",
    "grouped = df_voting.groupby('proposal_id')\n",
    "\n",
    "for pid, group in tqdm(grouped, desc=\"Voting Profile\"):\n",
    "    # Hitung frekuensi cluster di proposal ini\n",
    "    counts = group['cluster_label'].value_counts(normalize=True) # normalize=True jadi % (0.0 - 1.0)\n",
    "    \n",
    "    # Ambil Top 3 Cluster dominan\n",
    "    # (Misal: 60% Cluster 5, 30% Cluster 2, 10% Cluster 8)\n",
    "    top_clusters = counts.head(3)\n",
    "    \n",
    "    record = {'proposal_id': pid}\n",
    "    \n",
    "    # Isi Cluster 1 (Utama)\n",
    "    record['primary_cluster'] = top_clusters.index[0]\n",
    "    record['primary_ratio']   = top_clusters.iloc[0]\n",
    "    \n",
    "    # Isi Cluster 2 (Jika ada)\n",
    "    if len(top_clusters) > 1:\n",
    "        record['secondary_cluster'] = top_clusters.index[1]\n",
    "        record['secondary_ratio']   = top_clusters.iloc[1]\n",
    "    else:\n",
    "        record['secondary_cluster'] = -1\n",
    "        record['secondary_ratio']   = 0.0\n",
    "        \n",
    "    # Isi Cluster 3 (Jika ada)\n",
    "    if len(top_clusters) > 2:\n",
    "        record['tertiary_cluster']  = top_clusters.index[2]\n",
    "        record['tertiary_ratio']    = top_clusters.iloc[2]\n",
    "    else:\n",
    "        record['tertiary_cluster']  = -1\n",
    "        record['tertiary_ratio']    = 0.0\n",
    "    \n",
    "    proposal_profiles.append(record)\n",
    "\n",
    "# Simpan Hasil Profiling\n",
    "df_prop_clusters = pd.DataFrame(proposal_profiles)\n",
    "df_prop_clusters.to_csv(CLUSTER_MAP_FILE, index=False)\n",
    "\n",
    "print(f\"‚úÖ Peta Distribusi Cluster Proposal tersimpan di: {CLUSTER_MAP_FILE}\")\n",
    "print(\"\\n=== CONTOH DATA CLUSTERING ===\")\n",
    "print(df_prop_clusters.head(3))\n",
    "\n",
    "print(\"\\n=== SYSTEM ASSETS GENERATED SUCCESSFULLY ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3729b390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membaca data...\n",
      "Data tergabung: 1276 proposal.\n",
      "‚úÖ Mapping Cluster ke Dosen ID tersimpan: D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\\cluster_dosen_ids.json\n"
     ]
    }
   ],
   "source": [
    "# ================= CELL: GENERATE CLUSTER -> DOSEN ID MAPPING =================\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "BASE_DIR = r\"D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\"\n",
    "\n",
    "# Input 1: Hasil Clustering (Dari script sebelumnya)\n",
    "CLUSTER_FILE = os.path.join(BASE_DIR, \"proposal_clusters.csv\")\n",
    "\n",
    "# Input 2: Data ID Dosen (Dari Export SQL baru kamu)\n",
    "DOSEN_IDS_FILE = os.path.join(BASE_DIR, \"proposal_dosen_ids.csv\")\n",
    "\n",
    "# Output: Mapping Cluster -> List[DosenID]\n",
    "OUTPUT_MAP_JSON = os.path.join(BASE_DIR, \"cluster_dosen_ids.json\")\n",
    "\n",
    "# --- PROSES ---\n",
    "print(\"Membaca data...\")\n",
    "df_clusters = pd.read_csv(CLUSTER_FILE) # [proposal_id, primary_cluster, ...]\n",
    "df_dosen = pd.read_csv(DOSEN_IDS_FILE)  # [proposal_id, id_dosen_1, id_dosen_2]\n",
    "\n",
    "# Gabungkan\n",
    "df_merged = pd.merge(df_clusters, df_dosen, on='proposal_id', how='inner')\n",
    "print(f\"Data tergabung: {len(df_merged)} proposal.\")\n",
    "\n",
    "# Dictionary hasil: { cluster_id: [dosen_id_a, dosen_id_b] }\n",
    "cluster_dosen_map = {}\n",
    "\n",
    "# Group by Cluster Utama\n",
    "grouped = df_merged.groupby('primary_cluster')\n",
    "\n",
    "for cluster_id, group in grouped:\n",
    "    # Kumpulkan semua ID dosen di cluster ini\n",
    "    dosen_list = []\n",
    "    \n",
    "    # Ambil Dosen 1\n",
    "    d1 = group['id_dosen_1'].dropna().tolist()\n",
    "    dosen_list.extend(d1)\n",
    "    \n",
    "    # Ambil Dosen 2 (jika ada)\n",
    "    d2 = group['id_dosen_2'].dropna().tolist()\n",
    "    dosen_list.extend(d2)\n",
    "    \n",
    "    # Hitung Frekuensi (Siapa dosen paling sering di topik ini?)\n",
    "    if not dosen_list:\n",
    "        cluster_dosen_map[int(cluster_id)] = []\n",
    "        continue\n",
    "        \n",
    "    # Pakai numpy/pandas value_counts biar cepat\n",
    "    counts = pd.Series(dosen_list).value_counts()\n",
    "    \n",
    "    # Ambil Top 3 Dosen ID terbanyak\n",
    "    # Convert ke int (JSON standard)\n",
    "    top_ids = [int(x) for x in counts.head(3).index.tolist()]\n",
    "    \n",
    "    cluster_dosen_map[int(cluster_id)] = top_ids\n",
    "\n",
    "# Simpan\n",
    "with open(OUTPUT_MAP_JSON, 'w') as f:\n",
    "    json.dump(cluster_dosen_map, f)\n",
    "\n",
    "print(f\"‚úÖ Mapping Cluster ke Dosen ID tersimpan: {OUTPUT_MAP_JSON}\")\n",
    "# Contoh isi: { \"0\": [55, 12], \"1\": [60] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5be9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Memuat data...\n",
      "   -> Total Kalimat: 24509\n",
      "‚ö° Encoding vectors (SBERT)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd3dfe8e1754bee84b2a0cc09ad1082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è  Membangun Index FAISS...\n",
      "   -> Index tersimpan: D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\\proposal_vectors.index\n",
      "üß© Melakukan Clustering...\n",
      "üè∑Ô∏è  Generating Topic Names...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c464dcfaa6eb4f3288118a1e57af8066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Topics:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë• Mapping Dosen...\n",
      "‚úÖ Selesai! File D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\\cluster_info_complete.json telah diperbarui.\n",
      "Contoh Hasil: [('0', {'name': 'Batik, Citra, Klasifikasi', 'count': 152, 'dosen1': [6, 18, 7], 'dosen2': [7, 20, 50]}), ('1', {'name': 'Akademik, Learning, Media', 'count': 53, 'dosen1': [50, 12, 15], 'dosen2': [17, 2, 14]})]\n"
     ]
    }
   ],
   "source": [
    "# ================= CELL FINAL: RE-INDEXING & SMART CLUSTERING V3 =================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. KONFIGURASI PATH (SESUAIKAN DISINI) ---\n",
    "BASE_DIR = r\"D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\"\n",
    "MODEL_PATH = r\"D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Model\\Trained_SBERT\\finetuned_all-nusabert-base_v1\"\n",
    "INPUT_CSV = r\"D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Preprocessing\\dataTuning.csv\"\n",
    "DOSEN_IDS_FILE = r\"D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Index_and_Clusters\\proposal_dosen_ids.csv\" # Pastikan file ini ada!\n",
    "\n",
    "# Output Files\n",
    "INDEX_FILE = os.path.join(BASE_DIR, \"proposal_vectors.index\")\n",
    "META_FILE = os.path.join(BASE_DIR, \"proposal_metadata.pkl\")\n",
    "CLUSTER_MODEL_FILE = os.path.join(BASE_DIR, \"topic_cluster_model.pkl\")\n",
    "CLUSTER_INFO_FILE = os.path.join(BASE_DIR, \"cluster_info_complete.json\")\n",
    "\n",
    "NUM_CLUSTERS = 20\n",
    "\n",
    "# --- 2. LOAD & PREPARE DATA ---\n",
    "print(\"üì• Memuat data...\")\n",
    "df_texts = pd.read_csv(INPUT_CSV, encoding='utf-8-sig')\n",
    "df_dosen = pd.read_csv(DOSEN_IDS_FILE)\n",
    "\n",
    "# Pastikan tipe data string\n",
    "df_texts['text'] = df_texts['text'].astype(str)\n",
    "df_texts['proposal_id'] = df_texts['proposal_id'].astype(str)\n",
    "df_dosen['proposal_id'] = df_dosen['proposal_id'].astype(str)\n",
    "\n",
    "print(f\"   -> Total Kalimat: {len(df_texts)}\")\n",
    "\n",
    "# --- 3. ENCODING (SBERT) ---\n",
    "print(\"‚ö° Encoding vectors (SBERT)...\")\n",
    "model = SentenceTransformer(MODEL_PATH)\n",
    "\n",
    "sentences_list = df_texts['text'].tolist()\n",
    "if \"e5\" in MODEL_PATH.lower():\n",
    "    sentences_input = [\"query: \" + s for s in sentences_list]\n",
    "else:\n",
    "    sentences_input = sentences_list\n",
    "\n",
    "embeddings = model.encode(sentences_input, batch_size=64, show_progress_bar=True, convert_to_tensor=False)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# --- 4. FAISS INDEXING ---\n",
    "print(\"üóÇÔ∏è  Membangun Index FAISS...\")\n",
    "d = embeddings.shape[1]\n",
    "new_index = faiss.IndexFlatIP(d)\n",
    "new_index.add(embeddings)\n",
    "\n",
    "faiss.write_index(new_index, INDEX_FILE)\n",
    "df_texts.to_pickle(META_FILE)\n",
    "print(f\"   -> Index tersimpan: {INDEX_FILE}\")\n",
    "\n",
    "# --- 5. CLUSTERING (K-MEANS) ---\n",
    "print(\"üß© Melakukan Clustering...\")\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "with open(CLUSTER_MODEL_FILE, 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "df_texts['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# --- 6. TOPIC NAMING (SMART HYBRID) ---\n",
    "print(\"üè∑Ô∏è  Generating Topic Names...\")\n",
    "\n",
    "# Setup Stopwords & Normalization\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords_sastrawi = factory.get_stop_words()\n",
    "\n",
    "academic_stopwords = [\n",
    "                'banyak', 'perkembangan', 'mengalami', 'terjadi', 'sering', 'kerap', \n",
    "                'selama', 'masa', 'kini', 'saat', 'ini', 'itu', 'tersebut', 'berbagai',\n",
    "                'merupakan', 'adalah', 'yaitu', 'yakni', 'antara', 'lain', 'sangat',\n",
    "                'serta', 'ataupun', 'bagai', 'bagaimana', 'sebagai', 'banyak', 'sedikit',\n",
    "                'berdasarkan', 'melalui', 'secara', 'suatu', 'sebuah', 'jadi', 'lintas', \n",
    "                'pula', 'pun', 'dapat', 'bisa', 'mampu', 'guna', 'agar', 'semakin', 'beberapa',\n",
    "                \n",
    "                'menggunakan', 'digunakan', 'dilakukan', 'melakukan', 'membuat', 'membangun', 'kehidupan',\n",
    "                'merancang', 'mengimplementasikan', 'penerapan', 'implementasi', 'pembuatan', 'masyarakat',\n",
    "                'pembangunan', 'perancangan', 'pengembangan', 'analisis', 'menganalisis', 'orang',\n",
    "                'menentukan', 'menghitung', 'mencari', 'diterapkan', 'dibuat', 'dikembangkan', 'satunama',\n",
    "                'membantu', 'mengetahui', 'meningkatkan', 'diharapkan', 'perlu', 'mengenai',\n",
    "                'menyelesaikan', 'permasalahan', 'solusi', 'hasil', 'proses', 'lalu', 'pesat',\n",
    "                'menjadi', 'lebih', 'memiliki',  'satu', 'salah', 'benar', 'lama', 'manual',\n",
    "                'baik', 'besar', 'tinggi', 'cepat', 'kuat', 'tepat', 'efektif', 'efisien', 'bidang',\n",
    "                'mengatur', 'memastikan', 'menjamin', 'mengecek', 'mengevaluasi', 'manusia',\n",
    "                'mengawasi', 'mengontrol', 'mengoptimalkan', 'mengumpulkan', 'mengolah', 'barang',\n",
    "                'menyimpan', 'menampilkan', 'memberikan', 'menyiapkan', 'menyesuaikan', 'jenis',\n",
    "                'mensinkronkan', 'menggabungkan', 'mengganti', 'menghapus', 'menambah', 'jalan',\n",
    "                'satu', 'dua', 'tiga', 'utama', 'empat', 'lima', 'enam', 'tujuh', 'delapan', 'sembilan', 'sepuluh', 'puluh',\n",
    "                \n",
    "                'sistem', 'aplikasi', 'website', 'web', 'berbasis', 'program', 'fitur', 'perusahaan', 'organisasi',\n",
    "                'metode', 'metodologi', 'data', 'informasi', 'teknologi', 'komputer', 'siswa', 'algoritma',\n",
    "                'penelitian', 'skripsi', 'tugas', 'akhir', 'proposal', 'penulis', 'pengguna', 'yayasan',\n",
    "                'latar', 'belakang', 'masalah', 'tujuan', 'manfaat', 'rumusan', 'batasan', 'kampus',\n",
    "                'universitas', 'kristen', 'duta', 'wacana', 'ukdw', 'yogyakarta', 'fakultas', 'prodi',\n",
    "                'indonesia', 'tahun', 'waktu', 'jumlah', 'studi', 'kasus', 'dosen', 'mahasiswa'\n",
    "            ]\n",
    "final_stopwords = list(set(stopwords_sastrawi + academic_stopwords))\n",
    "\n",
    "normalization_map = {\n",
    "                \"mengenali\": \"pengenalan\",\"mengenal\": \"pengenalan\", \"dikenali\": \"pengenalan\", \"identifikasi\": \"identifikasi\",\n",
    "                \"mengidentifikasi\": \"identifikasi\", \"klasifikasi\": \"klasifikasi\", \"mengklasifikasikan\": \"klasifikasi\",\n",
    "                \"deteksi\": \"deteksi\", \"mendeteksi\": \"deteksi\", \"pendeteksian\": \"deteksi\",\n",
    "                \"diagnosa\": \"diagnosa\", \"mendiagnosa\": \"diagnosa\", \"prediksi\": \"prediksi\",\n",
    "                \"memprediksi\": \"prediksi\", \"rekomendasi\": \"rekomendasi\", \"merekomendasikan\": \"rekomendasi\",\n",
    "                \"belajar\": \"pembelajaran\", \"ajar\": \"pembelajaran\", \"edukasi\": \"pembelajaran\",\n",
    "                \"citra\": \"citra\", \"image\": \"citra\", \"mobile\": \"mobile\", \"android\": \"android\",\n",
    "                \"aman\": \"keamanan\", \"mengamankan\": \"keamanan\", \"pengamanan\": \"keamanan\",\n",
    "                \"jaring\": \"jaringan\", \"terhubung\": \"koneksi\"\n",
    "            }\n",
    "\n",
    "cluster_names = {}\n",
    "for i in tqdm(range(NUM_CLUSTERS), desc=\"Generating Topics\"):\n",
    "    c_df = df_texts[df_texts['cluster_label'] == i]\n",
    "    if len(c_df) > 1000: c_df = c_df.sample(1000, random_state=42)\n",
    "    \n",
    "    if c_df.empty:\n",
    "        cluster_names[str(i)] = f\"Topik {i}\"\n",
    "        continue\n",
    "    \n",
    "    raw_text = \" \".join(c_df['text'].tolist())\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', ' ', raw_text).lower()\n",
    "    \n",
    "    words = clean_text.split()\n",
    "    normalized_words = [normalization_map.get(w, w) for w in words]\n",
    "    final_text = \" \".join(normalized_words)\n",
    "    \n",
    "    try:\n",
    "        tfidf = TfidfVectorizer(\n",
    "            max_features=5,\n",
    "            stop_words=final_stopwords,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        tfidf.fit_transform([final_text])\n",
    "        feature_names = tfidf.get_feature_names_out()\n",
    "        keys = [k.title() for k in feature_names if len(k) > 3]\n",
    "        \n",
    "        if not keys:\n",
    "            cluster_names[str(i)] = f\"Topik {i}\"\n",
    "        else:\n",
    "            cluster_names[str(i)] = \", \".join(keys[:3])\n",
    "            \n",
    "    except Exception as e:\n",
    "        cluster_names[str(i)] = f\"Topik {i}\"\n",
    "\n",
    "# --- 7. MAPPING DOSEN (SPLIT ROLE) ---\n",
    "print(\"üë• Mapping Dosen...\")\n",
    "prop_cluster_map = df_texts.groupby('proposal_id')['cluster_label'].agg(\n",
    "    lambda x: x.value_counts().index[0]\n",
    ").reset_index()\n",
    "\n",
    "df_merged = pd.merge(prop_cluster_map, df_dosen, on='proposal_id')\n",
    "\n",
    "final_cluster_info = {}\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    g = df_merged[df_merged['cluster_label'] == i]\n",
    "    count_proposals = len(g)\n",
    "    \n",
    "    d1 = []\n",
    "    if not g.empty and 'id_dosen_1' in g:\n",
    "        vc1 = g['id_dosen_1'].value_counts()\n",
    "        if not vc1.empty: d1 = [int(x) for x in vc1.head(3).index.tolist()]\n",
    "    \n",
    "    d2 = []\n",
    "    if not g.empty and 'id_dosen_2' in g:\n",
    "        vc2 = g['id_dosen_2'].value_counts()\n",
    "        if not vc2.empty: d2 = [int(x) for x in vc2.head(3).index.tolist()]\n",
    "    \n",
    "    final_cluster_info[str(i)] = {\n",
    "        \"name\": cluster_names.get(str(i), \"\"),\n",
    "        \"count\": int(count_proposals),\n",
    "        \"dosen1\": d1, \n",
    "        \"dosen2\": d2\n",
    "    }\n",
    "\n",
    "with open(CLUSTER_INFO_FILE, 'w') as f: \n",
    "    json.dump(final_cluster_info, f)\n",
    "\n",
    "print(f\"‚úÖ Selesai! File {CLUSTER_INFO_FILE} telah diperbarui.\")\n",
    "print(\"Contoh Hasil:\", list(final_cluster_info.items())[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
