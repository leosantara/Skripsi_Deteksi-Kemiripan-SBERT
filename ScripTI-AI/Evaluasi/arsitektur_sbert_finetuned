digraph {
	graph [size="376.34999999999997,376.34999999999997"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2122051207088 [label="
 (1, 768)" fillcolor=darkolivegreen1]
	2122952785152 [label="CatBackward0
------------
dim: 1"]
	2122952784720 -> 2122952785152
	2122952784720 -> 2122051193456 [dir=none]
	2122051193456 [label="other
 (1, 768)" fillcolor=orange]
	2122952784720 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	2122952782896 -> 2122952784720
	2122952782896 [label="SumBackward1
---------------------------
dim           :        (1,)
keepdim       :       False
self_sym_sizes: (1, 8, 768)"]
	2122952783376 -> 2122952782896
	2122952783376 -> 2122051201808 [dir=none]
	2122051201808 [label="other
 (1, 8, 768)" fillcolor=orange]
	2122952783376 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	2122952785200 -> 2122952783376
	2122952785200 -> 2122050642544 [dir=none]
	2122050642544 [label="bias
 (768)" fillcolor=orange]
	2122952785200 -> 2122051206896 [dir=none]
	2122051206896 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122952785200 -> 2122051202000 [dir=none]
	2122051202000 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122952785200 -> 2122051204112 [dir=none]
	2122051204112 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122952785200 -> 2122050645328 [dir=none]
	2122050645328 [label="weight
 (768)" fillcolor=orange]
	2122952785200 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122952783040 -> 2122952785200
	2122952783040 [label="AddBackward0
------------
alpha: 1"]
	2122952782416 -> 2122952783040
	2122952782416 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122951679120 -> 2122952782416
	2122951679120 -> 2122051201040 [dir=none]
	2122051201040 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122951679120 -> 2122051191056 [dir=none]
	2122051191056 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122951679120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122951674752 -> 2122951679120
	2122050648976 [label="0.auto_model.encoder.layer.11.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050648976 -> 2122951674752
	2122951674752 [label=AccumulateGrad]
	2122951680128 -> 2122951679120
	2122951680128 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122951673936 -> 2122951680128
	2122951673936 -> 2122051204880 [dir=none]
	2122051204880 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122951673936 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122951679072 -> 2122951673936
	2122951679072 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122951674608 -> 2122951679072
	2122951674608 -> 2122050646576 [dir=none]
	2122050646576 [label="mat1
 (8, 768)" fillcolor=orange]
	2122951674608 -> 2122050648016 [dir=none]
	2122050648016 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122951674608 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122951674512 -> 2122951674608
	2122050635536 [label="0.auto_model.encoder.layer.11.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122050635536 -> 2122951674512
	2122951674512 [label=AccumulateGrad]
	2122951673792 -> 2122951674608
	2122951673792 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122952784816 -> 2122951673792
	2122952784816 -> 2122050637456 [dir=none]
	2122050637456 [label="bias
 (768)" fillcolor=orange]
	2122952784816 -> 2122051191152 [dir=none]
	2122051191152 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122952784816 -> 2122050642640 [dir=none]
	2122050642640 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122952784816 -> 2122050640528 [dir=none]
	2122050640528 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122952784816 -> 2122050645424 [dir=none]
	2122050645424 [label="weight
 (768)" fillcolor=orange]
	2122952784816 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122951679264 -> 2122952784816
	2122951679264 [label="AddBackward0
------------
alpha: 1"]
	2122953383968 -> 2122951679264
	2122953383968 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122953384064 -> 2122953383968
	2122953384064 -> 2122050639376 [dir=none]
	2122050639376 [label="mat1
 (8, 768)" fillcolor=orange]
	2122953384064 -> 2122050637744 [dir=none]
	2122050637744 [label="mat2
 (768, 768)" fillcolor=orange]
	2122953384064 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122953384208 -> 2122953384064
	2122050643504 [label="0.auto_model.encoder.layer.11.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050643504 -> 2122953384208
	2122953384208 [label=AccumulateGrad]
	2122953386464 -> 2122953384064
	2122953386464 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953382960 -> 2122953386464
	2122953382960 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122953380320 -> 2122953382960
	2122953380320 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122953391456 -> 2122953380320
	2122953391456 -> 2122051202576 [dir=none]
	2122051202576 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122953391456 -> 2122050649456 [dir=none]
	2122050649456 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122953391456 -> 2122050645520 [dir=none]
	2122050645520 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122953391456 -> 2122050647632 [dir=none]
	2122050647632 [label="philox_offset
 ()" fillcolor=orange]
	2122953391456 -> 2122050643600 [dir=none]
	2122050643600 [label="philox_seed
 ()" fillcolor=orange]
	2122953391456 -> 2122051206512 [dir=none]
	2122051206512 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122953391456 -> 2122051203344 [dir=none]
	2122051203344 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122953391456 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122953382864 -> 2122953391456
	2122953382864 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122953390256 -> 2122953382864
	2122953390256 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953383920 -> 2122953390256
	2122953383920 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122953390640 -> 2122953383920
	2122953390640 -> 2122050643024 [dir=none]
	2122050643024 [label="mat1
 (8, 768)" fillcolor=orange]
	2122953390640 -> 2122050648496 [dir=none]
	2122050648496 [label="mat2
 (768, 768)" fillcolor=orange]
	2122953390640 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122953379792 -> 2122953390640
	2122050638896 [label="0.auto_model.encoder.layer.11.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122050638896 -> 2122953379792
	2122953379792 [label=AccumulateGrad]
	2122953391312 -> 2122953390640
	2122953391312 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953384496 -> 2122953391312
	2122953384496 -> 2122050641200 [dir=none]
	2122050641200 [label="bias
 (768)" fillcolor=orange]
	2122953384496 -> 2122051201232 [dir=none]
	2122051201232 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122953384496 -> 2122050649072 [dir=none]
	2122050649072 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122953384496 -> 2122050648112 [dir=none]
	2122050648112 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122953384496 -> 2122050642352 [dir=none]
	2122050642352 [label="weight
 (768)" fillcolor=orange]
	2122953384496 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122953384016 -> 2122953384496
	2122953384016 [label="AddBackward0
------------
alpha: 1"]
	2122953382624 -> 2122953384016
	2122953382624 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122953390448 -> 2122953382624
	2122953390448 -> 2122050645712 [dir=none]
	2122050645712 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122953390448 -> 2122050644560 [dir=none]
	2122050644560 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122953390448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122953390688 -> 2122953390448
	2122050644080 [label="0.auto_model.encoder.layer.10.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050644080 -> 2122953390688
	2122953390688 [label=AccumulateGrad]
	2122953382432 -> 2122953390448
	2122953382432 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122953383584 -> 2122953382432
	2122953383584 -> 2122051203536 [dir=none]
	2122051203536 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122953383584 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122953390784 -> 2122953383584
	2122953390784 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122953383824 -> 2122953390784
	2122953383824 -> 2122051596080 [dir=none]
	2122051596080 [label="mat1
 (8, 768)" fillcolor=orange]
	2122953383824 -> 2122051593200 [dir=none]
	2122051593200 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122953383824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122953386128 -> 2122953383824
	2122050637264 [label="0.auto_model.encoder.layer.10.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122050637264 -> 2122953386128
	2122953386128 [label=AccumulateGrad]
	2122953390736 -> 2122953383824
	2122953390736 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953383104 -> 2122953390736
	2122953383104 -> 2122050512432 [dir=none]
	2122050512432 [label="bias
 (768)" fillcolor=orange]
	2122953383104 -> 2122051206128 [dir=none]
	2122051206128 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122953383104 -> 2122051594448 [dir=none]
	2122051594448 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122953383104 -> 2122051592912 [dir=none]
	2122051592912 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122953383104 -> 2122050640912 [dir=none]
	2122050640912 [label="weight
 (768)" fillcolor=orange]
	2122953383104 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122953386320 -> 2122953383104
	2122953386320 [label="AddBackward0
------------
alpha: 1"]
	2122953383248 -> 2122953386320
	2122953383248 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122953390352 -> 2122953383248
	2122953390352 -> 2122051587536 [dir=none]
	2122051587536 [label="mat1
 (8, 768)" fillcolor=orange]
	2122953390352 -> 2122051595312 [dir=none]
	2122051595312 [label="mat2
 (768, 768)" fillcolor=orange]
	2122953390352 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122953379744 -> 2122953390352
	2122050637840 [label="0.auto_model.encoder.layer.10.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050637840 -> 2122953379744
	2122953379744 [label=AccumulateGrad]
	2122953390208 -> 2122953390352
	2122953390208 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953382528 -> 2122953390208
	2122953382528 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122953384256 -> 2122953382528
	2122953384256 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122953383872 -> 2122953384256
	2122953383872 -> 2122051204784 [dir=none]
	2122051204784 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122953383872 -> 2122051587632 [dir=none]
	2122051587632 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122953383872 -> 2122051591088 [dir=none]
	2122051591088 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122953383872 -> 2122051586000 [dir=none]
	2122051586000 [label="philox_offset
 ()" fillcolor=orange]
	2122953383872 -> 2122051584176 [dir=none]
	2122051584176 [label="philox_seed
 ()" fillcolor=orange]
	2122953383872 -> 2122051206608 [dir=none]
	2122051206608 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122953383872 -> 2122051203728 [dir=none]
	2122051203728 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122953383872 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122953382912 -> 2122953383872
	2122953382912 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122953384160 -> 2122953382912
	2122953384160 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953386416 -> 2122953384160
	2122953386416 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122953391120 -> 2122953386416
	2122953391120 -> 2122051586768 [dir=none]
	2122051586768 [label="mat1
 (8, 768)" fillcolor=orange]
	2122953391120 -> 2122051591568 [dir=none]
	2122051591568 [label="mat2
 (768, 768)" fillcolor=orange]
	2122953391120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122953384304 -> 2122953391120
	2122050638032 [label="0.auto_model.encoder.layer.10.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122050638032 -> 2122953384304
	2122953384304 [label=AccumulateGrad]
	2122953386512 -> 2122953391120
	2122953386512 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953383632 -> 2122953386512
	2122953383632 -> 2122050647248 [dir=none]
	2122050647248 [label="bias
 (768)" fillcolor=orange]
	2122953383632 -> 2122051200656 [dir=none]
	2122051200656 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122953383632 -> 2122051595792 [dir=none]
	2122051595792 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122953383632 -> 2122051590896 [dir=none]
	2122051590896 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122953383632 -> 2122050640240 [dir=none]
	2122050640240 [label="weight
 (768)" fillcolor=orange]
	2122953383632 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2121924363216 -> 2122953383632
	2121924363216 [label="AddBackward0
------------
alpha: 1"]
	2121924363312 -> 2121924363216
	2121924363312 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2121924363264 -> 2121924363312
	2121924363264 -> 2122051586192 [dir=none]
	2122051586192 [label="mat1
 (8, 3072)" fillcolor=orange]
	2121924363264 -> 2122051587056 [dir=none]
	2122051587056 [label="mat2
 (3072, 768)" fillcolor=orange]
	2121924363264 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2121924375792 -> 2121924363264
	2122050641968 [label="0.auto_model.encoder.layer.9.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050641968 -> 2121924375792
	2121924375792 [label=AccumulateGrad]
	2121924373536 -> 2121924363264
	2121924373536 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2121924363072 -> 2121924373536
	2121924363072 -> 2122051205648 [dir=none]
	2122051205648 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2121924363072 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2121924363120 -> 2121924363072
	2121924363120 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2121924366576 -> 2121924363120
	2121924366576 -> 2122051591376 [dir=none]
	2122051591376 [label="mat1
 (8, 768)" fillcolor=orange]
	2121924366576 -> 2122051586576 [dir=none]
	2122051586576 [label="mat2
 (768, 3072)" fillcolor=orange]
	2121924366576 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2121924375408 -> 2121924366576
	2122050644848 [label="0.auto_model.encoder.layer.9.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122050644848 -> 2121924375408
	2121924375408 [label=AccumulateGrad]
	2121924374064 -> 2121924366576
	2121924374064 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2121924375552 -> 2121924374064
	2121924375552 -> 2122050640624 [dir=none]
	2122050640624 [label="bias
 (768)" fillcolor=orange]
	2121924375552 -> 2122051203056 [dir=none]
	2122051203056 [label="input
 (1, 8, 768)" fillcolor=orange]
	2121924375552 -> 2122051591184 [dir=none]
	2122051591184 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2121924375552 -> 2122051593392 [dir=none]
	2122051593392 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2121924375552 -> 2122050644944 [dir=none]
	2122050644944 [label="weight
 (768)" fillcolor=orange]
	2121924375552 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2121924373440 -> 2121924375552
	2121924373440 [label="AddBackward0
------------
alpha: 1"]
	2121924373920 -> 2121924373440
	2121924373920 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2121924374976 -> 2121924373920
	2121924374976 -> 2122051595696 [dir=none]
	2122051595696 [label="mat1
 (8, 768)" fillcolor=orange]
	2121924374976 -> 2122051591952 [dir=none]
	2122051591952 [label="mat2
 (768, 768)" fillcolor=orange]
	2121924374976 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2121924362880 -> 2121924374976
	2122050638800 [label="0.auto_model.encoder.layer.9.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050638800 -> 2121924362880
	2121924362880 [label=AccumulateGrad]
	2121924374736 -> 2121924374976
	2121924374736 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2121924374640 -> 2121924374736
	2121924374640 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2121924368640 -> 2121924374640
	2121924368640 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2121924365904 -> 2121924368640
	2121924365904 -> 2122051201904 [dir=none]
	2122051201904 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2121924365904 -> 2122051589552 [dir=none]
	2122051589552 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2121924365904 -> 2122051588880 [dir=none]
	2122051588880 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2121924365904 -> 2122051587152 [dir=none]
	2122051587152 [label="philox_offset
 ()" fillcolor=orange]
	2121924365904 -> 2122051595216 [dir=none]
	2122051595216 [label="philox_seed
 ()" fillcolor=orange]
	2121924365904 -> 2122051198832 [dir=none]
	2122051198832 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2121924365904 -> 2122051195376 [dir=none]
	2122051195376 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2121924365904 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2121924365808 -> 2121924365904
	2121924365808 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2121924373728 -> 2121924365808
	2121924373728 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2121924373632 -> 2121924373728
	2121924373632 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2121924375216 -> 2121924373632
	2121924375216 -> 2122051590608 [dir=none]
	2122051590608 [label="mat1
 (8, 768)" fillcolor=orange]
	2121924375216 -> 2122051589936 [dir=none]
	2122051589936 [label="mat2
 (768, 768)" fillcolor=orange]
	2121924375216 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2121924377760 -> 2121924375216
	2122050647344 [label="0.auto_model.encoder.layer.9.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122050647344 -> 2121924377760
	2121924377760 [label=AccumulateGrad]
	2122050965840 -> 2121924375216
	2122050965840 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2121924374208 -> 2122050965840
	2121924374208 -> 2122050639760 [dir=none]
	2122050639760 [label="bias
 (768)" fillcolor=orange]
	2121924374208 -> 2122051199312 [dir=none]
	2122051199312 [label="input
 (1, 8, 768)" fillcolor=orange]
	2121924374208 -> 2122051595024 [dir=none]
	2122051595024 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2121924374208 -> 2122051588208 [dir=none]
	2122051588208 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2121924374208 -> 2122050637552 [dir=none]
	2122050637552 [label="weight
 (768)" fillcolor=orange]
	2121924374208 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050969920 -> 2121924374208
	2122050969920 [label="AddBackward0
------------
alpha: 1"]
	2122050973232 -> 2122050969920
	2122050973232 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050966992 -> 2122050973232
	2122050966992 -> 2122051780976 [dir=none]
	2122051780976 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122050966992 -> 2122051781360 [dir=none]
	2122051781360 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122050966992 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122050961952 -> 2122050966992
	2122050638608 [label="0.auto_model.encoder.layer.8.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050638608 -> 2122050961952
	2122050961952 [label=AccumulateGrad]
	2122050973760 -> 2122050966992
	2122050973760 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122050973808 -> 2122050973760
	2122050973808 -> 2122051199024 [dir=none]
	2122051199024 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122050973808 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122050977216 -> 2122050973808
	2122050977216 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122050963200 -> 2122050977216
	2122050963200 -> 2122051782128 [dir=none]
	2122051782128 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050963200 -> 2122051782512 [dir=none]
	2122051782512 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122050963200 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122050961712 -> 2122050963200
	2122050639952 [label="0.auto_model.encoder.layer.8.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122050639952 -> 2122050961712
	2122050961712 [label=AccumulateGrad]
	2122050972512 -> 2122050963200
	2122050972512 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050968240 -> 2122050972512
	2122050968240 -> 2122050648688 [dir=none]
	2122050648688 [label="bias
 (768)" fillcolor=orange]
	2122050968240 -> 2122051197680 [dir=none]
	2122051197680 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050968240 -> 2122051783376 [dir=none]
	2122051783376 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050968240 -> 2122051783568 [dir=none]
	2122051783568 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050968240 -> 2122050647440 [dir=none]
	2122050647440 [label="weight
 (768)" fillcolor=orange]
	2122050968240 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050974912 -> 2122050968240
	2122050974912 [label="AddBackward0
------------
alpha: 1"]
	2122050963632 -> 2122050974912
	2122050963632 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050964832 -> 2122050963632
	2122050964832 -> 2122051783952 [dir=none]
	2122051783952 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050964832 -> 2122051784336 [dir=none]
	2122051784336 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050964832 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050971696 -> 2122050964832
	2122050639280 [label="0.auto_model.encoder.layer.8.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050639280 -> 2122050971696
	2122050971696 [label=AccumulateGrad]
	2122050966608 -> 2122050964832
	2122050966608 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050971840 -> 2122050966608
	2122050971840 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122050964160 -> 2122050971840
	2122050964160 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050975632 -> 2122050964160
	2122050975632 -> 2122051198544 [dir=none]
	2122051198544 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122050975632 -> 2122051785296 [dir=none]
	2122051785296 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122050975632 -> 2122051785392 [dir=none]
	2122051785392 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122050975632 -> 2122051785584 [dir=none]
	2122051785584 [label="philox_offset
 ()" fillcolor=orange]
	2122050975632 -> 2122051785872 [dir=none]
	2122051785872 [label="philox_seed
 ()" fillcolor=orange]
	2122050975632 -> 2122051199216 [dir=none]
	2122051199216 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122050975632 -> 2122051198352 [dir=none]
	2122051198352 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122050975632 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122050973664 -> 2122050975632
	2122050973664 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050975968 -> 2122050973664
	2122050975968 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050962576 -> 2122050975968
	2122050962576 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050961808 -> 2122050962576
	2122050961808 -> 2122051786160 [dir=none]
	2122051786160 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050961808 -> 2122051786832 [dir=none]
	2122051786832 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050961808 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050962240 -> 2122050961808
	2122050640720 [label="0.auto_model.encoder.layer.8.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122050640720 -> 2122050962240
	2122050962240 [label=AccumulateGrad]
	2122050964256 -> 2122050961808
	2122050964256 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050977504 -> 2122050964256
	2122050977504 -> 2122050636304 [dir=none]
	2122050636304 [label="bias
 (768)" fillcolor=orange]
	2122050977504 -> 2122051200272 [dir=none]
	2122051200272 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050977504 -> 2122051787696 [dir=none]
	2122051787696 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050977504 -> 2122051787888 [dir=none]
	2122051787888 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050977504 -> 2122050646096 [dir=none]
	2122050646096 [label="weight
 (768)" fillcolor=orange]
	2122050977504 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050965168 -> 2122050977504
	2122050965168 [label="AddBackward0
------------
alpha: 1"]
	2122050963296 -> 2122050965168
	2122050963296 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050968432 -> 2122050963296
	2122050968432 -> 2122051788272 [dir=none]
	2122051788272 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122050968432 -> 2122051788656 [dir=none]
	2122051788656 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122050968432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122050968096 -> 2122050968432
	2122050646768 [label="0.auto_model.encoder.layer.7.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050646768 -> 2122050968096
	2122050968096 [label=AccumulateGrad]
	2122050967376 -> 2122050968432
	2122050967376 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122050971360 -> 2122050967376
	2122050971360 -> 2122051200560 [dir=none]
	2122051200560 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122050971360 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122050974816 -> 2122050971360
	2122050974816 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122050969584 -> 2122050974816
	2122050969584 -> 2122051789424 [dir=none]
	2122051789424 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050969584 -> 2122051789808 [dir=none]
	2122051789808 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122050969584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122050975920 -> 2122050969584
	2122050634768 [label="0.auto_model.encoder.layer.7.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122050634768 -> 2122050975920
	2122050975920 [label=AccumulateGrad]
	2122050976304 -> 2122050969584
	2122050976304 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050962480 -> 2122050976304
	2122050962480 -> 2122050636976 [dir=none]
	2122050636976 [label="bias
 (768)" fillcolor=orange]
	2122050962480 -> 2122051198064 [dir=none]
	2122051198064 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050962480 -> 2122051790672 [dir=none]
	2122051790672 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050962480 -> 2122051790864 [dir=none]
	2122051790864 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050962480 -> 2122050647728 [dir=none]
	2122050647728 [label="weight
 (768)" fillcolor=orange]
	2122050962480 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050974528 -> 2122050962480
	2122050974528 [label="AddBackward0
------------
alpha: 1"]
	2122050964400 -> 2122050974528
	2122050964400 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050961760 -> 2122050964400
	2122050961760 -> 2122051791248 [dir=none]
	2122051791248 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050961760 -> 2122051791632 [dir=none]
	2122051791632 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050961760 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050964112 -> 2122050961760
	2122050635440 [label="0.auto_model.encoder.layer.7.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050635440 -> 2122050964112
	2122050964112 [label=AccumulateGrad]
	2122050977360 -> 2122050961760
	2122050977360 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050970256 -> 2122050977360
	2122050970256 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122050971312 -> 2122050970256
	2122050971312 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050976736 -> 2122050971312
	2122050976736 -> 2122051197584 [dir=none]
	2122051197584 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122050976736 -> 2122051792592 [dir=none]
	2122051792592 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122050976736 -> 2122051792688 [dir=none]
	2122051792688 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122050976736 -> 2122051792880 [dir=none]
	2122051792880 [label="philox_offset
 ()" fillcolor=orange]
	2122050976736 -> 2122051793168 [dir=none]
	2122051793168 [label="philox_seed
 ()" fillcolor=orange]
	2122050976736 -> 2122051198160 [dir=none]
	2122051198160 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122050976736 -> 2122051199792 [dir=none]
	2122051199792 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122050976736 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122050970496 -> 2122050976736
	2122050970496 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050971456 -> 2122050970496
	2122050971456 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050962432 -> 2122050971456
	2122050962432 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050975584 -> 2122050962432
	2122050975584 -> 2122051793456 [dir=none]
	2122051793456 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050975584 -> 2122051794128 [dir=none]
	2122051794128 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050975584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050972320 -> 2122050975584
	2122050636208 [label="0.auto_model.encoder.layer.7.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122050636208 -> 2122050972320
	2122050972320 [label=AccumulateGrad]
	2122050965600 -> 2122050975584
	2122050965600 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050974576 -> 2122050965600
	2122050974576 -> 2122050636016 [dir=none]
	2122050636016 [label="bias
 (768)" fillcolor=orange]
	2122050974576 -> 2122051197776 [dir=none]
	2122051197776 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050974576 -> 2122051794992 [dir=none]
	2122051794992 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050974576 -> 2122051795184 [dir=none]
	2122051795184 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050974576 -> 2122050650032 [dir=none]
	2122050650032 [label="weight
 (768)" fillcolor=orange]
	2122050974576 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050968864 -> 2122050974576
	2122050968864 [label="AddBackward0
------------
alpha: 1"]
	2122050976400 -> 2122050968864
	2122050976400 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050964208 -> 2122050976400
	2122050964208 -> 2122051795568 [dir=none]
	2122051795568 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122050964208 -> 2122051795952 [dir=none]
	2122051795952 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122050964208 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122050974192 -> 2122050964208
	2122050636688 [label="0.auto_model.encoder.layer.6.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050636688 -> 2122050974192
	2122050974192 [label=AccumulateGrad]
	2122050972224 -> 2122050964208
	2122050972224 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122050962000 -> 2122050972224
	2122050962000 -> 2122051194800 [dir=none]
	2122051194800 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122050962000 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122050964544 -> 2122050962000
	2122050964544 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122050976448 -> 2122050964544
	2122050976448 -> 2122051796720 [dir=none]
	2122051796720 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050976448 -> 2122051305648 [dir=none]
	2122051305648 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122050976448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122050973904 -> 2122050976448
	2122050506096 [label="0.auto_model.encoder.layer.6.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122050506096 -> 2122050973904
	2122050973904 [label=AccumulateGrad]
	2122050968288 -> 2122050976448
	2122050968288 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050973184 -> 2122050968288
	2122050973184 -> 2122050506768 [dir=none]
	2122050506768 [label="bias
 (768)" fillcolor=orange]
	2122050973184 -> 2122051198640 [dir=none]
	2122051198640 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050973184 -> 2122051306512 [dir=none]
	2122051306512 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050973184 -> 2122051306704 [dir=none]
	2122051306704 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050973184 -> 2122050648880 [dir=none]
	2122050648880 [label="weight
 (768)" fillcolor=orange]
	2122050973184 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050962816 -> 2122050973184
	2122050962816 [label="AddBackward0
------------
alpha: 1"]
	2122050969104 -> 2122050962816
	2122050969104 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050977744 -> 2122050969104
	2122050977744 -> 2122051307088 [dir=none]
	2122051307088 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050977744 -> 2122051307472 [dir=none]
	2122051307472 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050977744 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050966704 -> 2122050977744
	2122050648400 [label="0.auto_model.encoder.layer.6.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050648400 -> 2122050966704
	2122050966704 [label=AccumulateGrad]
	2122050972176 -> 2122050977744
	2122050972176 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050973136 -> 2122050972176
	2122050973136 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122050968768 -> 2122050973136
	2122050968768 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050962912 -> 2122050968768
	2122050962912 -> 2122051199600 [dir=none]
	2122051199600 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122050962912 -> 2122051308432 [dir=none]
	2122051308432 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122050962912 -> 2122051308528 [dir=none]
	2122051308528 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122050962912 -> 2122051308720 [dir=none]
	2122051308720 [label="philox_offset
 ()" fillcolor=orange]
	2122050962912 -> 2122051309008 [dir=none]
	2122051309008 [label="philox_seed
 ()" fillcolor=orange]
	2122050962912 -> 2122051206320 [dir=none]
	2122051206320 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122050962912 -> 2122051197872 [dir=none]
	2122051197872 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122050962912 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122050977120 -> 2122050962912
	2122050977120 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050973616 -> 2122050977120
	2122050973616 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050973088 -> 2122050973616
	2122050973088 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050963968 -> 2122050973088
	2122050963968 -> 2122051309296 [dir=none]
	2122051309296 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050963968 -> 2122051309968 [dir=none]
	2122051309968 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050963968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050965552 -> 2122050963968
	2122050506288 [label="0.auto_model.encoder.layer.6.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122050506288 -> 2122050965552
	2122050965552 [label=AccumulateGrad]
	2122050961904 -> 2122050963968
	2122050961904 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050968528 -> 2122050961904
	2122050968528 -> 2122050507440 [dir=none]
	2122050507440 [label="bias
 (768)" fillcolor=orange]
	2122050968528 -> 2122051198256 [dir=none]
	2122051198256 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050968528 -> 2122051310832 [dir=none]
	2122051310832 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050968528 -> 2122051311024 [dir=none]
	2122051311024 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050968528 -> 2122050516080 [dir=none]
	2122050516080 [label="weight
 (768)" fillcolor=orange]
	2122050968528 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050965312 -> 2122050968528
	2122050965312 [label="AddBackward0
------------
alpha: 1"]
	2122050963248 -> 2122050965312
	2122050963248 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050963104 -> 2122050963248
	2122050963104 -> 2122051311408 [dir=none]
	2122051311408 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122050963104 -> 2122051311792 [dir=none]
	2122051311792 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122050963104 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122050969632 -> 2122050963104
	2122050506576 [label="0.auto_model.encoder.layer.5.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050506576 -> 2122050969632
	2122050969632 [label=AccumulateGrad]
	2122050977648 -> 2122050963104
	2122050977648 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122050969824 -> 2122050977648
	2122050969824 -> 2122051200176 [dir=none]
	2122051200176 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122050969824 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122050971408 -> 2122050969824
	2122050971408 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122050967136 -> 2122050971408
	2122050967136 -> 2122051312560 [dir=none]
	2122051312560 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050967136 -> 2122051312944 [dir=none]
	2122051312944 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122050967136 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122050962192 -> 2122050967136
	2122050508304 [label="0.auto_model.encoder.layer.5.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122050508304 -> 2122050962192
	2122050962192 [label=AccumulateGrad]
	2122050977072 -> 2122050967136
	2122050977072 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050969728 -> 2122050977072
	2122050969728 -> 2122050517328 [dir=none]
	2122050517328 [label="bias
 (768)" fillcolor=orange]
	2122050969728 -> 2122051200080 [dir=none]
	2122051200080 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050969728 -> 2122051313808 [dir=none]
	2122051313808 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050969728 -> 2122051314000 [dir=none]
	2122051314000 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050969728 -> 2122050507248 [dir=none]
	2122050507248 [label="weight
 (768)" fillcolor=orange]
	2122050969728 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050965984 -> 2122050969728
	2122050965984 [label="AddBackward0
------------
alpha: 1"]
	2122050966896 -> 2122050965984
	2122050966896 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050967088 -> 2122050966896
	2122050967088 -> 2122051314384 [dir=none]
	2122051314384 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050967088 -> 2122051314768 [dir=none]
	2122051314768 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050967088 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050971936 -> 2122050967088
	2122050507152 [label="0.auto_model.encoder.layer.5.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050507152 -> 2122050971936
	2122050971936 [label=AccumulateGrad]
	2122050968048 -> 2122050967088
	2122050968048 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050970736 -> 2122050968048
	2122050970736 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122050965120 -> 2122050970736
	2122050965120 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050961664 -> 2122050965120
	2122050961664 -> 2122051199888 [dir=none]
	2122051199888 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122050961664 -> 2122051315728 [dir=none]
	2122051315728 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122050961664 -> 2122051315824 [dir=none]
	2122051315824 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122050961664 -> 2122051316016 [dir=none]
	2122051316016 [label="philox_offset
 ()" fillcolor=orange]
	2122050961664 -> 2122051316304 [dir=none]
	2122051316304 [label="philox_seed
 ()" fillcolor=orange]
	2122050961664 -> 2122051195088 [dir=none]
	2122051195088 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122050961664 -> 2122051197392 [dir=none]
	2122051197392 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122050961664 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122050966272 -> 2122050961664
	2122050966272 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050969680 -> 2122050966272
	2122050969680 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050962144 -> 2122050969680
	2122050962144 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050963056 -> 2122050962144
	2122050963056 -> 2122051316592 [dir=none]
	2122051316592 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050963056 -> 2122051317264 [dir=none]
	2122051317264 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050963056 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050973376 -> 2122050963056
	2122050517808 [label="0.auto_model.encoder.layer.5.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122050517808 -> 2122050973376
	2122050973376 [label=AccumulateGrad]
	2122050974240 -> 2122050963056
	2122050974240 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050963536 -> 2122050974240
	2122050963536 -> 2122050517136 [dir=none]
	2122050517136 [label="bias
 (768)" fillcolor=orange]
	2122050963536 -> 2122051206704 [dir=none]
	2122051206704 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050963536 -> 2122051318128 [dir=none]
	2122051318128 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050963536 -> 2122051318320 [dir=none]
	2122051318320 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050963536 -> 2122050517712 [dir=none]
	2122050517712 [label="weight
 (768)" fillcolor=orange]
	2122050963536 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050965072 -> 2122050963536
	2122050965072 [label="AddBackward0
------------
alpha: 1"]
	2122050964880 -> 2122050965072
	2122050964880 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050969776 -> 2122050964880
	2122050969776 -> 2122051318704 [dir=none]
	2122051318704 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122050969776 -> 2122051319088 [dir=none]
	2122051319088 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122050969776 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122050966368 -> 2122050969776
	2122050517232 [label="0.auto_model.encoder.layer.4.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050517232 -> 2122050966368
	2122050966368 [label=AccumulateGrad]
	2122050972080 -> 2122050969776
	2122050972080 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122050968384 -> 2122050972080
	2122050968384 -> 2122051195280 [dir=none]
	2122051195280 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122050968384 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122050975200 -> 2122050968384
	2122050975200 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122050972464 -> 2122050975200
	2122050972464 -> 2122051319856 [dir=none]
	2122051319856 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050972464 -> 2122051320240 [dir=none]
	2122051320240 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122050972464 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122050969968 -> 2122050972464
	2122050516464 [label="0.auto_model.encoder.layer.4.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122050516464 -> 2122050969968
	2122050969968 [label=AccumulateGrad]
	2122050971792 -> 2122050972464
	2122050971792 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050968192 -> 2122050971792
	2122050968192 -> 2122050505808 [dir=none]
	2122050505808 [label="bias
 (768)" fillcolor=orange]
	2122050968192 -> 2122051195184 [dir=none]
	2122051195184 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050968192 -> 2122051321104 [dir=none]
	2122051321104 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050968192 -> 2122051321296 [dir=none]
	2122051321296 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050968192 -> 2122050516560 [dir=none]
	2122050516560 [label="weight
 (768)" fillcolor=orange]
	2122050968192 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050971984 -> 2122050968192
	2122050971984 [label="AddBackward0
------------
alpha: 1"]
	2122050972272 -> 2122050971984
	2122050972272 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050965648 -> 2122050972272
	2122050965648 -> 2122051321680 [dir=none]
	2122051321680 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050965648 -> 2122051862800 [dir=none]
	2122051862800 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050965648 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050971168 -> 2122050965648
	2122050516944 [label="0.auto_model.encoder.layer.4.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050516944 -> 2122050971168
	2122050971168 [label=AccumulateGrad]
	2122050963488 -> 2122050965648
	2122050963488 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050965408 -> 2122050963488
	2122050965408 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122050963008 -> 2122050965408
	2122050963008 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050967424 -> 2122050963008
	2122050967424 -> 2122051195760 [dir=none]
	2122051195760 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122050967424 -> 2122051863760 [dir=none]
	2122051863760 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122050967424 -> 2122051863856 [dir=none]
	2122051863856 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122050967424 -> 2122051864048 [dir=none]
	2122051864048 [label="philox_offset
 ()" fillcolor=orange]
	2122050967424 -> 2122051864336 [dir=none]
	2122051864336 [label="philox_seed
 ()" fillcolor=orange]
	2122050967424 -> 2122051196048 [dir=none]
	2122051196048 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122050967424 -> 2122051195568 [dir=none]
	2122051195568 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122050967424 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122050962528 -> 2122050967424
	2122050962528 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050691536 -> 2122050962528
	2122050691536 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050698160 -> 2122050691536
	2122050698160 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050694560 -> 2122050698160
	2122050694560 -> 2122051864624 [dir=none]
	2122051864624 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050694560 -> 2122051865296 [dir=none]
	2122051865296 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050694560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050693936 -> 2122050694560
	2122050515984 [label="0.auto_model.encoder.layer.4.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122050515984 -> 2122050693936
	2122050693936 [label=AccumulateGrad]
	2122050690624 -> 2122050694560
	2122050690624 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050968336 -> 2122050690624
	2122050968336 -> 2122050512240 [dir=none]
	2122050512240 [label="bias
 (768)" fillcolor=orange]
	2122050968336 -> 2122051196240 [dir=none]
	2122051196240 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050968336 -> 2122051866160 [dir=none]
	2122051866160 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050968336 -> 2122051866352 [dir=none]
	2122051866352 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050968336 -> 2122050513008 [dir=none]
	2122050513008 [label="weight
 (768)" fillcolor=orange]
	2122050968336 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050695376 -> 2122050968336
	2122050695376 [label="AddBackward0
------------
alpha: 1"]
	2122050688368 -> 2122050695376
	2122050688368 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050695760 -> 2122050688368
	2122050695760 -> 2122051866736 [dir=none]
	2122051866736 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122050695760 -> 2122051867120 [dir=none]
	2122051867120 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122050695760 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122050684096 -> 2122050695760
	2122050505904 [label="0.auto_model.encoder.layer.3.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050505904 -> 2122050684096
	2122050684096 [label=AccumulateGrad]
	2122050688992 -> 2122050695760
	2122050688992 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122050694224 -> 2122050688992
	2122050694224 -> 2122051196720 [dir=none]
	2122051196720 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122050694224 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122050694176 -> 2122050694224
	2122050694176 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122050685488 -> 2122050694176
	2122050685488 -> 2122051867888 [dir=none]
	2122051867888 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050685488 -> 2122051868272 [dir=none]
	2122051868272 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122050685488 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122050696480 -> 2122050685488
	2122050513488 [label="0.auto_model.encoder.layer.3.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122050513488 -> 2122050696480
	2122050696480 [label=AccumulateGrad]
	2122050696864 -> 2122050685488
	2122050696864 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050695232 -> 2122050696864
	2122050695232 -> 2122050514448 [dir=none]
	2122050514448 [label="bias
 (768)" fillcolor=orange]
	2122050695232 -> 2122051196432 [dir=none]
	2122051196432 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050695232 -> 2122051869136 [dir=none]
	2122051869136 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050695232 -> 2122051869328 [dir=none]
	2122051869328 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050695232 -> 2122050508016 [dir=none]
	2122050508016 [label="weight
 (768)" fillcolor=orange]
	2122050695232 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050693360 -> 2122050695232
	2122050693360 [label="AddBackward0
------------
alpha: 1"]
	2122050683952 -> 2122050693360
	2122050683952 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050689472 -> 2122050683952
	2122050689472 -> 2122051869712 [dir=none]
	2122051869712 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050689472 -> 2122051870096 [dir=none]
	2122051870096 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050689472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050683760 -> 2122050689472
	2122050512336 [label="0.auto_model.encoder.layer.3.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050512336 -> 2122050683760
	2122050683760 [label=AccumulateGrad]
	2122050691344 -> 2122050689472
	2122050691344 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050685680 -> 2122050691344
	2122050685680 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122050687792 -> 2122050685680
	2122050687792 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050695280 -> 2122050687792
	2122050695280 -> 2122051196912 [dir=none]
	2122051196912 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122050695280 -> 2122051871056 [dir=none]
	2122051871056 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122050695280 -> 2122051871152 [dir=none]
	2122051871152 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122050695280 -> 2122051871344 [dir=none]
	2122051871344 [label="philox_offset
 ()" fillcolor=orange]
	2122050695280 -> 2122051871632 [dir=none]
	2122051871632 [label="philox_seed
 ()" fillcolor=orange]
	2122050695280 -> 2122051197008 [dir=none]
	2122051197008 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122050695280 -> 2122051196624 [dir=none]
	2122051196624 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122050695280 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122050699168 -> 2122050695280
	2122050699168 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050693312 -> 2122050699168
	2122050693312 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050698880 -> 2122050693312
	2122050698880 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050693840 -> 2122050698880
	2122050693840 -> 2122051871920 [dir=none]
	2122051871920 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050693840 -> 2122051872592 [dir=none]
	2122051872592 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050693840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050689040 -> 2122050693840
	2122050513200 [label="0.auto_model.encoder.layer.3.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122050513200 -> 2122050689040
	2122050689040 [label=AccumulateGrad]
	2122050695472 -> 2122050693840
	2122050695472 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050690672 -> 2122050695472
	2122050690672 -> 2122050514256 [dir=none]
	2122050514256 [label="bias
 (768)" fillcolor=orange]
	2122050690672 -> 2122051197296 [dir=none]
	2122051197296 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050690672 -> 2122051873456 [dir=none]
	2122051873456 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050690672 -> 2122051873648 [dir=none]
	2122051873648 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050690672 -> 2122050514352 [dir=none]
	2122050514352 [label="weight
 (768)" fillcolor=orange]
	2122050690672 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050692064 -> 2122050690672
	2122050692064 [label="AddBackward0
------------
alpha: 1"]
	2122050684288 -> 2122050692064
	2122050684288 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050686160 -> 2122050684288
	2122050686160 -> 2122051874032 [dir=none]
	2122051874032 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122050686160 -> 2122051874416 [dir=none]
	2122051874416 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122050686160 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122050685152 -> 2122050686160
	2122050513872 [label="0.auto_model.encoder.layer.2.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050513872 -> 2122050685152
	2122050685152 [label=AccumulateGrad]
	2122050691056 -> 2122050686160
	2122050691056 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122050695664 -> 2122050691056
	2122050695664 -> 2122051194512 [dir=none]
	2122051194512 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122050695664 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122050691584 -> 2122050695664
	2122050691584 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122050684864 -> 2122050691584
	2122050684864 -> 2122051875184 [dir=none]
	2122051875184 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050684864 -> 2122051875568 [dir=none]
	2122051875568 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122050684864 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122050687552 -> 2122050684864
	2122050512720 [label="0.auto_model.encoder.layer.2.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122050512720 -> 2122050687552
	2122050687552 [label=AccumulateGrad]
	2122050690144 -> 2122050684864
	2122050690144 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050692352 -> 2122050690144
	2122050692352 -> 2122050634864 [dir=none]
	2122050634864 [label="bias
 (768)" fillcolor=orange]
	2122050692352 -> 2122051194608 [dir=none]
	2122051194608 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050692352 -> 2122051876432 [dir=none]
	2122051876432 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050692352 -> 2122051876624 [dir=none]
	2122051876624 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050692352 -> 2122050517616 [dir=none]
	2122050517616 [label="weight
 (768)" fillcolor=orange]
	2122050692352 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050686784 -> 2122050692352
	2122050686784 [label="AddBackward0
------------
alpha: 1"]
	2122050697392 -> 2122050686784
	2122050697392 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050687936 -> 2122050697392
	2122050687936 -> 2122051877008 [dir=none]
	2122051877008 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050687936 -> 2122051877392 [dir=none]
	2122051877392 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050687936 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050692304 -> 2122050687936
	2122050506192 [label="0.auto_model.encoder.layer.2.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050506192 -> 2122050692304
	2122050692304 [label=AccumulateGrad]
	2122050697680 -> 2122050687936
	2122050697680 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050691920 -> 2122050697680
	2122050691920 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122050698640 -> 2122050691920
	2122050698640 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050692160 -> 2122050698640
	2122050692160 -> 2122051194128 [dir=none]
	2122051194128 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122050692160 -> 2122051878352 [dir=none]
	2122051878352 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122050692160 -> 2122051878448 [dir=none]
	2122051878448 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122050692160 -> 2122051878640 [dir=none]
	2122051878640 [label="philox_offset
 ()" fillcolor=orange]
	2122050692160 -> 2122051878832 [dir=none]
	2122051878832 [label="philox_seed
 ()" fillcolor=orange]
	2122050692160 -> 2122051193936 [dir=none]
	2122051193936 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122050692160 -> 2122051194416 [dir=none]
	2122051194416 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122050692160 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122050692976 -> 2122050692160
	2122050692976 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050690096 -> 2122050692976
	2122050690096 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050685920 -> 2122050690096
	2122050685920 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050697824 -> 2122050685920
	2122050697824 -> 2122051699056 [dir=none]
	2122051699056 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050697824 -> 2122051699728 [dir=none]
	2122051699728 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050697824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050696288 -> 2122050697824
	2122050506384 [label="0.auto_model.encoder.layer.2.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122050506384 -> 2122050696288
	2122050696288 [label=AccumulateGrad]
	2122050692544 -> 2122050697824
	2122050692544 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050695712 -> 2122050692544
	2122050695712 -> 2122050508592 [dir=none]
	2122050508592 [label="bias
 (768)" fillcolor=orange]
	2122050695712 -> 2122051193744 [dir=none]
	2122051193744 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050695712 -> 2122051700592 [dir=none]
	2122051700592 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050695712 -> 2122051700784 [dir=none]
	2122051700784 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050695712 -> 2122050516272 [dir=none]
	2122050516272 [label="weight
 (768)" fillcolor=orange]
	2122050695712 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050698736 -> 2122050695712
	2122050698736 [label="AddBackward0
------------
alpha: 1"]
	2122050689568 -> 2122050698736
	2122050689568 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050687648 -> 2122050689568
	2122050687648 -> 2122051701168 [dir=none]
	2122051701168 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122050687648 -> 2122051701552 [dir=none]
	2122051701552 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122050687648 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122050684048 -> 2122050687648
	2122050516656 [label="0.auto_model.encoder.layer.1.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050516656 -> 2122050684048
	2122050684048 [label=AccumulateGrad]
	2122050692880 -> 2122050687648
	2122050692880 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122050688224 -> 2122050692880
	2122050688224 -> 2122051191248 [dir=none]
	2122051191248 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122050688224 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122050697872 -> 2122050688224
	2122050697872 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122050688656 -> 2122050697872
	2122050688656 -> 2122051702320 [dir=none]
	2122051702320 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050688656 -> 2122051702704 [dir=none]
	2122051702704 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122050688656 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122050695136 -> 2122050688656
	2122051598960 [label="0.auto_model.encoder.layer.1.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122051598960 -> 2122050695136
	2122050695136 [label=AccumulateGrad]
	2122050698400 -> 2122050688656
	2122050698400 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050691680 -> 2122050698400
	2122050691680 -> 2122051598192 [dir=none]
	2122051598192 [label="bias
 (768)" fillcolor=orange]
	2122050691680 -> 2122051193552 [dir=none]
	2122051193552 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050691680 -> 2122051703568 [dir=none]
	2122051703568 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050691680 -> 2122051703760 [dir=none]
	2122051703760 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050691680 -> 2122050506672 [dir=none]
	2122050506672 [label="weight
 (768)" fillcolor=orange]
	2122050691680 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050695424 -> 2122050691680
	2122050695424 [label="AddBackward0
------------
alpha: 1"]
	2122050683424 -> 2122050695424
	2122050683424 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050693648 -> 2122050683424
	2122050693648 -> 2122051704144 [dir=none]
	2122051704144 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050693648 -> 2122051704528 [dir=none]
	2122051704528 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050693648 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050685440 -> 2122050693648
	2122050506960 [label="0.auto_model.encoder.layer.1.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122050506960 -> 2122050685440
	2122050685440 [label=AccumulateGrad]
	2122050690432 -> 2122050693648
	2122050690432 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050697200 -> 2122050690432
	2122050697200 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122050689616 -> 2122050697200
	2122050689616 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050698928 -> 2122050689616
	2122050698928 -> 2122051191536 [dir=none]
	2122051191536 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122050698928 -> 2122051705488 [dir=none]
	2122051705488 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122050698928 -> 2122051705584 [dir=none]
	2122051705584 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122050698928 -> 2122051705776 [dir=none]
	2122051705776 [label="philox_offset
 ()" fillcolor=orange]
	2122050698928 -> 2122051706064 [dir=none]
	2122051706064 [label="philox_seed
 ()" fillcolor=orange]
	2122050698928 -> 2122051192016 [dir=none]
	2122051192016 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122050698928 -> 2122051191344 [dir=none]
	2122051191344 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122050698928 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122050696000 -> 2122050698928
	2122050696000 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050694848 -> 2122050696000
	2122050694848 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050697728 -> 2122050694848
	2122050697728 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050685248 -> 2122050697728
	2122050685248 -> 2122051706352 [dir=none]
	2122051706352 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050685248 -> 2122051707024 [dir=none]
	2122051707024 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050685248 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050687600 -> 2122050685248
	2122051598768 [label="0.auto_model.encoder.layer.1.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122051598768 -> 2122050687600
	2122050687600 [label=AccumulateGrad]
	2122050691104 -> 2122050685248
	2122050691104 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050690384 -> 2122050691104
	2122050690384 -> 2122051598000 [dir=none]
	2122051598000 [label="bias
 (768)" fillcolor=orange]
	2122050690384 -> 2122051192112 [dir=none]
	2122051192112 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050690384 -> 2122051707888 [dir=none]
	2122051707888 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050690384 -> 2122051708080 [dir=none]
	2122051708080 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050690384 -> 2122051597904 [dir=none]
	2122051597904 [label="weight
 (768)" fillcolor=orange]
	2122050690384 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050696672 -> 2122050690384
	2122050696672 [label="AddBackward0
------------
alpha: 1"]
	2122050693984 -> 2122050696672
	2122050693984 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050692688 -> 2122050693984
	2122050692688 -> 2122051708464 [dir=none]
	2122051708464 [label="mat1
 (8, 3072)" fillcolor=orange]
	2122050692688 -> 2122051708848 [dir=none]
	2122051708848 [label="mat2
 (3072, 768)" fillcolor=orange]
	2122050692688 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (8, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	2122050693792 -> 2122050692688
	2122051598384 [label="0.auto_model.encoder.layer.0.output.dense.bias
 (768)" fillcolor=lightblue]
	2122051598384 -> 2122050693792
	2122050693792 [label=AccumulateGrad]
	2122050697968 -> 2122050692688
	2122050697968 [label="ViewBackward0
----------------------------
self_sym_sizes: (1, 8, 3072)"]
	2122050691008 -> 2122050697968
	2122050691008 -> 2122051192208 [dir=none]
	2122051192208 [label="self
 (1, 8, 3072)" fillcolor=orange]
	2122050691008 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	2122050685056 -> 2122050691008
	2122050685056 [label="ViewBackward0
-------------------------
self_sym_sizes: (8, 3072)"]
	2122050695184 -> 2122050685056
	2122050695184 -> 2122051709616 [dir=none]
	2122051709616 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050695184 -> 2122051710000 [dir=none]
	2122051710000 [label="mat2
 (768, 3072)" fillcolor=orange]
	2122050695184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	2122050689664 -> 2122050695184
	2122051597424 [label="0.auto_model.encoder.layer.0.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	2122051597424 -> 2122050689664
	2122050689664 [label=AccumulateGrad]
	2122050686256 -> 2122050695184
	2122050686256 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050684672 -> 2122050686256
	2122050684672 -> 2122051596272 [dir=none]
	2122051596272 [label="bias
 (768)" fillcolor=orange]
	2122050684672 -> 2122051191920 [dir=none]
	2122051191920 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050684672 -> 2122051710864 [dir=none]
	2122051710864 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050684672 -> 2122051711056 [dir=none]
	2122051711056 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050684672 -> 2122051596176 [dir=none]
	2122051596176 [label="weight
 (768)" fillcolor=orange]
	2122050684672 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050689184 -> 2122050684672
	2122050689184 [label="AddBackward0
------------
alpha: 1"]
	2122050698784 -> 2122050689184
	2122050698784 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050696768 -> 2122050698784
	2122050696768 -> 2122051711440 [dir=none]
	2122051711440 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050696768 -> 2122051711824 [dir=none]
	2122051711824 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050696768 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050686544 -> 2122050696768
	2122051597808 [label="0.auto_model.encoder.layer.0.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	2122051597808 -> 2122050686544
	2122050686544 [label=AccumulateGrad]
	2122050696144 -> 2122050696768
	2122050696144 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050695088 -> 2122050696144
	2122050695088 [label="ViewBackward0
------------------------------
self_sym_sizes: (1, 8, 12, 64)"]
	2122050687888 -> 2122050695088
	2122050687888 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050694992 -> 2122050687888
	2122050694992 -> 2122051192496 [dir=none]
	2122051192496 [label="key
 (1, 12, 8, 64)" fillcolor=orange]
	2122050694992 -> 2122051712784 [dir=none]
	2122051712784 [label="log_sumexp
 (1, 12, 32)" fillcolor=orange]
	2122050694992 -> 2122051712880 [dir=none]
	2122051712880 [label="output
 (1, 12, 8, 64)" fillcolor=orange]
	2122050694992 -> 2122051713072 [dir=none]
	2122051713072 [label="philox_offset
 ()" fillcolor=orange]
	2122050694992 -> 2122051713360 [dir=none]
	2122051713360 [label="philox_seed
 ()" fillcolor=orange]
	2122050694992 -> 2122051192592 [dir=none]
	2122051192592 [label="query
 (1, 12, 8, 64)" fillcolor=orange]
	2122050694992 -> 2122051192304 [dir=none]
	2122051192304 [label="value
 (1, 12, 8, 64)" fillcolor=orange]
	2122050694992 [label="ScaledDotProductEfficientAttentionBackward0
-------------------------------------------
attn_bias    :           None
dropout_p    :            0.0
is_causal    :          False
key          : [saved tensor]
log_sumexp   : [saved tensor]
output       : [saved tensor]
philox_offset: [saved tensor]
philox_seed  : [saved tensor]
query        : [saved tensor]
scale        :           None
value        : [saved tensor]"]
	2122050684528 -> 2122050694992
	2122050684528 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050688272 -> 2122050684528
	2122050688272 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050695520 -> 2122050688272
	2122050695520 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050692448 -> 2122050695520
	2122050692448 -> 2122051713648 [dir=none]
	2122051713648 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050692448 -> 2122051714320 [dir=none]
	2122051714320 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050692448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050683472 -> 2122050692448
	2122051597232 [label="0.auto_model.encoder.layer.0.attention.self.query.bias
 (768)" fillcolor=lightblue]
	2122051597232 -> 2122050683472
	2122050683472 [label=AccumulateGrad]
	2122050696432 -> 2122050692448
	2122050696432 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050698496 -> 2122050696432
	2122050698496 -> 2122051589648 [dir=none]
	2122051589648 [label="bias
 (768)" fillcolor=orange]
	2122050698496 -> 2122051192784 [dir=none]
	2122051192784 [label="input
 (1, 8, 768)" fillcolor=orange]
	2122050698496 -> 2122051666096 [dir=none]
	2122051666096 [label="result1
 (1, 8, 1)" fillcolor=orange]
	2122050698496 -> 2122051666288 [dir=none]
	2122051666288 [label="result2
 (1, 8, 1)" fillcolor=orange]
	2122050698496 -> 2122051590128 [dir=none]
	2122051590128 [label="weight
 (768)" fillcolor=orange]
	2122050698496 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	2122050685296 -> 2122050698496
	2122050685296 [label="AddBackward0
------------
alpha: 1"]
	2122050697008 -> 2122050685296
	2122050697008 [label="AddBackward0
------------
alpha: 1"]
	2122050691824 -> 2122050697008
	2122050691824 -> 2122051193360 [dir=none]
	2122051193360 [label="indices
 (1, 8)" fillcolor=orange]
	2122050691824 [label="EmbeddingBackward0
------------------------------------
indices             : [saved tensor]
padding_idx         :              0
scale_grad_by_freq  :          False
sparse              :          False
weight_sym_argsize_0:          32032"]
	2122050696960 -> 2122050691824
	2122051596464 [label="0.auto_model.embeddings.word_embeddings.weight
 (32032, 768)" fillcolor=lightblue]
	2122051596464 -> 2122050696960
	2122050696960 [label=AccumulateGrad]
	2122050690912 -> 2122050697008
	2122050690912 -> 2122051193168 [dir=none]
	2122051193168 [label="indices
 (1, 8)" fillcolor=orange]
	2122050690912 [label="EmbeddingBackward0
------------------------------------
indices             : [saved tensor]
padding_idx         :     4294967295
scale_grad_by_freq  :          False
sparse              :          False
weight_sym_argsize_0:              2"]
	2122050693600 -> 2122050690912
	2122051596368 [label="0.auto_model.embeddings.token_type_embeddings.weight
 (2, 768)" fillcolor=lightblue]
	2122051596368 -> 2122050693600
	2122050693600 [label=AccumulateGrad]
	2122050688128 -> 2122050685296
	2122050688128 -> 2122051193648 [dir=none]
	2122051193648 [label="indices
 (1, 8)" fillcolor=orange]
	2122050688128 [label="EmbeddingBackward0
------------------------------------
indices             : [saved tensor]
padding_idx         :     4294967295
scale_grad_by_freq  :          False
sparse              :          False
weight_sym_argsize_0:            512"]
	2122050690576 -> 2122050688128
	2122051596560 [label="0.auto_model.embeddings.position_embeddings.weight
 (512, 768)" fillcolor=lightblue]
	2122051596560 -> 2122050690576
	2122050690576 [label=AccumulateGrad]
	2122050696816 -> 2122050698496
	2122051590128 [label="0.auto_model.embeddings.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122051590128 -> 2122050696816
	2122050696816 [label=AccumulateGrad]
	2122050686880 -> 2122050698496
	2122051589648 [label="0.auto_model.embeddings.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122051589648 -> 2122050686880
	2122050686880 [label=AccumulateGrad]
	2122050696720 -> 2122050692448
	2122050696720 [label=TBackward0]
	2122050687840 -> 2122050696720
	2122051597136 [label="0.auto_model.encoder.layer.0.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122051597136 -> 2122050687840
	2122050687840 [label=AccumulateGrad]
	2122050697248 -> 2122050694992
	2122050697248 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050685104 -> 2122050697248
	2122050685104 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050688080 -> 2122050685104
	2122050688080 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050683088 -> 2122050688080
	2122050683088 -> 2122051669264 [dir=none]
	2122051669264 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050683088 -> 2122051669840 [dir=none]
	2122051669840 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050683088 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050698448 -> 2122050683088
	2122051597616 [label="0.auto_model.encoder.layer.0.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122051597616 -> 2122050698448
	2122050698448 [label=AccumulateGrad]
	2122050699120 -> 2122050683088
	2122050699120 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050698496 -> 2122050699120
	2122050685536 -> 2122050683088
	2122050685536 [label=TBackward0]
	2122050692736 -> 2122050685536
	2122051597520 [label="0.auto_model.encoder.layer.0.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122051597520 -> 2122050692736
	2122050692736 [label=AccumulateGrad]
	2122050691968 -> 2122050694992
	2122050691968 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050691776 -> 2122050691968
	2122050691776 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050686736 -> 2122050691776
	2122050686736 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050690960 -> 2122050686736
	2122050690960 -> 2122051670992 [dir=none]
	2122051670992 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050690960 -> 2122051671568 [dir=none]
	2122051671568 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050690960 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050694080 -> 2122050690960
	2122051597040 [label="0.auto_model.encoder.layer.0.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122051597040 -> 2122050694080
	2122050694080 [label=AccumulateGrad]
	2122050690288 -> 2122050690960
	2122050690288 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050698496 -> 2122050690288
	2122050690864 -> 2122050690960
	2122050690864 [label=TBackward0]
	2122050697584 -> 2122050690864
	2122051596944 [label="0.auto_model.encoder.layer.0.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122051596944 -> 2122050697584
	2122050697584 [label=AccumulateGrad]
	2122050694752 -> 2122050696768
	2122050694752 [label=TBackward0]
	2122050697536 -> 2122050694752
	2122051597712 [label="0.auto_model.encoder.layer.0.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122051597712 -> 2122050697536
	2122050697536 [label=AccumulateGrad]
	2122050698496 -> 2122050689184
	2122050684816 -> 2122050684672
	2122051596176 [label="0.auto_model.encoder.layer.0.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122051596176 -> 2122050684816
	2122050684816 [label=AccumulateGrad]
	2122050683136 -> 2122050684672
	2122051596272 [label="0.auto_model.encoder.layer.0.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122051596272 -> 2122050683136
	2122050683136 [label=AccumulateGrad]
	2122050686208 -> 2122050695184
	2122050686208 [label=TBackward0]
	2122050688944 -> 2122050686208
	2122051597328 [label="0.auto_model.encoder.layer.0.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122051597328 -> 2122050688944
	2122050688944 [label=AccumulateGrad]
	2122050698688 -> 2122050692688
	2122050698688 [label=TBackward0]
	2122050684432 -> 2122050698688
	2122051598288 [label="0.auto_model.encoder.layer.0.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122051598288 -> 2122050684432
	2122050684432 [label=AccumulateGrad]
	2122050684672 -> 2122050696672
	2122050691872 -> 2122050690384
	2122051597904 [label="0.auto_model.encoder.layer.0.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122051597904 -> 2122050691872
	2122050691872 [label=AccumulateGrad]
	2122050697104 -> 2122050690384
	2122051598000 [label="0.auto_model.encoder.layer.0.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122051598000 -> 2122050697104
	2122050697104 [label=AccumulateGrad]
	2122050689088 -> 2122050685248
	2122050689088 [label=TBackward0]
	2122050689328 -> 2122050689088
	2122051598672 [label="0.auto_model.encoder.layer.1.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122051598672 -> 2122050689328
	2122050689328 [label=AccumulateGrad]
	2122050694704 -> 2122050698928
	2122050694704 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050688848 -> 2122050694704
	2122050688848 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050696912 -> 2122050688848
	2122050696912 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050688608 -> 2122050696912
	2122050688608 -> 2122051675984 [dir=none]
	2122051675984 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050688608 -> 2122051676560 [dir=none]
	2122051676560 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050688608 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050692592 -> 2122050688608
	2121924143696 [label="0.auto_model.encoder.layer.1.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2121924143696 -> 2122050692592
	2122050692592 [label=AccumulateGrad]
	2122050694896 -> 2122050688608
	2122050694896 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050690384 -> 2122050694896
	2122050688896 -> 2122050688608
	2122050688896 [label=TBackward0]
	2122050693072 -> 2122050688896
	2122051598096 [label="0.auto_model.encoder.layer.1.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122051598096 -> 2122050693072
	2122050693072 [label=AccumulateGrad]
	2122050692400 -> 2122050698928
	2122050692400 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050691296 -> 2122050692400
	2122050691296 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050689280 -> 2122050691296
	2122050689280 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050686112 -> 2122050689280
	2122050686112 -> 2122051677712 [dir=none]
	2122051677712 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050686112 -> 2122051678288 [dir=none]
	2122051678288 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050686112 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050694464 -> 2122050686112
	2122051598576 [label="0.auto_model.encoder.layer.1.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122051598576 -> 2122050694464
	2122050694464 [label=AccumulateGrad]
	2122050696384 -> 2122050686112
	2122050696384 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050690384 -> 2122050696384
	2122050685344 -> 2122050686112
	2122050685344 [label=TBackward0]
	2122050682992 -> 2122050685344
	2122051598480 [label="0.auto_model.encoder.layer.1.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122051598480 -> 2122050682992
	2122050682992 [label=AccumulateGrad]
	2122050698592 -> 2122050693648
	2122050698592 [label=TBackward0]
	2122050693168 -> 2122050698592
	2122050516848 [label="0.auto_model.encoder.layer.1.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050516848 -> 2122050693168
	2122050693168 [label=AccumulateGrad]
	2122050690384 -> 2122050695424
	2122050690480 -> 2122050691680
	2122050506672 [label="0.auto_model.encoder.layer.1.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050506672 -> 2122050690480
	2122050690480 [label=AccumulateGrad]
	2122050684960 -> 2122050691680
	2122051598192 [label="0.auto_model.encoder.layer.1.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122051598192 -> 2122050684960
	2122050684960 [label=AccumulateGrad]
	2122050685632 -> 2122050688656
	2122050685632 [label=TBackward0]
	2122050697632 -> 2122050685632
	2122050507824 [label="0.auto_model.encoder.layer.1.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050507824 -> 2122050697632
	2122050697632 [label=AccumulateGrad]
	2122050696528 -> 2122050687648
	2122050696528 [label=TBackward0]
	2122050686352 -> 2122050696528
	2122050517520 [label="0.auto_model.encoder.layer.1.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050517520 -> 2122050686352
	2122050686352 [label=AccumulateGrad]
	2122050691680 -> 2122050698736
	2122050695616 -> 2122050695712
	2122050516272 [label="0.auto_model.encoder.layer.1.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050516272 -> 2122050695616
	2122050695616 [label=AccumulateGrad]
	2122050688704 -> 2122050695712
	2122050508592 [label="0.auto_model.encoder.layer.1.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050508592 -> 2122050688704
	2122050688704 [label=AccumulateGrad]
	2122050683664 -> 2122050697824
	2122050683664 [label=TBackward0]
	2122050695568 -> 2122050683664
	2122050507920 [label="0.auto_model.encoder.layer.2.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122050507920 -> 2122050695568
	2122050695568 [label=AccumulateGrad]
	2122050683328 -> 2122050692160
	2122050683328 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050689136 -> 2122050683328
	2122050689136 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050698976 -> 2122050689136
	2122050698976 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050684480 -> 2122050698976
	2122050684480 -> 2122051650000 [dir=none]
	2122051650000 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050684480 -> 2122051650576 [dir=none]
	2122051650576 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050684480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050693888 -> 2122050684480
	2122050513296 [label="0.auto_model.encoder.layer.2.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122050513296 -> 2122050693888
	2122050693888 [label=AccumulateGrad]
	2122050683568 -> 2122050684480
	2122050683568 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050695712 -> 2122050683568
	2122050697296 -> 2122050684480
	2122050697296 [label=TBackward0]
	2122050684384 -> 2122050697296
	2122050512624 [label="0.auto_model.encoder.layer.2.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122050512624 -> 2122050684384
	2122050684384 [label=AccumulateGrad]
	2122050684768 -> 2122050692160
	2122050684768 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050690240 -> 2122050684768
	2122050690240 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050687456 -> 2122050690240
	2122050687456 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050698304 -> 2122050687456
	2122050698304 -> 2122051651728 [dir=none]
	2122051651728 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050698304 -> 2122051652304 [dir=none]
	2122051652304 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050698304 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050698352 -> 2122050698304
	2122050518288 [label="0.auto_model.encoder.layer.2.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122050518288 -> 2122050698352
	2122050698352 [label=AccumulateGrad]
	2122050691632 -> 2122050698304
	2122050691632 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050695712 -> 2122050691632
	2122050686976 -> 2122050698304
	2122050686976 [label=TBackward0]
	2122050696240 -> 2122050686976
	2122050513680 [label="0.auto_model.encoder.layer.2.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122050513680 -> 2122050696240
	2122050696240 [label=AccumulateGrad]
	2122050684240 -> 2122050687936
	2122050684240 [label=TBackward0]
	2122050689520 -> 2122050684240
	2122050513968 [label="0.auto_model.encoder.layer.2.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050513968 -> 2122050689520
	2122050689520 [label=AccumulateGrad]
	2122050695712 -> 2122050686784
	2122050683712 -> 2122050692352
	2122050517616 [label="0.auto_model.encoder.layer.2.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050517616 -> 2122050683712
	2122050683712 [label=AccumulateGrad]
	2122050684192 -> 2122050692352
	2122050634864 [label="0.auto_model.encoder.layer.2.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050634864 -> 2122050684192
	2122050684192 [label=AccumulateGrad]
	2122050692256 -> 2122050684864
	2122050692256 [label=TBackward0]
	2122050694272 -> 2122050692256
	2122050515216 [label="0.auto_model.encoder.layer.2.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050515216 -> 2122050694272
	2122050694272 [label=AccumulateGrad]
	2122050695040 -> 2122050686160
	2122050695040 [label=TBackward0]
	2122050685776 -> 2122050695040
	2122050513776 [label="0.auto_model.encoder.layer.2.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050513776 -> 2122050685776
	2122050685776 [label=AccumulateGrad]
	2122050692352 -> 2122050692064
	2122050695808 -> 2122050690672
	2122050514352 [label="0.auto_model.encoder.layer.2.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050514352 -> 2122050695808
	2122050695808 [label=AccumulateGrad]
	2122050693216 -> 2122050690672
	2122050514256 [label="0.auto_model.encoder.layer.2.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050514256 -> 2122050693216
	2122050693216 [label=AccumulateGrad]
	2122050690528 -> 2122050693840
	2122050690528 [label=TBackward0]
	2122050690192 -> 2122050690528
	2122050513584 [label="0.auto_model.encoder.layer.3.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122050513584 -> 2122050690192
	2122050690192 [label=AccumulateGrad]
	2122050697920 -> 2122050695280
	2122050697920 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050695856 -> 2122050697920
	2122050695856 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050694656 -> 2122050695856
	2122050694656 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050686688 -> 2122050694656
	2122050686688 -> 2122051656720 [dir=none]
	2122051656720 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050686688 -> 2122051657296 [dir=none]
	2122051657296 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050686688 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050694416 -> 2122050686688
	2122050512816 [label="0.auto_model.encoder.layer.3.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122050512816 -> 2122050694416
	2122050694416 [label=AccumulateGrad]
	2122050694368 -> 2122050686688
	2122050694368 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050690672 -> 2122050694368
	2122050693552 -> 2122050686688
	2122050693552 [label=TBackward0]
	2122050694944 -> 2122050693552
	2122050515024 [label="0.auto_model.encoder.layer.3.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122050515024 -> 2122050694944
	2122050694944 [label=AccumulateGrad]
	2122050695952 -> 2122050695280
	2122050695952 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050695904 -> 2122050695952
	2122050695904 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050692928 -> 2122050695904
	2122050692928 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050692112 -> 2122050692928
	2122050692112 -> 2122051658448 [dir=none]
	2122051658448 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050692112 -> 2122051659024 [dir=none]
	2122051659024 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050692112 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050687504 -> 2122050692112
	2122050514064 [label="0.auto_model.encoder.layer.3.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122050514064 -> 2122050687504
	2122050687504 [label=AccumulateGrad]
	2122050697488 -> 2122050692112
	2122050697488 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050690672 -> 2122050697488
	2122050696192 -> 2122050692112
	2122050696192 [label=TBackward0]
	2122050692016 -> 2122050696192
	2122050512912 [label="0.auto_model.encoder.layer.3.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122050512912 -> 2122050692016
	2122050692016 [label=AccumulateGrad]
	2122050696096 -> 2122050689472
	2122050696096 [label=TBackward0]
	2122050685200 -> 2122050696096
	2122050513392 [label="0.auto_model.encoder.layer.3.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050513392 -> 2122050685200
	2122050685200 [label=AccumulateGrad]
	2122050690672 -> 2122050693360
	2122050698832 -> 2122050695232
	2122050508016 [label="0.auto_model.encoder.layer.3.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050508016 -> 2122050698832
	2122050698832 [label=AccumulateGrad]
	2122050686064 -> 2122050695232
	2122050514448 [label="0.auto_model.encoder.layer.3.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050514448 -> 2122050686064
	2122050686064 [label=AccumulateGrad]
	2122050688512 -> 2122050685488
	2122050688512 [label=TBackward0]
	2122050698208 -> 2122050688512
	2122050513104 [label="0.auto_model.encoder.layer.3.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050513104 -> 2122050698208
	2122050698208 [label=AccumulateGrad]
	2122050686304 -> 2122050695760
	2122050686304 [label=TBackward0]
	2122050689808 -> 2122050686304
	2122050505616 [label="0.auto_model.encoder.layer.3.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050505616 -> 2122050689808
	2122050689808 [label=AccumulateGrad]
	2122050695232 -> 2122050695376
	2122050698256 -> 2122050968336
	2122050513008 [label="0.auto_model.encoder.layer.3.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050513008 -> 2122050698256
	2122050698256 [label=AccumulateGrad]
	2122050688176 -> 2122050968336
	2122050512240 [label="0.auto_model.encoder.layer.3.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050512240 -> 2122050688176
	2122050688176 [label=AccumulateGrad]
	2122050696048 -> 2122050694560
	2122050696048 [label=TBackward0]
	2122050684720 -> 2122050696048
	2122050518384 [label="0.auto_model.encoder.layer.4.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122050518384 -> 2122050684720
	2122050684720 [label=AccumulateGrad]
	2122050973568 -> 2122050967424
	2122050973568 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050697344 -> 2122050973568
	2122050697344 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050698064 -> 2122050697344
	2122050698064 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050683616 -> 2122050698064
	2122050683616 -> 2122051663440 [dir=none]
	2122051663440 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050683616 -> 2122051664016 [dir=none]
	2122051664016 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050683616 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050687024 -> 2122050683616
	2122050516368 [label="0.auto_model.encoder.layer.4.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122050516368 -> 2122050687024
	2122050687024 [label=AccumulateGrad]
	2122050687312 -> 2122050683616
	2122050687312 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050968336 -> 2122050687312
	2122050683376 -> 2122050683616
	2122050683376 [label=TBackward0]
	2122050688752 -> 2122050683376
	2122050508112 [label="0.auto_model.encoder.layer.4.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122050508112 -> 2122050688752
	2122050688752 [label=AccumulateGrad]
	2122050689904 -> 2122050967424
	2122050689904 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050688416 -> 2122050689904
	2122050688416 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050699216 -> 2122050688416
	2122050699216 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050687264 -> 2122050699216
	2122050687264 -> 2122051665168 [dir=none]
	2122051665168 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050687264 -> 2122051665744 [dir=none]
	2122051665744 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050687264 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050698544 -> 2122050687264
	2122050506000 [label="0.auto_model.encoder.layer.4.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122050506000 -> 2122050698544
	2122050698544 [label=AccumulateGrad]
	2122050693120 -> 2122050687264
	2122050693120 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050968336 -> 2122050693120
	2122050686400 -> 2122050687264
	2122050686400 [label=TBackward0]
	2122050694608 -> 2122050686400
	2122050516752 [label="0.auto_model.encoder.layer.4.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122050516752 -> 2122050694608
	2122050694608 [label=AccumulateGrad]
	2122050968720 -> 2122050965648
	2122050968720 [label=TBackward0]
	2122050974144 -> 2122050968720
	2122050516176 [label="0.auto_model.encoder.layer.4.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050516176 -> 2122050974144
	2122050974144 [label=AccumulateGrad]
	2122050968336 -> 2122050971984
	2122050965696 -> 2122050968192
	2122050516560 [label="0.auto_model.encoder.layer.4.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050516560 -> 2122050965696
	2122050965696 [label=AccumulateGrad]
	2122050965024 -> 2122050968192
	2122050505808 [label="0.auto_model.encoder.layer.4.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050505808 -> 2122050965024
	2122050965024 [label=AccumulateGrad]
	2122050970208 -> 2122050972464
	2122050970208 [label=TBackward0]
	2122050975872 -> 2122050970208
	2122050517424 [label="0.auto_model.encoder.layer.4.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050517424 -> 2122050975872
	2122050975872 [label=AccumulateGrad]
	2122050975104 -> 2122050969776
	2122050975104 [label=TBackward0]
	2122050966944 -> 2122050975104
	2122050517904 [label="0.auto_model.encoder.layer.4.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050517904 -> 2122050966944
	2122050966944 [label=AccumulateGrad]
	2122050968192 -> 2122050965072
	2122050975488 -> 2122050963536
	2122050517712 [label="0.auto_model.encoder.layer.4.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050517712 -> 2122050975488
	2122050975488 [label=AccumulateGrad]
	2122050973328 -> 2122050963536
	2122050517136 [label="0.auto_model.encoder.layer.4.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050517136 -> 2122050973328
	2122050973328 [label=AccumulateGrad]
	2122050969440 -> 2122050963056
	2122050969440 [label=TBackward0]
	2122050975680 -> 2122050969440
	2122050508400 [label="0.auto_model.encoder.layer.5.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122050508400 -> 2122050975680
	2122050975680 [label=AccumulateGrad]
	2122050971888 -> 2122050961664
	2122050971888 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050976640 -> 2122050971888
	2122050976640 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050970976 -> 2122050976640
	2122050970976 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050975296 -> 2122050970976
	2122050975296 -> 2122051113168 [dir=none]
	2122051113168 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050975296 -> 2122051113744 [dir=none]
	2122051113744 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050975296 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050970784 -> 2122050975296
	2122050517040 [label="0.auto_model.encoder.layer.5.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122050517040 -> 2122050970784
	2122050970784 [label=AccumulateGrad]
	2122050976592 -> 2122050975296
	2122050976592 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050963536 -> 2122050976592
	2122050972560 -> 2122050975296
	2122050972560 [label=TBackward0]
	2122050976880 -> 2122050972560
	2122050508688 [label="0.auto_model.encoder.layer.5.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122050508688 -> 2122050976880
	2122050976880 [label=AccumulateGrad]
	2122050967712 -> 2122050961664
	2122050967712 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050975248 -> 2122050967712
	2122050975248 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050977312 -> 2122050975248
	2122050977312 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050975536 -> 2122050977312
	2122050975536 -> 2122051114896 [dir=none]
	2122051114896 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050975536 -> 2122051115472 [dir=none]
	2122051115472 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050975536 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050966560 -> 2122050975536
	2122050508208 [label="0.auto_model.encoder.layer.5.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122050508208 -> 2122050966560
	2122050966560 [label=AccumulateGrad]
	2122050963344 -> 2122050975536
	2122050963344 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050963536 -> 2122050963344
	2122050977024 -> 2122050975536
	2122050977024 [label=TBackward0]
	2122050685872 -> 2122050977024
	2122050507536 [label="0.auto_model.encoder.layer.5.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122050507536 -> 2122050685872
	2122050685872 [label=AccumulateGrad]
	2122050968672 -> 2122050967088
	2122050968672 [label=TBackward0]
	2122050966464 -> 2122050968672
	2122050507632 [label="0.auto_model.encoder.layer.5.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050507632 -> 2122050966464
	2122050966464 [label=AccumulateGrad]
	2122050963536 -> 2122050965984
	2122050965888 -> 2122050969728
	2122050507248 [label="0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050507248 -> 2122050965888
	2122050965888 [label=AccumulateGrad]
	2122050976160 -> 2122050969728
	2122050517328 [label="0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050517328 -> 2122050976160
	2122050976160 [label=AccumulateGrad]
	2122050967952 -> 2122050967136
	2122050967952 [label=TBackward0]
	2122050975344 -> 2122050967952
	2122050506480 [label="0.auto_model.encoder.layer.5.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050506480 -> 2122050975344
	2122050975344 [label=AccumulateGrad]
	2122050976256 -> 2122050963104
	2122050976256 [label=TBackward0]
	2122050964592 -> 2122050976256
	2122050514928 [label="0.auto_model.encoder.layer.5.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050514928 -> 2122050964592
	2122050964592 [label=AccumulateGrad]
	2122050969728 -> 2122050965312
	2122050976784 -> 2122050968528
	2122050516080 [label="0.auto_model.encoder.layer.5.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050516080 -> 2122050976784
	2122050976784 [label=AccumulateGrad]
	2122050969056 -> 2122050968528
	2122050507440 [label="0.auto_model.encoder.layer.5.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050507440 -> 2122050969056
	2122050969056 [label=AccumulateGrad]
	2122050976112 -> 2122050963968
	2122050976112 [label=TBackward0]
	2122050972800 -> 2122050976112
	2122050507344 [label="0.auto_model.encoder.layer.6.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122050507344 -> 2122050972800
	2122050972800 [label=AccumulateGrad]
	2122050964448 -> 2122050962912
	2122050964448 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050971504 -> 2122050964448
	2122050971504 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050974288 -> 2122050971504
	2122050974288 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050964736 -> 2122050974288
	2122050964736 -> 2122051119888 [dir=none]
	2122051119888 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050964736 -> 2122051120464 [dir=none]
	2122051120464 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050964736 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050972848 -> 2122050964736
	2122050648592 [label="0.auto_model.encoder.layer.6.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122050648592 -> 2122050972848
	2122050972848 [label=AccumulateGrad]
	2122050973952 -> 2122050964736
	2122050973952 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050968528 -> 2122050973952
	2122050962096 -> 2122050964736
	2122050962096 [label=TBackward0]
	2122050966128 -> 2122050962096
	2122050506864 [label="0.auto_model.encoder.layer.6.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122050506864 -> 2122050966128
	2122050966128 [label=AccumulateGrad]
	2122050970112 -> 2122050962912
	2122050970112 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050967280 -> 2122050970112
	2122050967280 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050966224 -> 2122050967280
	2122050966224 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050968000 -> 2122050966224
	2122050968000 -> 2122051121616 [dir=none]
	2122051121616 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050968000 -> 2122051122192 [dir=none]
	2122051122192 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050968000 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050970880 -> 2122050968000
	2122050507056 [label="0.auto_model.encoder.layer.6.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122050507056 -> 2122050970880
	2122050970880 [label=AccumulateGrad]
	2122050966512 -> 2122050968000
	2122050966512 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050968528 -> 2122050966512
	2122050965360 -> 2122050968000
	2122050965360 [label=TBackward0]
	2122050962864 -> 2122050965360
	2122050507728 [label="0.auto_model.encoder.layer.6.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122050507728 -> 2122050962864
	2122050962864 [label=AccumulateGrad]
	2122050968576 -> 2122050977744
	2122050968576 [label=TBackward0]
	2122050977600 -> 2122050968576
	2122050637072 [label="0.auto_model.encoder.layer.6.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050637072 -> 2122050977600
	2122050977600 [label=AccumulateGrad]
	2122050968528 -> 2122050962816
	2122050975008 -> 2122050973184
	2122050648880 [label="0.auto_model.encoder.layer.6.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050648880 -> 2122050975008
	2122050975008 [label=AccumulateGrad]
	2122050977696 -> 2122050973184
	2122050506768 [label="0.auto_model.encoder.layer.6.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050506768 -> 2122050977696
	2122050977696 [label=AccumulateGrad]
	2122050967904 -> 2122050976448
	2122050967904 [label=TBackward0]
	2122050967520 -> 2122050967904
	2122050649168 [label="0.auto_model.encoder.layer.6.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050649168 -> 2122050967520
	2122050967520 [label=AccumulateGrad]
	2122050972704 -> 2122050964208
	2122050972704 [label=TBackward0]
	2122050969296 -> 2122050972704
	2122050649552 [label="0.auto_model.encoder.layer.6.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050649552 -> 2122050969296
	2122050969296 [label=AccumulateGrad]
	2122050973184 -> 2122050968864
	2122050970928 -> 2122050974576
	2122050650032 [label="0.auto_model.encoder.layer.6.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050650032 -> 2122050970928
	2122050970928 [label=AccumulateGrad]
	2122050971120 -> 2122050974576
	2122050636016 [label="0.auto_model.encoder.layer.6.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050636016 -> 2122050971120
	2122050971120 [label=AccumulateGrad]
	2122050963392 -> 2122050975584
	2122050963392 [label=TBackward0]
	2122050974096 -> 2122050963392
	2122050634576 [label="0.auto_model.encoder.layer.7.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122050634576 -> 2122050974096
	2122050974096 [label=AccumulateGrad]
	2122050972992 -> 2122050976736
	2122050972992 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050972752 -> 2122050972992
	2122050972752 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050965264 -> 2122050972752
	2122050965264 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050966656 -> 2122050965264
	2122050966656 -> 2122051126672 [dir=none]
	2122051126672 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050966656 -> 2122051127248 [dir=none]
	2122051127248 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050966656 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050976688 -> 2122050966656
	2122050635632 [label="0.auto_model.encoder.layer.7.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122050635632 -> 2122050976688
	2122050976688 [label=AccumulateGrad]
	2122050969008 -> 2122050966656
	2122050969008 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050974576 -> 2122050969008
	2122050969152 -> 2122050966656
	2122050969152 [label=TBackward0]
	2122050969344 -> 2122050969152
	2122050633808 [label="0.auto_model.encoder.layer.7.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122050633808 -> 2122050969344
	2122050969344 [label=AccumulateGrad]
	2122050974336 -> 2122050976736
	2122050974336 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050961856 -> 2122050974336
	2122050961856 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050975392 -> 2122050961856
	2122050975392 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050964352 -> 2122050975392
	2122050964352 -> 2122051128400 [dir=none]
	2122051128400 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050964352 -> 2122051128976 [dir=none]
	2122051128976 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050964352 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050968816 -> 2122050964352
	2122050649744 [label="0.auto_model.encoder.layer.7.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122050649744 -> 2122050968816
	2122050968816 [label=AccumulateGrad]
	2122050968480 -> 2122050964352
	2122050968480 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050974576 -> 2122050968480
	2122050972416 -> 2122050964352
	2122050972416 [label=TBackward0]
	2122050964784 -> 2122050972416
	2122050634960 [label="0.auto_model.encoder.layer.7.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122050634960 -> 2122050964784
	2122050964784 [label=AccumulateGrad]
	2122050975728 -> 2122050961760
	2122050975728 [label=TBackward0]
	2122050963152 -> 2122050975728
	2122050635824 [label="0.auto_model.encoder.layer.7.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050635824 -> 2122050963152
	2122050963152 [label=AccumulateGrad]
	2122050974576 -> 2122050974528
	2122050965216 -> 2122050962480
	2122050647728 [label="0.auto_model.encoder.layer.7.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050647728 -> 2122050965216
	2122050965216 [label=AccumulateGrad]
	2122050975824 -> 2122050962480
	2122050636976 [label="0.auto_model.encoder.layer.7.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050636976 -> 2122050975824
	2122050975824 [label=AccumulateGrad]
	2122050967040 -> 2122050969584
	2122050967040 [label=TBackward0]
	2122050970688 -> 2122050967040
	2122050645232 [label="0.auto_model.encoder.layer.7.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050645232 -> 2122050970688
	2122050970688 [label=AccumulateGrad]
	2122050970064 -> 2122050968432
	2122050970064 [label=TBackward0]
	2122050966752 -> 2122050970064
	2122050649936 [label="0.auto_model.encoder.layer.7.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050649936 -> 2122050966752
	2122050966752 [label=AccumulateGrad]
	2122050962480 -> 2122050965168
	2122050967328 -> 2122050977504
	2122050646096 [label="0.auto_model.encoder.layer.7.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050646096 -> 2122050967328
	2122050967328 [label=AccumulateGrad]
	2122050974768 -> 2122050977504
	2122050636304 [label="0.auto_model.encoder.layer.7.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050636304 -> 2122050974768
	2122050974768 [label=AccumulateGrad]
	2122050963728 -> 2122050961808
	2122050963728 [label=TBackward0]
	2122050971744 -> 2122050963728
	2122050642160 [label="0.auto_model.encoder.layer.8.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122050642160 -> 2122050971744
	2122050971744 [label=AccumulateGrad]
	2122050969392 -> 2122050975632
	2122050969392 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050967760 -> 2122050969392
	2122050967760 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050970160 -> 2122050967760
	2122050970160 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050965744 -> 2122050970160
	2122050965744 -> 2122051133392 [dir=none]
	2122051133392 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050965744 -> 2122051133968 [dir=none]
	2122051133968 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050965744 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050973472 -> 2122050965744
	2122050634672 [label="0.auto_model.encoder.layer.8.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122050634672 -> 2122050973472
	2122050973472 [label=AccumulateGrad]
	2122050966176 -> 2122050965744
	2122050966176 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050977504 -> 2122050966176
	2122050971648 -> 2122050965744
	2122050971648 [label=TBackward0]
	2122050977168 -> 2122050971648
	2122050640432 [label="0.auto_model.encoder.layer.8.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122050640432 -> 2122050977168
	2122050977168 [label=AccumulateGrad]
	2122050961616 -> 2122050975632
	2122050961616 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122050975152 -> 2122050961616
	2122050975152 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050973280 -> 2122050975152
	2122050973280 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050961472 -> 2122050973280
	2122050961472 -> 2122051135120 [dir=none]
	2122051135120 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050961472 -> 2122051135696 [dir=none]
	2122051135696 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050961472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050974432 -> 2122050961472
	2122050644752 [label="0.auto_model.encoder.layer.8.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122050644752 -> 2122050974432
	2122050974432 [label=AccumulateGrad]
	2122050963824 -> 2122050961472
	2122050963824 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050977504 -> 2122050963824
	2122050967616 -> 2122050961472
	2122050967616 [label=TBackward0]
	2122050965504 -> 2122050967616
	2122050637360 [label="0.auto_model.encoder.layer.8.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122050637360 -> 2122050965504
	2122050965504 [label=AccumulateGrad]
	2122050964304 -> 2122050964832
	2122050964304 [label=TBackward0]
	2122050966848 -> 2122050964304
	2122050636112 [label="0.auto_model.encoder.layer.8.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050636112 -> 2122050966848
	2122050966848 [label=AccumulateGrad]
	2122050977504 -> 2122050974912
	2122050967472 -> 2122050968240
	2122050647440 [label="0.auto_model.encoder.layer.8.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050647440 -> 2122050967472
	2122050967472 [label=AccumulateGrad]
	2122050966080 -> 2122050968240
	2122050648688 [label="0.auto_model.encoder.layer.8.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050648688 -> 2122050966080
	2122050966080 [label=AccumulateGrad]
	2122050975056 -> 2122050963200
	2122050975056 [label=TBackward0]
	2122050961520 -> 2122050975056
	2122050643696 [label="0.auto_model.encoder.layer.8.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050643696 -> 2122050961520
	2122050961520 [label=AccumulateGrad]
	2122050973520 -> 2122050966992
	2122050973520 [label=TBackward0]
	2122050972896 -> 2122050973520
	2122050639664 [label="0.auto_model.encoder.layer.8.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050639664 -> 2122050972896
	2122050972896 [label=AccumulateGrad]
	2122050968240 -> 2122050969920
	2122050962336 -> 2121924374208
	2122050637552 [label="0.auto_model.encoder.layer.8.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050637552 -> 2122050962336
	2122050962336 [label=AccumulateGrad]
	2122050976544 -> 2121924374208
	2122050639760 [label="0.auto_model.encoder.layer.8.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050639760 -> 2122050976544
	2122050976544 [label=AccumulateGrad]
	2122050976496 -> 2121924375216
	2122050976496 [label=TBackward0]
	2122050973712 -> 2122050976496
	2122050648784 [label="0.auto_model.encoder.layer.9.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122050648784 -> 2122050973712
	2122050973712 [label=AccumulateGrad]
	2121924373392 -> 2121924365904
	2121924373392 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2121924365712 -> 2121924373392
	2121924365712 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2121924372768 -> 2121924365712
	2121924372768 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050970352 -> 2121924372768
	2122050970352 -> 2122051140112 [dir=none]
	2122051140112 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050970352 -> 2122051140688 [dir=none]
	2122051140688 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050970352 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050969488 -> 2122050970352
	2122050641392 [label="0.auto_model.encoder.layer.9.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122050641392 -> 2122050969488
	2122050969488 [label=AccumulateGrad]
	2122050965936 -> 2122050970352
	2122050965936 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2121924374208 -> 2122050965936
	2122050970400 -> 2122050970352
	2122050970400 [label=TBackward0]
	2122050962768 -> 2122050970400
	2122050646864 [label="0.auto_model.encoder.layer.9.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122050646864 -> 2122050962768
	2122050962768 [label=AccumulateGrad]
	2121924375600 -> 2121924365904
	2121924375600 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2121924362928 -> 2121924375600
	2121924362928 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122050970832 -> 2121924362928
	2122050970832 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122050962720 -> 2122050970832
	2122050962720 -> 2122051223824 [dir=none]
	2122051223824 [label="mat1
 (8, 768)" fillcolor=orange]
	2122050962720 -> 2122051224400 [dir=none]
	2122051224400 [label="mat2
 (768, 768)" fillcolor=orange]
	2122050962720 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122050966800 -> 2122050962720
	2122050649648 [label="0.auto_model.encoder.layer.9.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122050649648 -> 2122050966800
	2122050966800 [label=AccumulateGrad]
	2122050974864 -> 2122050962720
	2122050974864 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2121924374208 -> 2122050974864
	2122050968912 -> 2122050962720
	2122050968912 [label=TBackward0]
	2122050966032 -> 2122050968912
	2122050643408 [label="0.auto_model.encoder.layer.9.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122050643408 -> 2122050966032
	2122050966032 [label=AccumulateGrad]
	2121924362976 -> 2121924374976
	2121924362976 [label=TBackward0]
	2121924363360 -> 2121924362976
	2122050637648 [label="0.auto_model.encoder.layer.9.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050637648 -> 2121924363360
	2121924363360 [label=AccumulateGrad]
	2121924374208 -> 2121924373440
	2121924375456 -> 2121924375552
	2122050644944 [label="0.auto_model.encoder.layer.9.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050644944 -> 2121924375456
	2121924375456 [label=AccumulateGrad]
	2121924375168 -> 2121924375552
	2122050640624 [label="0.auto_model.encoder.layer.9.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050640624 -> 2121924375168
	2121924375168 [label=AccumulateGrad]
	2121924375024 -> 2121924366576
	2121924375024 [label=TBackward0]
	2121924372720 -> 2121924375024
	2122050639856 [label="0.auto_model.encoder.layer.9.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050639856 -> 2121924372720
	2121924372720 [label=AccumulateGrad]
	2121924374592 -> 2121924363264
	2121924374592 [label=TBackward0]
	2121924374832 -> 2121924374592
	2122050639472 [label="0.auto_model.encoder.layer.9.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050639472 -> 2121924374832
	2121924374832 [label=AccumulateGrad]
	2121924375552 -> 2121924363216
	2121924366960 -> 2122953383632
	2122050640240 [label="0.auto_model.encoder.layer.9.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050640240 -> 2121924366960
	2121924366960 [label=AccumulateGrad]
	2121924373488 -> 2122953383632
	2122050647248 [label="0.auto_model.encoder.layer.9.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050647248 -> 2121924373488
	2121924373488 [label=AccumulateGrad]
	2122953390832 -> 2122953391120
	2122953390832 [label=TBackward0]
	2122953384448 -> 2122953390832
	2122050637168 [label="0.auto_model.encoder.layer.10.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122050637168 -> 2122953384448
	2122953384448 [label=AccumulateGrad]
	2122953383056 -> 2122953383872
	2122953383056 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122953390544 -> 2122953383056
	2122953390544 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953380032 -> 2122953390544
	2122953380032 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2121924374016 -> 2122953380032
	2121924374016 -> 2122051228816 [dir=none]
	2122051228816 [label="mat1
 (8, 768)" fillcolor=orange]
	2121924374016 -> 2122051229392 [dir=none]
	2122051229392 [label="mat2
 (768, 768)" fillcolor=orange]
	2121924374016 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2121924376848 -> 2121924374016
	2122050641104 [label="0.auto_model.encoder.layer.10.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122050641104 -> 2121924376848
	2121924376848 [label=AccumulateGrad]
	2121924374112 -> 2121924374016
	2121924374112 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953383632 -> 2121924374112
	2121924374928 -> 2121924374016
	2121924374928 [label=TBackward0]
	2121924366864 -> 2121924374928
	2122050638512 [label="0.auto_model.encoder.layer.10.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122050638512 -> 2121924366864
	2121924366864 [label=AccumulateGrad]
	2122953383440 -> 2122953383872
	2122953383440 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122953391408 -> 2122953383440
	2122953391408 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2121924365472 -> 2122953391408
	2121924365472 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2121924365520 -> 2121924365472
	2121924365520 -> 2122051230544 [dir=none]
	2122051230544 [label="mat1
 (8, 768)" fillcolor=orange]
	2121924365520 -> 2122051231120 [dir=none]
	2122051231120 [label="mat2
 (768, 768)" fillcolor=orange]
	2121924365520 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2121924365952 -> 2121924365520
	2122050638320 [label="0.auto_model.encoder.layer.10.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122050638320 -> 2121924365952
	2121924365952 [label=AccumulateGrad]
	2121924375120 -> 2121924365520
	2121924375120 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953383632 -> 2121924375120
	2121924372576 -> 2121924365520
	2121924372576 [label=TBackward0]
	2121924373584 -> 2121924372576
	2122050640048 [label="0.auto_model.encoder.layer.10.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122050640048 -> 2121924373584
	2121924373584 [label=AccumulateGrad]
	2122953390064 -> 2122953390352
	2122953390064 [label=TBackward0]
	2122953379936 -> 2122953390064
	2122050638992 [label="0.auto_model.encoder.layer.10.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050638992 -> 2122953379936
	2122953379936 [label=AccumulateGrad]
	2122953383632 -> 2122953386320
	2122953382672 -> 2122953383104
	2122050640912 [label="0.auto_model.encoder.layer.10.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050640912 -> 2122953382672
	2122953382672 [label=AccumulateGrad]
	2122953380176 -> 2122953383104
	2122050512432 [label="0.auto_model.encoder.layer.10.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050512432 -> 2122953380176
	2122953380176 [label=AccumulateGrad]
	2122953390592 -> 2122953383824
	2122953390592 [label=TBackward0]
	2122953391072 -> 2122953390592
	2122050641776 [label="0.auto_model.encoder.layer.10.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050641776 -> 2122953391072
	2122953391072 [label=AccumulateGrad]
	2122953383536 -> 2122953390448
	2122953383536 [label=TBackward0]
	2122953391360 -> 2122953383536
	2122050642448 [label="0.auto_model.encoder.layer.10.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050642448 -> 2122953391360
	2122953391360 [label=AccumulateGrad]
	2122953383104 -> 2122953384016
	2122953382480 -> 2122953384496
	2122050642352 [label="0.auto_model.encoder.layer.10.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050642352 -> 2122953382480
	2122953382480 [label=AccumulateGrad]
	2122953386176 -> 2122953384496
	2122050641200 [label="0.auto_model.encoder.layer.10.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050641200 -> 2122953386176
	2122953386176 [label=AccumulateGrad]
	2122953384592 -> 2122953390640
	2122953384592 [label=TBackward0]
	2122953380128 -> 2122953384592
	2122050641008 [label="0.auto_model.encoder.layer.11.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	2122050641008 -> 2122953380128
	2122953380128 [label=AccumulateGrad]
	2122953380224 -> 2122953391456
	2122953380224 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122953390112 -> 2122953380224
	2122953390112 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953380416 -> 2122953390112
	2122953380416 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122953390304 -> 2122953380416
	2122953390304 -> 2122051235536 [dir=none]
	2122051235536 [label="mat1
 (8, 768)" fillcolor=orange]
	2122953390304 -> 2122051236112 [dir=none]
	2122051236112 [label="mat2
 (768, 768)" fillcolor=orange]
	2122953390304 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122953383776 -> 2122953390304
	2122050636592 [label="0.auto_model.encoder.layer.11.attention.self.key.bias
 (768)" fillcolor=lightblue]
	2122050636592 -> 2122953383776
	2122953383776 [label=AccumulateGrad]
	2122953390400 -> 2122953390304
	2122953390400 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953384496 -> 2122953390400
	2122953386272 -> 2122953390304
	2122953386272 [label=TBackward0]
	2122953383392 -> 2122953386272
	2122050637936 [label="0.auto_model.encoder.layer.11.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	2122050637936 -> 2122953383392
	2122953383392 [label=AccumulateGrad]
	2122953382144 -> 2122953391456
	2122953382144 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	2122953382336 -> 2122953382144
	2122953382336 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953384352 -> 2122953382336
	2122953384352 [label="ViewBackward0
------------------------
self_sym_sizes: (8, 768)"]
	2122953386368 -> 2122953384352
	2122953386368 -> 2122051237264 [dir=none]
	2122051237264 [label="mat1
 (8, 768)" fillcolor=orange]
	2122953386368 -> 2122051237840 [dir=none]
	2122051237840 [label="mat2
 (768, 768)" fillcolor=orange]
	2122953386368 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (8, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	2122953390496 -> 2122953386368
	2122050638704 [label="0.auto_model.encoder.layer.11.attention.self.value.bias
 (768)" fillcolor=lightblue]
	2122050638704 -> 2122953390496
	2122953390496 [label=AccumulateGrad]
	2122953383152 -> 2122953386368
	2122953383152 [label="ViewBackward0
---------------------------
self_sym_sizes: (1, 8, 768)"]
	2122953384496 -> 2122953383152
	2122953384400 -> 2122953386368
	2122953384400 [label=TBackward0]
	2122953384112 -> 2122953384400
	2122050633904 [label="0.auto_model.encoder.layer.11.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	2122050633904 -> 2122953384112
	2122953384112 [label=AccumulateGrad]
	2122953383200 -> 2122953384064
	2122953383200 [label=TBackward0]
	2122953382048 -> 2122953383200
	2122050647536 [label="0.auto_model.encoder.layer.11.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	2122050647536 -> 2122953382048
	2122953382048 [label=AccumulateGrad]
	2122953384496 -> 2122951679264
	2122951673360 -> 2122952784816
	2122050645424 [label="0.auto_model.encoder.layer.11.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050645424 -> 2122951673360
	2122951673360 [label=AccumulateGrad]
	2122951673168 -> 2122952784816
	2122050637456 [label="0.auto_model.encoder.layer.11.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050637456 -> 2122951673168
	2122951673168 [label=AccumulateGrad]
	2122951675232 -> 2122951674608
	2122951675232 [label=TBackward0]
	2122951678832 -> 2122951675232
	2122050635344 [label="0.auto_model.encoder.layer.11.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	2122050635344 -> 2122951678832
	2122951678832 [label=AccumulateGrad]
	2122951678976 -> 2122951679120
	2122951678976 [label=TBackward0]
	2122951679504 -> 2122951678976
	2122050649840 [label="0.auto_model.encoder.layer.11.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	2122050649840 -> 2122951679504
	2122951679504 [label=AccumulateGrad]
	2122952784816 -> 2122952783040
	2122952782512 -> 2122952785200
	2122050645328 [label="0.auto_model.encoder.layer.11.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	2122050645328 -> 2122952782512
	2122952782512 [label=AccumulateGrad]
	2122952784912 -> 2122952785200
	2122050642544 [label="0.auto_model.encoder.layer.11.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	2122050642544 -> 2122952784912
	2122952784912 [label=AccumulateGrad]
	2122952785152 -> 2122051207088
}
