{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d6180c",
   "metadata": {},
   "source": [
    "Pengeksportan Data Dari File Database ke dalam String Besar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69466dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. Panjang hasil: 8,819,044 chars\n",
      "Simpan: D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Preprocessing\\between_insert_and_indexes.sql\n",
      "Juga file Python: D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Preprocessing\\Data.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# ====== KONFIGURASI ======\n",
    "SQL_PATHS = [\n",
    "    r\"D:\\Media\\Kuliah\\Skripsi\\ScripTI\\Versi 4\\Preprocessing\\proposals.sql\",\n",
    "    \"/mnt/data/proposals.sql\",\n",
    "]\n",
    "\n",
    "START_MARKER = \"\"\"INSERT INTO `proposals` (`id`, `nim`, `periode_id`, `judul`, `latar_belakang`, `rumusan`, `batasan`, `tujuan`, `referensi`, `status`, `user_id`, `modified`, `filename`, `filedir`, `mime_type`, `manfaat`, `revisi_dari`, `dosen_id`, `upload_revisi`) VALUES\"\"\"\n",
    "END_MARKER = \"\"\"\n",
    "--\n",
    "-- Indexes for dumped tables\n",
    "--\n",
    "\"\"\".strip(\"\\n\")  # biar lebih fleksibel di awal/akhir baris\n",
    "\n",
    "# ====== UTIL ======\n",
    "def read_all(paths):\n",
    "    for p in map(Path, paths):\n",
    "        if p.exists():\n",
    "            for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
    "                try:\n",
    "                    return p.read_text(encoding=enc)\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            return p.read_bytes().decode(\"latin-1\", errors=\"replace\")\n",
    "    raise FileNotFoundError(\"File SQL tidak ditemukan di path yang disediakan.\")\n",
    "\n",
    "def normalize_newlines(s: str) -> str:\n",
    "    return s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "def whitespace_tolerant_regex(s: str) -> re.Pattern:\n",
    "    \"\"\"\n",
    "    Buat regex yang menganggap semua whitespace di 's' bisa jadi\n",
    "    spasi/enter/tab berapa pun. Karakter lain di-escape supaya literal.\n",
    "    \"\"\"\n",
    "    # Normalize whitespace di marker dulu\n",
    "    s = normalize_newlines(s)\n",
    "    # Pisah berdasar whitespace lalu sambung dengan \\s+\n",
    "    parts = re.split(r\"\\s+\", s.strip())\n",
    "    pattern = r\"\\s*\".join(map(re.escape, parts))\n",
    "    return re.compile(pattern, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def find_first(haystack: str, needle: str) -> int:\n",
    "    \"\"\"Cari needle persis; kalau gagal, pakai regex toleran whitespace.\"\"\"\n",
    "    idx = haystack.find(needle)\n",
    "    if idx >= 0:\n",
    "        return idx\n",
    "    # fallback regex\n",
    "    m = whitespace_tolerant_regex(needle).search(haystack)\n",
    "    return m.start() if m else -1\n",
    "\n",
    "def find_between_markers(text: str, start_marker: str, end_marker: str) -> str:\n",
    "    t = normalize_newlines(text)\n",
    "    sm = normalize_newlines(start_marker).strip()\n",
    "    em = normalize_newlines(end_marker).strip()\n",
    "\n",
    "    i = find_first(t, sm)\n",
    "    if i < 0:\n",
    "     \n",
    "        return t\n",
    "\n",
    "    sm_re = whitespace_tolerant_regex(sm)\n",
    "    m = sm_re.search(t, i)\n",
    "    start_after = m.end() if m else i + len(sm)\n",
    "\n",
    "    j = find_first(t[start_after:], em)\n",
    "    if j < 0:\n",
    "        # Tidak ada penutup -> ambil dari setelah pembuka sampai akhir\n",
    "        return t[start_after:]\n",
    "    end_index = start_after + j\n",
    "    return t[start_after:end_index]\n",
    "\n",
    "# ====== MAIN ======\n",
    "if __name__ == \"__main__\":\n",
    "    whole = read_all(SQL_PATHS)\n",
    "    data = find_between_markers(whole, START_MARKER, END_MARKER)\n",
    "    out = Path(\"between_insert_and_indexes.sql\")\n",
    "    out.write_text(data, encoding=\"utf-8\")\n",
    "\n",
    "    # Bungkus ke variabel Python 'data' kalau mau dipakai di script lain\n",
    "    py_out = Path(\"./Data.py\")\n",
    "    py_out.write_text('data = \"\"\"\\n' + data.replace('\"\"\"', r'\\\"\\\"\\\"') + '\\n\"\"\"', encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"OK. Panjang hasil: {len(data):,} chars\")\n",
    "    print(f\"Simpan: {out.resolve()}\")\n",
    "    print(f\"Juga file Python: {py_out.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d0ae05",
   "metadata": {},
   "source": [
    "Parsing String Ke List Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9601576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(914\n",
      "debug\n",
      "debug\n",
      "Total Remove Proposal:  1\n",
      "Jumlah data setelah diproses:  1279\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "import unicodedata as ud\n",
    "from Data import data\n",
    "\n",
    "def is_alpha(ch):\n",
    "    return bool(ch) and (ch.isalpha() or ud.category(ch).startswith('L'))\n",
    "\n",
    "def is_numeric(ch): \n",
    "    return bool(ch) and (ch.isdigit() or ud.category(ch).startswith('N'))\n",
    "\n",
    "def is_space_or_punct_or_symbol(ch: str) -> bool:\n",
    "    cat = ud.category(ch)     # contoh: 'Ll','Lu','Nd','Ps','Pe','Zs','Po','Sm','So',...\n",
    "    return cat[0] in ('Z', 'P', 'S')\n",
    "\n",
    "def removeHTMLString(data):\n",
    "    \n",
    "    #Komleks Taq HTML\n",
    "    data = data.replace('<p class=\\\"MsoNormal\\\" style=\\\"margin-left:.25in;text-align:justify;text-indent:\\r\\n.25in;line-height:150%\\\"><span style=\\\"font-family:times new roman,serif; font-size:12.0pt; line-height:150%\\\">',\"\")\n",
    "    data = data.replace(\"<o:p></o:p>\", \"\")\n",
    "    data = data.replace(\"<p class=\\\"MsoNormal\\\" style=\\\"margin-left:.25in;text-align:justify;text-indent:\\r\\n.25in;line-height:150%\\\">\", \"\")\n",
    "    data = data.replace(\"</p>\\r\\n\\r\\n<p><!--[if supportFields]><b style=\\'mso-bidi-font-weight:normal\\'><span lang=ZH-CN\\r\\nstyle=\\'font-size:12.0pt;line-height:107%;font-family:\\\"Times New Roman\\\",serif;\\r\\nmso-fareast-font-family:Cambria;mso-ansi-language:ZH-CN;mso-fareast-language:\\r\\nZH-CN;mso-bidi-language:AR-SA\\'><span style=\\'mso-element:field-end\\'></span></span></b><![endif]--></p>\\r\\n\",\"\")\n",
    "    data = data.replace(\"<p><!--[if supportFields]><b style=\\'mso-bidi-font-weight:\\r\\nnormal\\'><span lang=ZH-CN style=\\'font-size:12.0pt;line-height:107%;font-family:\\r\\n\\\"Times New Roman\\\",serif;mso-fareast-font-family:Cambria\\'><span\\r\\nstyle=\\'mso-element:field-begin\\'></span></span></b><b style=\\'mso-bidi-font-weight:\\r\\nnormal\\'><span style=\\'font-size:12.0pt;line-height:107%;font-family:\\\"Times New Roman\\\",serif;\\r\\nmso-fareast-font-family:Cambria;mso-ansi-language:EN-US\\'><span\\r\\nstyle=\\'mso-spacerun:yes\\'> </span>BIBLIOGRAPHY<span style=\\'mso-spacerun:yes\\'> \\r\\n</span>\\\\l 1033 </span></b><b style=\\'mso-bidi-font-weight:normal\\'><span\\r\\nlang=ZH-CN style=\\'font-size:12.0pt;line-height:107%;font-family:\\\"Times New Roman\\\",serif;\\r\\nmso-fareast-font-family:Cambria\\'><span style=\\'mso-element:field-separator\\'></span></span></b><![endif]-->\",\"\")\n",
    "    data = data.replace(\"<!--[if supportFields]><b style=\\'mso-bidi-font-weight:\\r\\nnormal\\'><span style=\\'font-size:12.0pt;line-height:150%;font-family:\\\"Times New Roman\\\",serif;\\r\\nmso-fareast-font-family:\\\"Times New Roman\\\";color:black\\'><span style=\\'mso-element:\\r\\nfield-begin;mso-field-lock:yes\\'></span>\",\"\")\n",
    "    data = data.replace(\"<span style=\\'mso-element:field-separator\\'></span></span></b><![endif]-->\",\"\")\n",
    "    data = data.replace(\"</p>\\r\\n\\r\\n<p><!--[if supportFields]><b\\r\\nstyle=\\'mso-bidi-font-weight:normal\\'><span style=\\'font-size:12.0pt;line-height:\\r\\n150%;font-family:\\\"Times New Roman\\\",serif;mso-fareast-font-family:\\\"Times New Roman\\\";\\r\\ncolor:black\\'><span style=\\'mso-element:field-end\\'></span></span></b><![endif]--><strong>&nbsp;</strong></p>\\r\\n\", \"\")\n",
    "    data = data.replace(\"INSERT INTO `proposals` (`id`, `nim`, `periode_id`, `judul`, `latar_belakang`, `rumusan`, `batasan`, `tujuan`, `referensi`, `status`, `user_id`, `modified`, `filename`, `filedir`, `mime_type`, `manfaat`, `revisi_dari`, `dosen_id`, `upload_revisi`) VALUES\", \"\")\n",
    "    data = data.replace(\":</p>\\r\\n\\r\\n<p>\", \": \")\n",
    "    data = data.replace(\")</p>\\r\\n\\r\\n<p>\", \"). \")\n",
    "    data = data.replace(\".</p>\\r\\n\\r\\n<p>\", \". \")\n",
    "    data = data.replace(\"</p>\\r\\n\\r\\n<p>\", \" \")\n",
    "    data = data.replace(\":\\r\\na\", \": \")\n",
    "\n",
    "    #Vairasi Taq P\n",
    "    data = data.replace(\"<p>?</p>\",\"\")\n",
    "    data = data.replace(\"1.</p>\",\"\")\n",
    "    data = data.replace(\".?</p>\",\".\")\n",
    "    data = data.replace(\".</p>\",\".\")\n",
    "    data = data.replace(\")</p>\",\").\")\n",
    "    data = data.replace('</p>',\"\")\n",
    "    data = data.replace(\"</p>\",\"\")\n",
    "    data = data.replace(\"<p>\",\"\")\n",
    "    data = data.replace(\"./p>\",\".\")\n",
    "    \n",
    "    #Variasi Entitas\n",
    "    data = data.replace(\"&#39;\",\"`\")\n",
    "    data = data.replace(\"&hellip;..\",\"\")\n",
    "    data = data.replace(\"&nbsp;\",\" \")\n",
    "    data = data.replace(\"&iuml;\", \"ï\")\n",
    "    data = data.replace(\"&igrave;;\",\"ì\")\n",
    "    data = data.replace(\"&uuml;\",\"ü\")\n",
    "    data = data.replace(\"&ouml;\",\"ö\")\n",
    "    data = data.replace(\"&iacute;\",\"í\")\n",
    "    data = data.replace(\"&egrave;\",\"è\")\n",
    "    data = data.replace(\"&eacute;\",\"é\")\n",
    "    data = data.replace(\"&agrave;\",\"à\")\n",
    "    data = data.replace(\"v&gt;\",\"\")\n",
    "    \n",
    "    \n",
    "    #Variasi Gagal Encoding\n",
    "    data = data.replace(\"p?ny?n\", \"pinyin\")\n",
    "    data = data.replace(\"h?nz?\", \"hanzi\")\n",
    "    \n",
    "    #Variasi Strong\n",
    "    data = data.replace(\"<strong>\", \"\")\n",
    "    data = data.replace(\"/strong\",\"\")\n",
    "    \n",
    "    #Variasi ol li\n",
    "    data = data.replace(\"<ol><li><em>\",\"\")\n",
    "    data = data.replace(\"<li><em>\",\", \")\n",
    "    data = data.replace(\".</li>\\r\\n\",\", \")\n",
    "    data = data.replace(\"</li>\",\"\")\n",
    "    data = data.replace(\"<li>\",\" \")\n",
    "    data = data.replace(\"<ul>\", \"\")\n",
    "    data = data.replace(\"</ul>\", \"\")\n",
    "    data = data.replace(\"<ul\", \"\")\n",
    "    data = data.replace(\"</ul\", \"\")\n",
    "    data = data.replace(\"<ol>\",\"\")\n",
    "    data = data.replace(\"</ol>\",\"\")\n",
    "    data = data.replace(\"<>\",\"\")\n",
    "\n",
    "    #Variasi T\n",
    "    data = data.replace(\"\\t1\", \"\")\n",
    "    data = data.replace(\"\\t2\", \"\")\n",
    "    data = data.replace(\"\\t\", \"\")\n",
    "\n",
    "    #Variasi Lainnya    \n",
    "    data = data.replace(\"\\r\",\"\")\n",
    "    data = data.replace(\"</span>\",\"\")\n",
    "    data = data.replace(\"<span>\",\"\")\n",
    "    data = data.replace(\"<br />\",\"\")\n",
    "    data = data.replace(\"<br/>\",\"\")\n",
    "    data = data.replace(\"</em>\", \"\")\n",
    "    data = data.replace(\"<em>\", \"\")\n",
    "    data = data.replace(\"\\n\",\"\")\n",
    "    data = data.replace(\"/b\",\"\")\n",
    "    \n",
    "    data = _fix_em_dot_rule(data)\n",
    "\n",
    "    # ---- Hilangkan penanda [number] (mis. [1], [23]) ----\n",
    "    data = re.sub(r'\\[\\s*\\d+\\s*\\]\\s*', '', data)\n",
    "    data = re.sub(r'\\.\\s*([A-Za-z])\\s*\\.', r'. \\1.', data)\n",
    "    # data = data.replace(\"</em>.\", \",\")\n",
    "\n",
    "    data = re.sub(r'([.!?])(?:\\s*[.!?])+(?=\\s|$)', r'\\1', data)\n",
    "    data = re.sub(r'<[^>]+>', '', data)\n",
    "    data = html.unescape(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "import re\n",
    "\n",
    "def _fix_em_dot_rule(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Ubah pola:  <word></em>. <word>\n",
    "    sesuai aturan kapitalisasi yang diminta.\n",
    "    \"\"\"\n",
    "    # word = huruf awal (Unicode letter) lalu susulan \\w/- (biar aman utk bahasa Indo/Inggris)\n",
    "    # gunakan UNICODE agar .isupper()/islower() relevan\n",
    "    pat = re.compile(r'(?P<left>\\b[^\\W\\d_][\\w\\-]*)\\s*</em>\\.\\s+(?P<right>[^\\W\\d_][\\w\\-]*)', re.UNICODE)\n",
    "\n",
    "    out = []\n",
    "    last = 0\n",
    "    for m in pat.finditer(text):\n",
    "        lw = m.group('left')\n",
    "        rw = m.group('right')\n",
    "\n",
    "        l_upper = lw[0].isupper()\n",
    "        l_lower = lw[0].islower()\n",
    "        r_upper = rw[0].isupper()\n",
    "        r_lower = rw[0].islower()\n",
    "\n",
    "        # default: boundary kalimat\n",
    "        sep = \". \"\n",
    "        new_lw = lw\n",
    "\n",
    "        if l_lower and r_lower:\n",
    "            sep = \", \"\n",
    "            new_lw = lw\n",
    "        elif l_upper and r_upper:\n",
    "            sep = \". \"\n",
    "            new_lw = lw\n",
    "        elif l_upper and r_lower:\n",
    "            sep = \", \"\n",
    "            # decapitalize kata sebelum\n",
    "            new_lw = lw[0].lower() + lw[1:]\n",
    "        elif l_lower and r_upper:\n",
    "            sep = \". \"\n",
    "            new_lw = lw\n",
    "\n",
    "        # tambahkan bagian sebelum match\n",
    "        out.append(text[last:m.start('left')])\n",
    "        # susun pengganti\n",
    "        out.append(new_lw + sep + rw)\n",
    "        last = m.end('right')\n",
    "\n",
    "    out.append(text[last:])\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitField(rawData): #Data String Besar\n",
    "    # Memecah string besar menjadi potongan-potongan data per-insert.\n",
    "    # Setiap elemen dalam list ini berisi 9 data proposal.\n",
    "    dataPerInsert = []\n",
    "\n",
    "    # Memecah 9 data proposal dari setiap elemen dataPerInsert\n",
    "    # menjadi list yang setiap elemennya adalah 1 data proposal.\n",
    "    proposalList = []\n",
    "\n",
    "    # Mengelompokkan data proposal yang telah dipisahkan\n",
    "    # berdasarkan ID-nya.\n",
    "    groupedProposalsById = []\n",
    "\n",
    "    # Memisahkan setiap data proposal menjadi bagian-bagian yang lebih kecil\n",
    "    # sesuai struktur field. Namun, beberapa field yang seharusnya digabung\n",
    "    # ikut terpisah.\n",
    "    rawSplitFields = []\n",
    "\n",
    "    # Menggabungkan kembali bagian-bagian field yang terpisah dari\n",
    "    # rawSplitFields.\n",
    "    recombinedFields = []\n",
    "\n",
    "    dataFinal = [] #Data proposal yang sudah sesuai struktur proposal\n",
    "    kalimat = \"\" #Variabel untuk menampung kalimat yang terpisah untuk digabung kembali\n",
    "    kalimatSebelumnya = \"\" #Variabel untuk menampung kalimat sebelumnya yang ketika sudah lengkap akan dimasukkan ke dalam variable kalimat sebelum di insert ke data proposal\n",
    "    StringBuka = False #Variable untuk mendeteksi apakah ada tanda ' yang belum tertutup dalam sebuah string\n",
    "\n",
    "    totalRemoveProposal = 0 #Variabel untuk menampung jumlah proposal yang dihapus karena judul kosong\n",
    "    dataPerInsert = rawData.split(\"');\")\n",
    "    for i in dataPerInsert :\n",
    "        proposalList = i.split(\"'),\")\n",
    "        for j in proposalList :\n",
    "            groupedProposalsById.append(j)\n",
    "\n",
    "    for i in groupedProposalsById :\n",
    "        if len(recombinedFields) == 19 :\n",
    "            # if recombinedFields[0] == '1169':\n",
    "            #     print(\"debug\")\n",
    "            # if recombinedFields[0] == '(380' :\n",
    "            #     print(\"debug\")\n",
    "            if recombinedFields[3] == \"\" or recombinedFields[3] == \"-\" or recombinedFields[3] == \" '-'\" or recombinedFields[3] == \"''\" or recombinedFields[3] == \" ' '\" or recombinedFields[3] == \" \" or recombinedFields[3] == \"  \" or recombinedFields[3] == \" ''\":\n",
    "                totalRemoveProposal = totalRemoveProposal + 1\n",
    "                print(recombinedFields[0])\n",
    "                recombinedFields = []\n",
    "            elif len(recombinedFields[4].split()) < 2 and len(recombinedFields[5].split()) < 2 and len(recombinedFields[6].split()) < 2 and len(recombinedFields[7].split()) < 2 and len(recombinedFields[8].split()) < 2 :\n",
    "                totalRemoveProposal = totalRemoveProposal + 1\n",
    "                print(recombinedFields[0])\n",
    "                recombinedFields = []\n",
    "            else :\n",
    "                index = 0\n",
    "                for field in recombinedFields :\n",
    "                    listChar = list(field)\n",
    "                    for indexChar in range(len(listChar)) :\n",
    "                        if is_alpha(listChar[indexChar]) or is_numeric(listChar[indexChar]):\n",
    "                            recombinedFields[index] = field\n",
    "                            index = index + 1\n",
    "                            break\n",
    "                        if  (is_space_or_punct_or_symbol(listChar[indexChar]) or listChar[indexChar] in (\"'\", '\"', \".\", \",\", \":\", \";\", \"!\", \"?\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"-\", \"_\", \"/\", \"\\\\\", \"&\", \"%\", \"$\", \"#\", \"@\", \"*\", \"+\", \"=\", \"~\", \"`\", \"^\")):\n",
    "                            EXCLUDE = (\"'\", '\"')  # yang TIDAK dihapus\n",
    "                            if listChar[indexChar] not in EXCLUDE:\n",
    "                                listChar[indexChar] = \"\"  # cek bukan kutip\n",
    "                                field = \"\".join(listChar)\n",
    "                            else :\n",
    "                                if is_alpha(listChar[indexChar+1]) or is_numeric(listChar[indexChar+1]):\n",
    "                                    recombinedFields[index] = field\n",
    "                                    index = index + 1\n",
    "                                    break\n",
    "                                    # print(listChar[indexChar+1])\n",
    "                                else :\n",
    "                                    listChar[indexChar+1] = \"\"  # cek bukan kutip\n",
    "                                    field = \"\".join(listChar)\n",
    "                                    recombinedFields[index] = field\n",
    "                                    index = index + 1\n",
    "                                    break\n",
    "                dataFinal.append(recombinedFields)\n",
    "\n",
    "                recombinedFields = []\n",
    "        debug  = \"sabar\"\n",
    "        debugIndex = 0\n",
    "        rawSplitFields = i.split(\",\")\n",
    "        for j in rawSplitFields  :\n",
    "            if (rawSplitFields[len(rawSplitFields)- 1] == j):\n",
    "                recombinedFields.append(j)\n",
    "            else :\n",
    "\n",
    "                if j == '(3328' :\n",
    "                    debug = \"gas\"\n",
    "                    print(\"debug\")\n",
    "                if debug == \"gas\" :\n",
    "                    debugIndex = debugIndex + 1\n",
    "                    if debugIndex == 13 :\n",
    "                        print(\"debug\")\n",
    "                if len(j) != 0:\n",
    "                    if j.count(\"'\") == 1 and StringBuka == False: #Mendeteksi apakah Kutip String belun dibuka dan hanya terdapat 1 kutip untuk menandakan bahwa kalimat itu memiliki kelanjutan sehingga hasil akhir kalimat tidak akan langsung diinputkan ke dalam data proposal\n",
    "                        kalimat = kalimat + \", \" + j #kalimat pertama masuk ke sini\n",
    "                        StringBuka = True\n",
    "                    elif j.count(\"'\") == 0 and StringBuka == True: #Mendeteksi apakah kutip string sudah dibuka dan tidak ada kutip pada kata tersebut untuk menandakan kalimat akan terus berlanjut atau memilikii kelanjutan\n",
    "                        listCharKalimat = list(j) #Pengecekan apakah ini merupakan kalimat yang akhir atau hanya kalimat yang terpisah hasil split tanda (,)\n",
    "                        for indexChara in range(len(listCharKalimat)) :\n",
    "                            if is_alpha(listCharKalimat[indexChara]) or is_numeric(listCharKalimat[indexChara]):\n",
    "                                kalimat = kalimat + \", \" + j\n",
    "                                break\n",
    "                            if  (is_space_or_punct_or_symbol(listCharKalimat[indexChara]) or listCharKalimat[indexChara] in (\"'\", '\"', \".\", \",\", \":\", \";\", \"!\", \"?\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"-\", \"_\", \"/\", \"\\\\\", \"&\", \"%\", \"$\", \"#\", \"@\", \"*\", \"+\", \"=\", \"~\", \"`\", \"^\")):\n",
    "                                EXCLUDE = (\"'\", '\"','.')  # yang TIDAK dihapus\n",
    "                                if listCharKalimat[indexChara] not in EXCLUDE:\n",
    "                                    listCharKalimat[indexChara] = \"\"\n",
    "                                else:\n",
    "                                    aha = \"\".join(listCharKalimat)\n",
    "                                    kalimat = kalimat + aha\n",
    "                                    break\n",
    "                    elif j.count(\"'\") >= 1 and StringBuka == True and (re.findall(r\"\\w+'\\w+|\\w+'\\s\\w+|'\\w+|\\b\\w+'\\w*\\b|'\\w[\\w\\s]*?'|'\\w+\",j)) : #Mendeteksi apakah kutip string sudah dibuka dan dalam kalimat terdeteksi lebih atau setidaknya terdapat 1 kutip dalam kalimat, kemudian mendeteksi apakah kutip itu merepakan kutip penutup atau kutip dari nama orang, jalan, tempat atau suatu kata sehingga menyatakan kalimat belum berakhir)\n",
    "                        if  (j[-1]==\"'\" and j[-2]==\".\") : #Mendeteksi apakah kutip itu merupakan kutip penutup kalimat dengan melihat adanya titik sebelum kutip penutup\n",
    "                            kalimatSebelumnya = \"\" + kalimat\n",
    "                            kalimat = kalimat + \", \" + j\n",
    "                            recombinedFields.append(kalimat)\n",
    "                            # writer.writerow([recombinedFields[0], kalimat])\n",
    "                            kalimat = \"\"\n",
    "                            StringBuka = False\n",
    "\n",
    "                        else  :\n",
    "                            kalimat = kalimat + \", \" + j\n",
    "                    elif j.count(\"'\") == 1 and StringBuka == True and ( not re.findall(r\"\\w+'\\w+|\\w+'\\s\\w+\",j) or not re.findall(r\"\\b\\w+'\\w*\\b|'\\w[\\w\\s]*?'\",j)) : #Mendeteksi apakah hanya terdapat 1 kutip dan kutip itu berada dipaling belakang sebagai penutup kalimat\n",
    "                        kalimatLast =\"\" #Untuk mengatasi kelebihan tanda baca , pada akhir kalimat\n",
    "                        if len(j) < 4:\n",
    "                            for k in j :\n",
    "                                if k != \" \":\n",
    "                                    kalimatLast = kalimatLast + k\n",
    "                            recombinedFields.append(kalimat + kalimatLast)\n",
    "                            kalimat = \"\"\n",
    "                            StringBuka = False\n",
    "                            continue\n",
    "                        recombinedFields.append(kalimat + \", \" + j)\n",
    "                        # writer.writerow([recombinedFields[0], kalimat])\n",
    "                        kalimat = \"\"\n",
    "                        StringBuka = False\n",
    "                    elif j.count(\"'\") == 2 : #Medeteksi apa bila terdeteksi 2 kutip dalam 1 kalimat sebagai pembuka dan penutup, bila terdapat 2  dan bukan penutup atau pembuka dia akan masuk pada elif kedua terlebih dahulu, biasa kalimat pendek masuk ke dalam sini\n",
    "                        recombinedFields.append(j) #judul masuk disini\n",
    "                    elif j.count(\"'\") == 0 and j[len(j)-1].isdigit() and j[0].isdigit() : #Mendeteksi apabila value dalam string hanya berupa angka dan bukan string alfabet\n",
    "                        recombinedFields.append(j)\n",
    "                    else : #Diluar dari semua itu akan dianggap sebagai sebuah field yang tidak terdeteksi sama sekali karena alasan tertentu\n",
    "                        recombinedFields.append(j)\n",
    "    print(\"Total Remove Proposal: \", totalRemoveProposal)\n",
    "    return dataFinal\n",
    "daftarProposal = []\n",
    "data = \"INSERT INTO `proposals` (`id`, `nim`, `periode_id`, `judul`, `latar_belakang`, `rumusan`, `batasan`, `tujuan`, `referensi`, `status`, `user_id`, `modified`, `filename`, `filedir`, `mime_type`, `manfaat`, `revisi_dari`, `dosen_id`, `upload_revisi`) VALUES\" + data\n",
    "data = removeHTMLString(data) #Penghapusan HTML tag pada data\n",
    "daftarProposal = splitField(data)\n",
    "print(\"Jumlah data setelah diproses: \", len(daftarProposal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d74d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "daftarParagraf = []\n",
    "for proposal in daftarProposal :\n",
    "    daftarParagraf.append([proposal[0],\"Judul\",proposal[3]])\n",
    "    daftarParagraf.append([proposal[0],\"LatarBelakang\",proposal[4]])\n",
    "    daftarParagraf.append([proposal[0],\"Rumusan\",proposal[5]])\n",
    "    daftarParagraf.append([proposal[0],\"Tujuan\",proposal[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21ec1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Paragraf Latar Belakang:  1279\n",
      "Jumlah Paragraf Judul:  1279\n",
      "Jumlah Paragraf Rumusan:  1279\n",
      "Jumlah Paragraf Tujuan:  1279\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"debug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e02b89",
   "metadata": {},
   "source": [
    "Code untuk memecah proposal pada bagian latar belakang dan tujuan menjadi kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd1158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Paragraf: 5116\n",
      "Jumlah Kalimat: 24509\n",
      "Jumlah Link yang disimpan: 124\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ======================= UTIL: Masking & Unmasking URL =======================\n",
    "# Kita anggap data paragraf aslinya aman (tidak terpecah), jadi regex URL yang \"normal\" cukup\n",
    "_URL_PATTERN = re.compile(\n",
    "    r'(?i)\\b(?:https?://|www\\.)[^\\s<>()]+'  # http(s)://... atau www....\n",
    ")\n",
    "\n",
    "# ====== PRE-HEAL ringan untuk domain terpisah (ClickTale. Com, dll.) ======\n",
    "def pre_heal_domains(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    # “example. com” / “ClickTale. Com” → “example.com” / “ClickTale.Com”\n",
    "    TLD = r'(?:com|net|org|edu|gov|mil|id|co\\.id|ac\\.id|go\\.id|sch\\.id|my\\.id|io|ai)'\n",
    "    text = re.sub(rf'\\b([A-Za-z0-9-]{{2,}})\\.\\s*({TLD})\\b', r'\\1.\\2', text, flags=re.I)\n",
    "    # “www . example . com” → “www.example.com” (jaga-jaga)\n",
    "    text = re.sub(r'(?i)\\bwww\\s*\\.\\s*', 'www.', text)\n",
    "    text = re.sub(r'(?i)\\b([A-Za-z0-9-]{2,})\\s*\\.\\s*', r'\\1.', text)  # hanya saat terlihat rantai domain\n",
    "    return text\n",
    "\n",
    "\n",
    "def mask_links(text: str, daftarLink: list) -> str:\n",
    "    \"\"\"\n",
    "    Deteksi URL, simpan ke daftarLink, lalu ganti di teks menjadi token $daftarLink{idx}.\n",
    "    Trailing punctuation seperti '.', ',', ')', ';', ':' akan dipertahankan di luar token.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    def _repl(m: re.Match) -> str:\n",
    "        raw = m.group(0)\n",
    "        # Pisahkan trailing punctuations yang tak termasuk URL\n",
    "        trail = ''\n",
    "        while raw and raw[-1] in '.,;:)]':\n",
    "            trail = raw[-1] + trail\n",
    "            raw = raw[:-1]\n",
    "        idx = len(daftarLink)\n",
    "        daftarLink.append(raw)\n",
    "        return f\"$daftarLink{idx}{trail}\"\n",
    "\n",
    "    return _URL_PATTERN.sub(_repl, text)\n",
    "\n",
    "def unmask_links(text: str, daftarLink: list) -> str:\n",
    "    \"\"\"Kembalikan token $daftarLink{idx} menjadi URL aslinya.\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    return re.sub(\n",
    "        r'\\$daftarLink(\\d+)',\n",
    "        lambda m: daftarLink[int(m.group(1))] if int(m.group(1)) < len(daftarLink) else m.group(0),\n",
    "        text\n",
    "    )\n",
    "\n",
    "# ======================= Normalisasi whitespace ringan =======================\n",
    "def normalize_ws(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    ELLIPS = \"<ELLIPS_TOKEN>\"\n",
    "    s = s.replace(\"...\", \"…\").replace(\"…\", ELLIPS)\n",
    "\n",
    "    # kompres pengulangan tanda baca umum\n",
    "    s = re.sub(r'(?<=\\d)\\.\\s+(?=\\d)', '.', s)\n",
    "    s = re.sub(r'\\.(?:\\s*\\.)+', '.', s)\n",
    "    s = re.sub(r'([!?])\\1+', r'\\1', s)\n",
    "    s = re.sub(r'([,;:])\\s*\\1+', r'\\1', s)\n",
    "\n",
    "    # whitespace\n",
    "    s = s.replace(\"\\u00A0\", \" \")\n",
    "    s = re.sub(r\"[ \\t\\r\\f\\v]+\", \" \", s)\n",
    "\n",
    "    # spasi sebelum penutup\n",
    "    s = re.sub(r\"\\s+([,.;?!)\\]\\}\\\"'])\", r\"\\1\", s)\n",
    "    # spasi setelah tanda baca umum (kecuali diikuti penutup/petik)\n",
    "    s = re.sub(r\"(?<!\\d)([,;?!\\.])(?!\\d|[)\\]]|['\\\"]) (?=\\S)\", r\"\\1 \", s)\n",
    "\n",
    "    # titik ribuan/desimal yang sering kepisah\n",
    "    s = re.sub(r\"(?<=\\d)\\s*\\.\\s*(?=\\d{3}\\b)\", \".\", s)\n",
    "    s = re.sub(r\"(?<=\\d)\\s*\\.\\s*(?=\\d)\", \".\", s)\n",
    "\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    s = s.replace(ELLIPS, \"…\")\n",
    "    return s.strip()\n",
    "\n",
    "# ===== Fallback clean_html =====\n",
    "def clean_html(text, keep_linebreaks=True):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    t = re.sub(r\"(?is)<br\\s*/?>\", \"\\n\", text) if keep_linebreaks else text\n",
    "    t = re.sub(r\"(?is)<[^>]+>\", \" \", t)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "\n",
    "# -------------------- Proteksi titik yang bukan akhir kalimat -----------------\n",
    "PROTECTED  = \"<DOT>\"\n",
    "PCOLON     = \"<COLON>\"\n",
    "_END_PRIORITY = {\"?\": 3, \"!\": 2, \".\": 1, \"…\": 1}\n",
    "\n",
    "def protect_dynamic(text: str) -> str:\n",
    "    s = text\n",
    "\n",
    "    # Gelar medis Sp.X\n",
    "    s = re.sub(r'\\bSp\\s*\\.\\s*([A-Z]{1,3})\\b', lambda m: 'Sp' + PROTECTED + m.group(1), s)\n",
    "\n",
    "    # Gelar di depan nama (dr., drs., ir.)\n",
    "    s = re.sub(r'\\b(dr|drs|ir)\\s*\\.\\s*(?=[A-Z])', lambda m: m.group(1) + PROTECTED + ' ', s, flags=re.I)\n",
    "\n",
    "    # enumerasi \"1.\" / \"a.\"\n",
    "    s = re.sub(r\"(?:(?<=^)|(?<=[\\s\\\"'(\\[\\{<:;?!\\.]))(\\d{1,3})\\.(?=\\s*\\S)\", r\"\\1\" + PROTECTED, s)\n",
    "    s = re.sub(r\"(?:(?<=^)|(?<=[\\s\\\"'(\\[\\{<:;?!\\.]))([A-Za-z])\\.(?=\\s*\\S)\", r\"\\1\" + PROTECTED, s)\n",
    "    # juga varian \")\"\n",
    "    s = re.sub(r\"(?:(?<=^)|(?<=[\\s\\\"'(\\[\\{<:;?!\\.]))(\\d{1,3})\\)(?=\\s*\\S)\", r\"\\1)\", s)\n",
    "    s = re.sub(r\"(?:(?<=^)|(?<=[\\s\\\"'(\\[\\{<:;?!\\.]))([A-Za-z])\\)(?=\\s*\\S)\", r\"\\1)\", s)\n",
    "\n",
    "    # ellipsis → lindungi titiknya\n",
    "    s = re.sub(r\"\\.\\.\\.\", lambda m: m.group(0).replace(\".\", PROTECTED), s)\n",
    "\n",
    "    # Prof. dr.\n",
    "    s = re.sub(r\"\\bProf\\.\\s*dr\\.\", lambda m: m.group(0).replace(\".\", PROTECTED), s)\n",
    "\n",
    "    # Multi-dot abbr (S.Kom., Ph.D., d.s.b.)\n",
    "    s = re.sub(r\"\\b(?:[A-Za-z]{1,4}\\.){2,}(?=[\\s,;:)]|$)\", lambda m: m.group(0).replace(\".\", PROTECTED), s)\n",
    "\n",
    "    # whitelist lowercase abbr\n",
    "    LOWER_ABBR_EXT = (\"dll\", \"dsb\", \"dst\", \"dkk\", \"al\", \"ibid\", \"op\", \"cit\", \"vs\", \"hlm\", \"hal\")\n",
    "    s = re.sub(rf\"\\b(?:{'|'.join(LOWER_ABBR_EXT)})\\.(?=\\s)\", lambda m: m.group(0).replace(\".\", PROTECTED), s, flags=re.I)\n",
    "\n",
    "    # TitleCase short abbr (No., Jl., Prof., St.)\n",
    "    s = re.sub(r\"\\b([A-Z][a-z]{0,3})\\.(?=\\s*[a-z0-9(])\", r\"\\1\" + PROTECTED, s)\n",
    "    s = re.sub(r\"\\b([A-Z][a-z]{0,3})\\.(?=\\s*[A-Z][a-z])\", r\"\\1\" + PROTECTED, s)\n",
    "    s = re.sub(r'\\b(St)\\.(?=\\s)', r'\\1' + PROTECTED, s)\n",
    "\n",
    "    # inisial berurutan (R. A.)\n",
    "    s = re.sub(r\"\\b([A-Z])\\.(?=\\s*[A-Z])\", r\"\\1\" + PROTECTED, s)\n",
    "\n",
    "    # Dotted ALLCAP suffix: Node.JS, Socket.IO\n",
    "    s = re.sub(r\"(?<=\\b[A-Za-z])\\.(?=[A-Z]{2,5}\\b)\", PROTECTED, s)\n",
    "\n",
    "    # Kode regulasi/nomor: KA.401 dst.\n",
    "    s = re.sub(r'\\b([A-Z]{1,4})\\.(\\d+)\\b', lambda m: m.group(0).replace('.', PROTECTED), s)\n",
    "\n",
    "    # rantai kode bertitik (AHU-0016081.AH.01.01)\n",
    "    s = re.sub(r\"\\b([A-Z0-9-]+(?:\\.[A-Z0-9-]+){1,})\\b\",\n",
    "               lambda m: m.group(1).replace(\".\", PROTECTED), s)\n",
    "\n",
    "    # desimal 1.23\n",
    "    s = re.sub(r\"(?<=\\d)\\.(?=\\d)\", PROTECTED, s)\n",
    "\n",
    "    # titik sebelum tahun di dalam kurung (…., 2014)\n",
    "    s = re.sub(r\"\\(([^)]*)\\.(?=\\s*\\d{4}\\))\", lambda m: m.group(0).replace(\".\", PROTECTED), s)\n",
    "\n",
    "    # pola (*.JPG)\n",
    "    s = re.sub(r\"\\(\\*\\.[A-Za-z0-9]{1,5}\\)\", lambda m: m.group(0).replace(\".\", PROTECTED), s)\n",
    "\n",
    "    # 'et al.' dan 'dkk.'\n",
    "    s = re.sub(r'\\bet\\.\\s*al\\.', lambda m: m.group(0).replace('.', PROTECTED), s, flags=re.I)\n",
    "    s = re.sub(r'\\bdkk\\.', lambda m: m.group(0).replace('.', PROTECTED), s, flags=re.I)\n",
    "\n",
    "    # Titik antara kapital dan digit (KA.401)\n",
    "    s = re.sub(r'(?<=\\b[A-Z])\\.(?=\\d)', PROTECTED, s)\n",
    "\n",
    "    # *** Tambahan kecil untuk \"24 / 7\" → \"24/7\" ***\n",
    "    s = re.sub(r'(\\d+)\\s*/\\s*(\\d+)', r'\\1/\\2', s)\n",
    "\n",
    "    # 1) Lindungi titik DI DALAM kurung pada pola \"(Nama, 2016).\" tapi biarkan titik sesudah kurung\n",
    "    s = re.sub(\n",
    "        r'\\(([^()]*?,\\s*\\d{4}[a-z]?[^()]*)\\)\\.',\n",
    "        lambda m: '(' + m.group(1).replace('.', PROTECTED) + ').',\n",
    "        s\n",
    "    )\n",
    "\n",
    "    # Derajat/gelar populer\n",
    "    s = re.sub(r'\\bPh\\s*\\.\\s*D\\b', lambda m: m.group(0).replace('.', PROTECTED), s)\n",
    "    s = re.sub(r'\\bM\\s*\\.\\s*[Ss]c\\b\\.?', lambda m: m.group(0).replace('.', PROTECTED), s)\n",
    "    s = re.sub(r'\\bS\\s*\\.\\s*Kom\\b\\.?', lambda m: m.group(0).replace('.', PROTECTED), s)\n",
    "    s = re.sub(r'\\bD\\s*\\.\\s*I\\s*\\.\\s*C\\b\\.?', lambda m: m.group(0).replace('.', PROTECTED), s)\n",
    "\n",
    "    # Ir.Nizam → Ir.<DOT> Nizam\n",
    "    s = re.sub(r'\\b(Ir)\\s*\\.(?=[A-Z])', r'\\1' + PROTECTED + ' ', s)\n",
    "\n",
    "    # 2) Varian titik setelah kurung di akhir string\n",
    "    s = re.sub(\n",
    "        r'\\(([^()]*?,\\s*\\d{4}[a-z]?[^()]*)\\)\\.\\s*$',\n",
    "        lambda m: '(' + m.group(1).replace('.', PROTECTED) + ').',\n",
    "        s\n",
    "    )\n",
    "\n",
    "    # (*.EXT) varian ber-spasi\n",
    "    s = re.sub(r\"\\(\\s*\\*\\s*\\.\\s*[A-Za-z0-9]{1,5}\\s*\\)\", lambda m: m.group(0).replace(\".\", PROTECTED), s)\n",
    "\n",
    "    # Nama.Inisial → lindungi\n",
    "    s = re.sub(r'\\b([A-Z][a-z]+)\\.(?=[A-Z]\\b)', lambda m: m.group(1) + PROTECTED, s)\n",
    "\n",
    "    # KAPITAL.DIGIT varian spasi\n",
    "    s = re.sub(r'\\b([A-Z]{1,4})\\s*\\.\\s*(\\d+)\\b', lambda m: m.group(1) + PROTECTED + m.group(2), s)\n",
    "\n",
    "    # RANTAI KODE DENGAN SPASI\n",
    "    s = re.sub(\n",
    "        r'\\b([A-Z]{1,5})\\s*\\.\\s*(\\d+)/([A-Z]{1,5})\\s*\\.\\s*(\\d+)/([A-Z]{2,6})/(\\d{4})\\b',\n",
    "        lambda m: (m.group(1) + PROTECTED + m.group(2) + '/' +\n",
    "                   m.group(3) + PROTECTED + m.group(4) + '/' +\n",
    "                   m.group(5) + '/' + m.group(6)),\n",
    "        s\n",
    "    )\n",
    "\n",
    "    return s\n",
    "\n",
    "def unprotect(text: str) -> str:\n",
    "    return text.replace(PROTECTED, \".\").replace(PCOLON, \":\")\n",
    "\n",
    "# ====== Anti-glue post-pass (tanpa logic URL) ======\n",
    "def anti_glue_overmerged(seg: str):\n",
    "    if not seg or seg.rstrip().endswith(\":\"):\n",
    "        return [seg]\n",
    "\n",
    "    t = seg.strip()\n",
    "    if t.count('(') != t.count(')'):\n",
    "        return [seg]\n",
    "    if re.search(r'\\.\\s+\\((?=[^)]*\\d{4})', t):\n",
    "        return [seg]\n",
    "    if re.match(r'^\\(\\s*[^()]*\\d{4}[^()]*\\)\\.?$', t):\n",
    "        return [seg]\n",
    "\n",
    "    prot = protect_dynamic(t)\n",
    "    # FORCE_END setelah \"). \" bila diikuti kata sitasi umum\n",
    "    prot = re.sub(r'\\)\\.\\s+(?=(?:diakses|diunduh|accessed|retrieved)\\b)', '). <FORCE_END> ', prot, flags=re.I)\n",
    "    split_re = re.compile(r'(?:(?<=[\\.!?…])|\\<FORCE_END\\>)\\s+(?=(?:[A-Z0-9\"\"])|(?!\\s)\\()')\n",
    "\n",
    "    parts = split_re.split(prot)\n",
    "    parts = [unprotect(p).strip() for p in parts if p and p.strip()]\n",
    "    parts = [p.replace('<FORCE_END>', '').strip() for p in parts if p and p.strip()]\n",
    "\n",
    "    if len(parts) >= 2 and any(\" \" in p for p in parts):\n",
    "        return parts\n",
    "    return [seg]\n",
    "\n",
    "# -------------------- Split per kalimat (robust, tanpa URL-rules) --------------------\n",
    "def _choose_ender(cluster: str) -> str:\n",
    "    best = \".\"\n",
    "    best_score = 0\n",
    "    for ch in cluster:\n",
    "        score = _END_PRIORITY.get(ch, 0)\n",
    "        if score > best_score:\n",
    "            best, best_score = ch, score\n",
    "    return best\n",
    "\n",
    "def preclean_minimal(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    ELLIPS = \"<ELLIPS_TOKEN>\"\n",
    "    s = s.replace(\"...\", \"…\").replace(\"…\", ELLIPS)\n",
    "    s = re.sub(r'(?<=\\d)\\.\\s+(?=\\d)', '.', s)\n",
    "    s = re.sub(r'\\.(?:\\s*\\.)+', '.', s)\n",
    "    s = re.sub(r'([!?])\\1+', r'\\1', s)\n",
    "    s = re.sub(r'([,;:])\\s*\\1+', r'\\1', s)\n",
    "    # buang penutup kurung yatim\n",
    "    s = re.sub(r'([\\.!?])\\s*[\\)\\]\\}]+(\\s+)(?=[A-Z(\\\"\\'\"])', r'\\1\\2', s)\n",
    "    s = re.sub(r'([\\.!?])\\s*[\\)\\]\\}]+\\s*$', r'\\1', s)\n",
    "    s = s.replace(ELLIPS, \"…\")\n",
    "    return s\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    cleaned = clean_html(text, keep_linebreaks=True)\n",
    "    cleaned = preclean_minimal(cleaned)\n",
    "    s = protect_dynamic(cleaned).strip()\n",
    "\n",
    "    END   = r\"[\\.?!;:…]\"\n",
    "    CLOSE = r\"[\\\"')\\]\\}]\"\n",
    "    OPEN  = r\"[\\\"'(\\[\\{<]\"\n",
    "    BUL   = r\"[-–—•·*]+\"\n",
    "    sep = re.compile(rf\"({END}+)(?:{CLOSE}+)?(?:\\s*{BUL})?(?=(?:\\s+|$|{OPEN}|[A-Z0-9]|{BUL}))\")\n",
    "\n",
    "    # pola sitasi\n",
    "    citation_paren_re = re.compile(r\"\"\"^\\(\\s*[^()]*\\d{4}[a-z]?[^()]*\\)\\.?$\"\"\", re.I | re.VERBOSE)\n",
    "    citation_author_year_re = re.compile(r\"\"\"^[A-Z][A-Za-z .'\\-]+?,\\s*(?:[A-Z]\\.\\s*)+\n",
    "                                             (?:,(?:\\s*&|\\s*and|\\s*)[A-Z][A-Za-z .'\\-]+?,\\s*(?:[A-Z]\\.\\s*)+)*\n",
    "                                             (?:,\\s*et\\s+al\\.)?\\s*\\(\\d{4}[a-z]?\\)\\.?$\"\"\",\n",
    "                                         re.I | re.VERBOSE)\n",
    "    surname_only = re.compile(r'^[A-Z][A-Za-z.\\- ]+\\.\\s*$', re.I)\n",
    "    surname_with_year_paren = re.compile(r\"\"\"^[A-Z][A-Za-z .'\\-]+?\\.\\s*\\(\\d{4}[a-z]?[^)]*\\)\\.?$\"\"\",\n",
    "                                         re.I | re.VERBOSE)\n",
    "\n",
    "    # split awal\n",
    "    parts, last = [], 0\n",
    "    for m in sep.finditer(s):\n",
    "        seg = s[last:m.end()].strip()\n",
    "        if seg:\n",
    "            parts.append(seg)\n",
    "        last = m.end()\n",
    "    tail = s[last:].strip()\n",
    "    if tail:\n",
    "        parts.append(tail)\n",
    "\n",
    "    # tidy + unprotect\n",
    "    parts = [re.sub(rf\"^\\s*{BUL}\\s*\", \"\", p) for p in parts]\n",
    "    parts = [re.sub(r'^[\\'\\\"]+', \"\", p) for p in parts]\n",
    "    parts = [re.sub(r'[\\'\\\"]+$', \"\", p) for p in parts]\n",
    "    parts = [unprotect(p).strip() for p in parts if p.strip()]\n",
    "\n",
    "    # GLUE: token enumerasi yang yatim\n",
    "    glued, i = [], 0\n",
    "    enum_token_re = re.compile(r'^[\\s\"\\'(\\[]([0-9]{1,3}|[A-Za-z])\\.\\s*$')\n",
    "    while i < len(parts):\n",
    "        cur = parts[i]\n",
    "        if enum_token_re.match(cur) and i + 1 < len(parts):\n",
    "            token = enum_token_re.match(cur).group(1) + \".\"\n",
    "            glued.append(f\"{token} {parts[i+1].lstrip()}\")\n",
    "            i += 2\n",
    "        else:\n",
    "            glued.append(cur)\n",
    "            i += 1\n",
    "\n",
    "    # GLUE: segmen berakhir \":\" → satukan hanya dgn segmen berikutnya\n",
    "    merged, buf = [], \"\"\n",
    "    for seg in glued:\n",
    "        if buf:\n",
    "            merged.append((buf + \" \" + seg).strip()); buf = \"\"\n",
    "        elif seg.rstrip().endswith(\":\"):\n",
    "            buf = seg\n",
    "        else:\n",
    "            merged.append(seg)\n",
    "    if buf:\n",
    "        merged.append(buf.strip())\n",
    "    final = merged\n",
    "\n",
    "    # GLUE: sitasi multi‐segmen sampai ')' muncul\n",
    "    combined, buf = [], None\n",
    "    for seg in final:\n",
    "        t = seg.strip()\n",
    "        if buf is not None:\n",
    "            buf = f\"{buf} {t}\"\n",
    "            if re.search(r'\\)\\.?$', t):\n",
    "                combined.append(buf.strip()); buf = None\n",
    "            continue\n",
    "        if re.match(r'^\\(', t) and not re.search(r'\\)\\.?$', t):\n",
    "            buf = t\n",
    "        elif re.search(r'\\(\\s*$', t) or re.search(r'\\bhlm\\.\\s*$', t, re.I):\n",
    "            buf = t\n",
    "        else:\n",
    "            combined.append(seg)\n",
    "    if buf:\n",
    "        combined.append(buf.strip())\n",
    "    final = combined\n",
    "\n",
    "    # GLUE: \"Ocepek.\" + \"(2013: 1-5).\"\n",
    "    glued_name_year = []\n",
    "    for seg in final:\n",
    "        if glued_name_year and surname_only.match(glued_name_year[-1]) and citation_paren_re.match(seg.strip()):\n",
    "            glued_name_year[-1] = f\"{glued_name_year[-1].rstrip()} {seg.strip()}\"\n",
    "        else:\n",
    "            glued_name_year.append(seg)\n",
    "    final = glued_name_year\n",
    "\n",
    "    # GLUE: sitasi utuh tempel ke sebelumnya\n",
    "    glued_citations = []\n",
    "    for seg in final:\n",
    "        _t = seg.strip()\n",
    "        if glued_citations and (\n",
    "            citation_paren_re.match(_t) or\n",
    "            citation_author_year_re.match(_t) or\n",
    "            surname_with_year_paren.match(_t)\n",
    "        ):\n",
    "            glued_citations[-1] = f\"{glued_citations[-1].rstrip()} {seg}\".strip()\n",
    "        else:\n",
    "            glued_citations.append(seg)\n",
    "    final = glued_citations\n",
    "\n",
    "    # orphan close seg\n",
    "    orphan_close_seg = re.compile(r'^[\\)\\]\\}]+\\.?$')\n",
    "    final2 = []\n",
    "    for seg in final:\n",
    "        if final2 and orphan_close_seg.match(seg):\n",
    "            continue\n",
    "        final2.append(seg)\n",
    "    final = final2\n",
    "\n",
    "    # \"15).\" tempel ke sebelumnya\n",
    "    glued4 = []\n",
    "    for seg in final:\n",
    "        if glued4 and re.match(r\"^\\d+\\)\\.?$\", seg):\n",
    "            glued4[-1] = glued4[-1].rstrip() + \" \" + seg\n",
    "        else:\n",
    "            glued4.append(seg)\n",
    "    final = glued4\n",
    "\n",
    "    # angka desimal/enumerasi yatim \"0.\" \"9).\" \"57).\"\n",
    "    orphan_num_dot = re.compile(r'^\\d+[\\.\\)]\\.?\\s*$')\n",
    "    glued_num = []\n",
    "    for seg in final:\n",
    "        t = seg.strip()\n",
    "        if glued_num and orphan_num_dot.match(t):\n",
    "            prev = glued_num[-1].rstrip()\n",
    "            if not prev or prev[-1] not in '.!?':\n",
    "                glued_num[-1] = (prev + \" \" + t).strip()\n",
    "            else:\n",
    "                glued_num.append(seg)\n",
    "        else:\n",
    "            glued_num.append(seg)\n",
    "    final = glued_num\n",
    "\n",
    "    # POST-PASS: pecah segmen yang masih memuat >1 kalimat\n",
    "    post = []\n",
    "    for seg in final:\n",
    "        post.extend(anti_glue_overmerged(seg))\n",
    "    final = [normalize_ws(x) for x in post]\n",
    "    return final\n",
    "\n",
    "# ============================= Pipeline utama ================================\n",
    "# Asumsikan kamu sudah membangun daftarParagraf seperti:\n",
    "# daftarParagraf = [\n",
    "#   [idProposal, \"Judul\", \"Isi\"],\n",
    "#   [idProposal, \"LatarBelakang\", \"Isi\"],\n",
    "#   [idProposal, \"Rumusan\", \"Isi\"],\n",
    "#   [idProposal, \"Tujuan\", \"Isi\"],\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "print(\"Jumlah Paragraf:\", len(daftarParagraf))\n",
    "\n",
    "daftarLink = []  # bank URL global\n",
    "daftarKalimatProposal = []\n",
    "\n",
    "def _tag_lower(t: str) -> str:\n",
    "    t = (t or \"\").strip()\n",
    "    mapping = {\n",
    "        \"Judul\": \"judul\",\n",
    "        \"LatarBelakang\": \"latar_belakang\",\n",
    "        \"Rumusan\": \"rumusan\",\n",
    "        \"Tujuan\": \"tujuan\"\n",
    "    }\n",
    "    return mapping.get(t, t.lower())\n",
    "\n",
    "for pid, tag, isi in daftarParagraf:\n",
    "    tag_norm = _tag_lower(tag)\n",
    "\n",
    "    if tag_norm == \"judul\":\n",
    "        # Judul langsung 1 baris (tetap mask/unmask untuk konsistensi)\n",
    "        healed = pre_heal_domains(isi or \"\")\n",
    "        masked = mask_links(healed, daftarLink)\n",
    "        kal = normalize_ws(masked)\n",
    "        kal = unmask_links(kal, daftarLink)\n",
    "        daftarKalimatProposal.append([pid, kal, \"judul\"])\n",
    "        continue\n",
    "\n",
    "    # Untuk paragraf lain: mask → split → unmask per kalimat\n",
    "    healed = pre_heal_domains(isi or \"\")\n",
    "    masked = mask_links(healed, daftarLink)\n",
    "    for kal in split_sentences(masked):\n",
    "        kal = normalize_ws(kal)\n",
    "        kal = unmask_links(kal, daftarLink)\n",
    "        if kal:\n",
    "            daftarKalimatProposal.append([pid, kal, tag_norm])\n",
    "\n",
    "# ============ POST-FLATTEN: pecah lagi item over-merged =============\n",
    "daftarKalimatProposal_fix = []\n",
    "for pid, kal, tag in daftarKalimatProposal:\n",
    "    parts = anti_glue_overmerged(kal)\n",
    "    if len(parts) > 1 and any(\" \" in p for p in parts):\n",
    "        for p in parts:\n",
    "            p = normalize_ws(p)\n",
    "            if p:\n",
    "                daftarKalimatProposal_fix.append([pid, p, tag])\n",
    "    else:\n",
    "        daftarKalimatProposal_fix.append([pid, normalize_ws(kal), tag])\n",
    "daftarKalimatProposal = daftarKalimatProposal_fix\n",
    "\n",
    "# (Opsional) Recovery kata nempel ekstrem (bukan URL terkait)\n",
    "def _recover_overmerged_words(t: str) -> str:\n",
    "    if re.search(r'[A-Za-z]{30,}', t):\n",
    "        t = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', t)\n",
    "        t = re.sub(r'([a-z]{5,})([a-z]{5,})', r'\\1 \\2', t)\n",
    "    return t\n",
    "\n",
    "# =================== POST-PASS LINTAS ITEM: re-glue cerdas ===================\n",
    "def cross_glue_items(items):\n",
    "    \"\"\"\n",
    "    Lintas-item (pid+tag sama) penggabungan:\n",
    "    - URL-only & (URL) & angka+URL → tempel ke sebelumnya.\n",
    "    - Ekor sitasi umum ( … (2016). ), sumber angka (24/7 … (2010).), paren tanpa tahun → tempel ke sebelumnya.\n",
    "    - “Risti?” + “(2014, …) mengatakan …” (segmen berikut diawali kurung+year lalu lanjut teks) → tempel.\n",
    "    - Brand. + Com/Net/Org/Id … → jadikan Brand.Com/Net/… (tanpa spasi nyasar).\n",
    "    - Frasa judul pendek (≤3 kata) / typo ':.', ';' → tempel ke berikutnya.\n",
    "    - Tail “diakses/accessed/retrieved …” → tempel ke sebelumnya.\n",
    "    - Singkatan badan/instansi & alamat: UD., CV., PT., TBK., RS., PD., MT., KH. → zip kiri/kanan.\n",
    "    - Node. JS / Socket. IO → tempel.\n",
    "    - Rangkaian setelah ':' (daftar/pertanyaan beruntun) → gulung.\n",
    "    \"\"\"\n",
    "    if not items:\n",
    "        return items\n",
    "\n",
    "    # --- Pola umum ---\n",
    "    url_only        = re.compile(r'^\\s*\\(?\\s*(?:https?://|www\\.)\\S+\\s*\\)?\\.?\\s*$', re.I)\n",
    "    num_then_url    = re.compile(r'^\\s*\\d+\\s+(?:https?://|www\\.)\\S+\\s*$', re.I)\n",
    "    paren_url_only  = re.compile(r'^\\s*\\(\\s*(?:https?://|www\\.)\\S+\\s*\\)\\.?\\s*$', re.I)\n",
    "\n",
    "    # sitasi klasik \"(... 2016 ...).\" satu baris penuh\n",
    "    tail_citation_paren = re.compile(r'^\\s*\\([^()]*\\d{4}[a-z]?\\s*[^()]*\\)\\.?\\s*$', re.I)\n",
    "    # \"Author ... (2016).\" (awalan huruf)\n",
    "    tail_author_year    = re.compile(r'^[A-Z][^()]{0,200}\\(\\s*\\d{4}[a-z]?\\s*\\)\\.?\\s*$', re.I)\n",
    "    # *** BARU: \"24/7 Wall St. (2010).\" (awalan angka/karakter non-huruf) ***\n",
    "    tail_source_year_num= re.compile(r'^[0-9][^()]{0,200}\\(\\s*\\d{4}[a-z]?\\s*\\)\\.?\\s*$', re.I)\n",
    "    # *** BARU: paren TANPA tahun (anggap sitasi) ***\n",
    "    tail_paren_no_year  = re.compile(r'^\\s*\\([^()]{1,120}\\)\\.?\\s*$', re.I)\n",
    "    # *** BARU: baris MULAI dengan (tahun …) LALU lanjut teks (untuk kasus \"Risti? (2014, hlm. 4) mengatakan …\") ***\n",
    "    starts_paren_year_then_text = re.compile(r'^\\s*\\(\\s*\\d{4}[^\\)]*\\)\\s+\\S', re.I)\n",
    "\n",
    "    # prev akhiri \"et al.\" / \"dkk.\"\n",
    "    prev_etal_end = re.compile(r'(?:et\\s+al|dkk)\\.\\s*$', re.I)\n",
    "\n",
    "    # singkatan korporat/alamat\n",
    "    corp_abbr_only   = re.compile(r'^\\s*(?:UD|CV|PT|TBK|RS|PD|MT|KH)\\.\\s*$', re.I)\n",
    "    ends_with_corp   = re.compile(r'(?:UD|CV|PT|TBK|RS|PD|MT|KH)\\.\\s*$', re.I)\n",
    "    short_name_line  = re.compile(r'^[A-Z][\\w&.\\'\\- ]{1,40}\\.\\s*$', re.U)\n",
    "    starts_with_name = re.compile(r'^\\s*[\\'\"“”‘’(]*[A-Z][A-Za-z0-9&.\\'\\-]*(?:\\s+[A-Z][A-Za-z0-9&.\\'\\-]*)*', re.U)\n",
    "\n",
    "    # domain/TLD\n",
    "    tld = r'(?:com|net|org|io|ai|id|co\\.id|ac\\.id|go\\.id|sch\\.id|my\\.id)'\n",
    "    domainish_end    = re.compile(rf'\\b[A-Za-z0-9-]{{2,}}\\.{tld}\\.\\s*$', re.I)\n",
    "    tld_only_start   = re.compile(rf'^{tld}\\b', re.I)\n",
    "    # *** BARU: prev \"Brand.\" + next \"Com/Net/...\" ***\n",
    "    prev_word_dot    = re.compile(r'\\b([A-Za-z][A-Za-z0-9-]{1,})\\.\\s*$')\n",
    "\n",
    "    # daftar/pertanyaan pendek\n",
    "    short_q = re.compile(r'^(?:per\\s+\\w+|\\w.{0,80}\\?)$', re.I)\n",
    "\n",
    "    # (IPK)., (CPU), (UX).\n",
    "    lone_paren_abbr  = re.compile(r'^\\s*\\([A-Za-z]{2,8}\\)\\.?\\s*$')\n",
    "\n",
    "    # Node. JS / Socket. IO\n",
    "    word_dot_prev    = re.compile(r'.*\\b[A-Za-z]{2,}\\.\\s*$')\n",
    "    next_allcaps     = re.compile(r'^[A-Z]{1,4}\\b')\n",
    "\n",
    "    # *** BARU: tail \"diakses/accessed/retrieved\" ***\n",
    "    tail_access_note = re.compile(r'\\b(diakses|accessed|retrieved)\\b', re.I)\n",
    "\n",
    "    def _unbalanced_paren_or_quote(s: str) -> bool:\n",
    "        s = s.strip()\n",
    "        if s.count('(') != s.count(')'):\n",
    "            return True\n",
    "        for left, right in [(\"'\", \"'\"), ('\"', '\"'), ('“','”'), ('‘','’')]:\n",
    "            if s.count(left) != s.count(right):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _is_very_short_opening(s: str) -> bool:\n",
    "        w = s.strip().split()\n",
    "        return len(w) <= 3 and s.endswith('.')\n",
    "\n",
    "    # *** BARU: frasa pendek capitalized tanpa tanda akhir (≤3 kata) → kandidat tempel ***\n",
    "    def _is_short_titleish(s: str) -> bool:\n",
    "        s = s.strip()\n",
    "        if re.search(r'[.!?]$', s):\n",
    "            return False\n",
    "        w = s.split()\n",
    "        return 1 <= len(w) <= 3 and w[0][:1].isupper()\n",
    "\n",
    "    out = []\n",
    "    i = 0\n",
    "    n = len(items)\n",
    "\n",
    "    while i < n:\n",
    "        pid, txt, tag = items[i]\n",
    "        t = normalize_ws(txt)\n",
    "\n",
    "        has_prev = bool(out) and out[-1][0] == pid and out[-1][2] == tag\n",
    "        prev_txt = out[-1][1] if has_prev else None\n",
    "\n",
    "        has_next = (i + 1 < n) and (items[i+1][0] == pid) and (items[i+1][2] == tag)\n",
    "        next_txt = normalize_ws(items[i+1][1]) if has_next else None\n",
    "\n",
    "        # 0) URL-only / (URL) / angka+URL\n",
    "        if (url_only.match(t) or paren_url_only.match(t) or num_then_url.match(t)) and has_prev:\n",
    "            out[-1][1] = normalize_ws(prev_txt.rstrip() + \" \" + t)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # 1) Pembuka sangat pendek ATAU typo ':.', ';' → gabung ke berikutnya\n",
    "        if has_next and (_is_very_short_opening(t) or t.rstrip().endswith(':.') or t.rstrip().endswith(';')):\n",
    "            out.append([pid, normalize_ws(t.rstrip() + \" \" + next_txt.lstrip()), tag])\n",
    "            i += 2\n",
    "            continue\n",
    "\n",
    "        # 1b) *** BARU: frasa pendek 'title-ish' (mis. \"Kepuasan (satisfaction)\") → tempel ke sebelumnya\n",
    "        if has_prev and _is_short_titleish(t):\n",
    "            out[-1][1] = normalize_ws(prev_txt.rstrip() + \" \" + t)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # 2) \"Risti?\" + \"(2014, hlm. 4) mengatakan …\" → jika next mulai \"(YYYY ... ) <teks>\"\n",
    "        if has_next and t.rstrip().endswith('?') and starts_paren_year_then_text.match(next_txt):\n",
    "            out.append([pid, normalize_ws(t + \" \" + next_txt), tag])\n",
    "            i += 2\n",
    "            continue\n",
    "        \n",
    "        # 2b) Prev berakhir \"et al.\" / \"dkk.\" + next mulai \"(YYYY) ...\": gabung\n",
    "        if has_prev and prev_etal_end.search(prev_txt) and starts_paren_year_then_text.match(t):\n",
    "            out[-1][1] = normalize_ws(prev_txt.rstrip() + \" \" + t)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        # 3) Ekor sitasi luas (huruf / angka / paren tanpa tahun) → tempel ke sebelumnya\n",
    "        if has_prev and (tail_citation_paren.match(t) or tail_author_year.match(t) or tail_source_year_num.match(t) or tail_paren_no_year.match(t)):\n",
    "            out[-1][1] = normalize_ws(prev_txt.rstrip() + \" \" + t)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # 3b) *** BARU: baris mengandung 'diakses/accessed/retrieved' → tempel ke sebelumnya\n",
    "        if has_prev and tail_access_note.search(t):\n",
    "            out[-1][1] = normalize_ws(prev_txt.rstrip() + \" \" + t)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # 4) UD./CV./PT./TBK./RS./PD./MT./KH. berdiri sendiri → zip kiri+kanan\n",
    "        if corp_abbr_only.match(t) and has_prev:\n",
    "            merged = prev_txt.rstrip() + \" \" + t\n",
    "            consumed = 1\n",
    "            if has_next:\n",
    "                merged += \" \" + next_txt\n",
    "                consumed = 2\n",
    "            out[-1][1] = normalize_ws(merged)\n",
    "            i += consumed\n",
    "            continue\n",
    "\n",
    "        # 5) Prev berakhir singkatan → tempel lanjutan nama/alamat/abbr dalam paren\n",
    "        if has_prev and ends_with_corp.search(prev_txt) and (starts_with_name.match(t) or short_name_line.match(t) or lone_paren_abbr.match(t)):\n",
    "            out[-1][1] = normalize_ws(prev_txt.rstrip() + \" \" + t)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # 6) Node. JS / Socket. IO\n",
    "        if has_prev and word_dot_prev.match(prev_txt) and next_allcaps.match(t):\n",
    "            out[-1][1] = normalize_ws(prev_txt.rstrip() + \" \" + t)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # 7) Prev \"Brand.\" + next \"Com/Net/Org/Id ...\" → jadikan \"Brand.Com ...\"\n",
    "        if has_prev and prev_word_dot.search(prev_txt) and tld_only_start.match(t):\n",
    "            out[-1][1] = normalize_ws(re.sub(r'\\.\\s+$', '.', prev_txt) + t)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # 8) Lone paren abbreviation (IPK). → tempel ke sebelumnya\n",
    "        if has_prev and lone_paren_abbr.match(t):\n",
    "            out[-1][1] = normalize_ws(prev_txt.rstrip() + \" \" + t)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # 9) Rangkaian setelah ':' → gulung (daftar/pertanyaan/“Diakses : …”, “Rayon 2 : …” dst.)\n",
    "        if t.rstrip().endswith(':') and has_next:\n",
    "            buf = t\n",
    "            j = i + 1\n",
    "            while j < n and items[j][0] == pid and items[j][2] == tag:\n",
    "                nxt = normalize_ws(items[j][1])\n",
    "                if (short_q.match(nxt)\n",
    "                    or nxt[0:1].islower()\n",
    "                    or nxt.rstrip().endswith(':')\n",
    "                    or re.search(r'^(Diakses\\s*:|InfoKomputer\\s*:|Rayon\\s*\\d+\\s*:|\\d+\\s*\\w*)', nxt, re.I)):\n",
    "                    buf = normalize_ws(buf + \" \" + nxt); j += 1; continue\n",
    "                if not re.search(r'[.!?]$', buf):\n",
    "                    buf = normalize_ws(buf + \" \" + nxt); j += 1; continue\n",
    "                break\n",
    "            out.append([pid, buf, tag])\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        # 10) Teks tak seimbang kurung/petik → rekatkan agar seimbang\n",
    "        if _unbalanced_paren_or_quote(t):\n",
    "            if has_prev:\n",
    "                out[-1][1] = normalize_ws(prev_txt.rstrip() + \" \" + t)\n",
    "                i += 1\n",
    "                continue\n",
    "            elif has_next and not _unbalanced_paren_or_quote(next_txt):\n",
    "                out.append([pid, normalize_ws(t + \" \" + next_txt), tag])\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "        # default\n",
    "        out.append([pid, t, tag])\n",
    "        i += 1\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "daftarKalimatProposal = [\n",
    "    [pid, _recover_overmerged_words(kal), tag]\n",
    "    for pid, kal, tag in daftarKalimatProposal\n",
    "]\n",
    "\n",
    "daftarKalimatProposal = cross_glue_items(daftarKalimatProposal)\n",
    "\n",
    "# Cari kalimat pendek\n",
    "listKalimatPendek = [k for k in daftarKalimatProposal if len(k[1].split()) <= 5]\n",
    "\n",
    "print(\"Jumlah Kalimat:\", len(daftarKalimatProposal))\n",
    "print(\"Jumlah Link yang disimpan:\", len(daftarLink))\n",
    "# daftarLink sekarang berisi semua URL unik yang dimasking selama proses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc997f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug\n"
     ]
    }
   ],
   "source": [
    "print(\"Debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33e3e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Paragraf Latar Belakang:  19113\n",
      "Jumlah Paragraf Judul:  1279\n",
      "Jumlah Paragraf Rumusan:  2407\n",
      "Jumlah Paragraf Tujuan:  1710\n"
     ]
    }
   ],
   "source": [
    "JumlahLatarBelakang = 0\n",
    "JumlahJudul = 0\n",
    "JumalahRumusan = 0\n",
    "JumlahTujuan = 0\n",
    "\n",
    "for daftar in daftarKalimatProposal :\n",
    "    if daftar[2] == 'latar_belakang' :\n",
    "        JumlahLatarBelakang = JumlahLatarBelakang + 1\n",
    "    elif daftar[2] == 'judul' :\n",
    "        JumlahJudul = JumlahJudul + 1\n",
    "    elif daftar[2] == 'rumusan' :\n",
    "        JumalahRumusan = JumalahRumusan + 1\n",
    "    elif daftar[2] == 'tujuan' :\n",
    "        JumlahTujuan = JumlahTujuan + 1\n",
    "        \n",
    "print(\"Jumlah Paragraf Latar Belakang: \", JumlahLatarBelakang)\n",
    "print(\"Jumlah Paragraf Judul: \", JumlahJudul)\n",
    "print(\"Jumlah Paragraf Rumusan: \", JumalahRumusan)\n",
    "print(\"Jumlah Paragraf Tujuan: \", JumlahTujuan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eba8be",
   "metadata": {},
   "source": [
    "Debugging Isi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9deffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah dataTuning: 24509\n",
      "Contoh: [1, '26', \"'sistem penyensoran gambar dewasa secara otomatis dengan menggunakan normalized cut, fuzzy clustering dan backpropagation'\", 'judul']\n"
     ]
    }
   ],
   "source": [
    "# === BUILD dataTuning DARI HASIL AKHIR PIPELINE ===\n",
    "import re\n",
    "\n",
    "def _normalize_ws_final(s: str) -> str:\n",
    "    # ringan saja di tahap akhir\n",
    "    s = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "    return s\n",
    "\n",
    "# (opsional) perbaiki typo \"Bove? e\" → \"Bove e\"\n",
    "def _fix_name_typos_final(s: str) -> str:\n",
    "    return re.sub(r'([A-Za-z])\\?\\s*([A-Za-z])', r'\\1\\2', s)\n",
    "\n",
    "dataTuning = []\n",
    "for idx, (pid, txt, asp) in enumerate(daftarKalimatProposal, start=1):\n",
    "    txt = _fix_name_typos_final(_normalize_ws_final(txt))\n",
    "    dataTuning.append([idx, str(pid), txt.lower(), str(asp).lower()])\n",
    "\n",
    "print(\"Jumlah dataTuning:\", len(dataTuning))\n",
    "print(\"Contoh:\", dataTuning[0] if dataTuning else \"(kosong)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c3c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tersimpan 24509 baris. Buang -> short:0 empty:0 aspect:0\n"
     ]
    }
   ],
   "source": [
    "# === SIMPAN dataTuning -> dataTuning.csv (aman Excel) ===\n",
    "# Harap: dataTuning = [[idKalimat, idProposal, Text, Aspek], ...]\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- mapping aspek ---\n",
    "_ASPECT_ALIASES = {\n",
    "    \"judul\": \"judul\", \"title\": \"judul\",\n",
    "    \"latar\": \"latar_belakang\", \"latar belakang\": \"latar_belakang\", \"latar_belakang\": \"latar_belakang\",\n",
    "    \"rumusan\": \"rumusan_masalah\", \"rumusan masalah\": \"rumusan_masalah\",\n",
    "    \"rumusan_masalah\": \"rumusan_masalah\", \"perumusan masalah\": \"rumusan_masalah\",\n",
    "    \"tujuan\": \"tujuan\", \"objective\": \"tujuan\", \"objectives\": \"tujuan\",\n",
    "}\n",
    "ALLOWED_ASPECTS = {\"judul\", \"latar_belakang\", \"rumusan_masalah\", \"tujuan\"}\n",
    "\n",
    "def normalize_aspect(a: str) -> str:\n",
    "    if a is None: return \"\"\n",
    "    key = re.sub(r\"\\s+\", \" \", str(a).strip().lower().replace(\"-\", \" \").replace(\"/\", \" \"))\n",
    "    return _ASPECT_ALIASES.get(key, key.replace(\" \", \"_\"))\n",
    "\n",
    "# Pilihan: set True jika memang ingin lowercase konten teks di CSV\n",
    "LOWER_TEXT = False\n",
    "\n",
    "rows = []\n",
    "dropped = {\"short\":0, \"empty\":0, \"aspect\":0}\n",
    "\n",
    "for row in dataTuning:\n",
    "    if not isinstance(row, (list, tuple)) or len(row) < 4:\n",
    "        dropped[\"short\"] += 1\n",
    "        continue\n",
    "    _sid, _pid, _txt, _asp = row[0], row[1], row[2], row[3]\n",
    "\n",
    "    txt = \"\" if _txt is None else str(_txt).strip()\n",
    "    if not txt:\n",
    "        dropped[\"empty\"] += 1\n",
    "        continue\n",
    "\n",
    "    asp = normalize_aspect(_asp)\n",
    "    if asp not in ALLOWED_ASPECTS:\n",
    "        dropped[\"aspect\"] += 1\n",
    "        continue\n",
    "\n",
    "    # jaga karakter unicode (—, –, “ ”, dll) apa adanya\n",
    "    if LOWER_TEXT:\n",
    "        txt = txt.lower()\n",
    "\n",
    "    rows.append({\"proposal_id\": str(_pid), \"aspect\": asp, \"text\": txt})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.insert(0, \"sentence_id\", range(1, len(df)+1))\n",
    "\n",
    "# Penting: utf-8-sig supaya Excel tidak mojibake (â€“, â€œ, dll)\n",
    "df.to_csv(\"dataTuning.csv\", index=False, encoding=\"utf-8-sig\", lineterminator=\"\\n\")\n",
    "\n",
    "\n",
    "print(f\"Tersimpan {len(df)} baris. Buang -> short:{dropped['short']} empty:{dropped['empty']} aspect:{dropped['aspect']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
